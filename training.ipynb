{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b9df08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Traditional-Chinese-Handwriting-Dataset'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AI-FREE-Team/Traditional-Chinese-Handwriting-Dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344ef29b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '113-2-ml-JRTFNsIF-py3.12 (Python 3.12.6)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/yahui/AppData/Local/pypoetry/Cache/virtualenvs/113-2-ml-JRTFNsIF-py3.12/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "OutputFolder = 'Handwritten_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6797ba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create the new \"Handwritten_Data\" folder\n",
      "目前的 Output 資料夾路徑是： c:\\Users\\yahui\\Programming\\loan-predict\\loan-risk-predict_refactor\\temp\\ML\\Handwritten_Data\n",
      "✅ 找到資料來源路徑: c:\\Users\\yahui\\Programming\\loan-predict\\loan-risk-predict_refactor\\temp\\ML\\Traditional-Chinese-Handwriting-Dataset\\data\n",
      "✅ 找到 4 個壓縮檔:\n",
      "  - cleaned_data(50_50)-20200420T071507Z-001.zip\n",
      "  - cleaned_data(50_50)-20200420T071507Z-002.zip\n",
      "  - cleaned_data(50_50)-20200420T071507Z-003.zip\n",
      "  - cleaned_data(50_50)-20200420T071507Z-004.zip\n",
      "正在解壓縮 cleaned_data(50_50)-20200420T071507Z-001.zip ......\n",
      "✅ 解壓縮完成 cleaned_data(50_50)-20200420T071507Z-001.zip\n",
      "正在解壓縮 cleaned_data(50_50)-20200420T071507Z-002.zip ......\n",
      "✅ 解壓縮完成 cleaned_data(50_50)-20200420T071507Z-002.zip\n",
      "正在解壓縮 cleaned_data(50_50)-20200420T071507Z-003.zip ......\n",
      "✅ 解壓縮完成 cleaned_data(50_50)-20200420T071507Z-003.zip\n",
      "正在解壓縮 cleaned_data(50_50)-20200420T071507Z-004.zip ......\n",
      "✅ 解壓縮完成 cleaned_data(50_50)-20200420T071507Z-004.zip\n",
      "📁 開始依照繁體中文字符整理圖片......\n",
      "找到 4803 個字符，共 250712 張圖片\n",
      "🎉 資料部署完成！\n",
      "所有圖片已整理到: c:\\Users\\yahui\\Programming\\loan-predict\\loan-risk-predict_refactor\\temp\\ML\\Handwritten_Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# 你的原本設定\n",
    "# 檢查是否存在，否則建立\n",
    "if not os.path.exists(OutputFolder):\n",
    "    os.mkdir(OutputFolder)\n",
    "    print(f'Create the new \"{OutputFolder}\" folder')\n",
    "\n",
    "# 切換到 OutputFolder\n",
    "os.chdir(OutputFolder)\n",
    "\n",
    "# 印出當前工作目錄（即 OutputFolder 的絕對路徑）\n",
    "print(\"目前的 Output 資料夾路徑是：\", os.getcwd())\n",
    "\n",
    "# 設定資料來源路徑\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# 嘗試找到 Traditional-Chinese-Handwriting-Dataset/data 資料夾\n",
    "possible_paths = [\n",
    "    os.path.join(current_dir, 'Traditional-Chinese-Handwriting-Dataset', 'data'),\n",
    "    os.path.join(parent_dir, 'Traditional-Chinese-Handwriting-Dataset', 'data'),\n",
    "    os.path.join(parent_dir, 'Handwritten_Data', 'Traditional-Chinese-Handwriting-Dataset', 'data'),\n",
    "    os.path.join(os.path.dirname(parent_dir), 'Traditional-Chinese-Handwriting-Dataset', 'data')\n",
    "]\n",
    "\n",
    "DataPath = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        DataPath = path\n",
    "        break\n",
    "\n",
    "if DataPath is None:\n",
    "    print(\"❌ 找不到 Traditional-Chinese-Handwriting-Dataset/data 資料夾\")\n",
    "    print(\"已嘗試的路徑:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"  - {path}\")\n",
    "    print(f\"\\n請確認 Traditional-Chinese-Handwriting-Dataset 資料夾的位置\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"✅ 找到資料來源路徑: {DataPath}\")\n",
    "\n",
    "CompressedFiles = []\n",
    "\n",
    "# 在 Traditional-Chinese-Handwriting-Dataset/data 中尋找 .zip 檔案\n",
    "for item in os.listdir(DataPath):\n",
    "    if item.endswith('.zip'):  # 檢查 \".zip\" 副檔名\n",
    "        file_path = os.path.join(DataPath, item)  # 取得壓縮檔的完整路徑\n",
    "        CompressedFiles.append(file_path)\n",
    "\n",
    "if len(CompressedFiles) == 0:\n",
    "    print(f\"❌ 在 {DataPath} 中找不到 .zip 檔案\")\n",
    "    print(\"該資料夾的檔案清單:\")\n",
    "    for item in os.listdir(DataPath):\n",
    "        print(f\"  - {item}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"✅ 找到 {len(CompressedFiles)} 個壓縮檔:\")\n",
    "for file in CompressedFiles:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# 解壓縮所有檔案\n",
    "for file in CompressedFiles:\n",
    "    print(f'正在解壓縮 {os.path.basename(file)} ......')\n",
    "    \n",
    "    # 建立 ZipFile 物件並解壓縮到當前目錄（OutputFolder）\n",
    "    with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "\n",
    "    # 檢查是否有 cleaned_data(50_50) 資料夾\n",
    "    source_path = './cleaned_data(50_50)'\n",
    "    \n",
    "    if os.path.exists(source_path):\n",
    "        img_list = os.listdir(source_path)\n",
    "        \n",
    "        # 移動圖片檔案到當前目錄（OutputFolder）\n",
    "        for img in img_list:\n",
    "            shutil.move(os.path.join(source_path, img), '.')\n",
    "        \n",
    "        # 刪除空的 cleaned_data(50_50) 資料夾\n",
    "        shutil.rmtree(source_path)\n",
    "    \n",
    "    print(f'✅ 解壓縮完成 {os.path.basename(file)}')\n",
    "\n",
    "print('📁 開始依照繁體中文字符整理圖片......')\n",
    "\n",
    "# 取得所有圖片檔案清單（在當前的 OutputFolder 中）\n",
    "ImageList = [f for f in os.listdir('.') if os.path.isfile(f) and len(f) > 1 and '.' in f]\n",
    "WordList = list(set([w.split('_')[0] for w in ImageList]))\n",
    "\n",
    "print(f\"找到 {len(WordList)} 個字符，共 {len(ImageList)} 張圖片\")\n",
    "\n",
    "# 為每個字符建立資料夾並移動對應的圖片\n",
    "for w in WordList:\n",
    "    try:\n",
    "        # 建立字符資料夾\n",
    "        if not os.path.exists(w):\n",
    "            os.mkdir(w)\n",
    "        \n",
    "        MoveList = [img for img in ImageList if w in img]\n",
    "        \n",
    "        # 移動相關圖片到對應的字符資料夾\n",
    "        for img in MoveList:\n",
    "            if os.path.exists(img):\n",
    "                shutil.move(img, os.path.join(w, img))\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"處理字符 '{w}' 時發生錯誤: {e}\")\n",
    "\n",
    "print('🎉 資料部署完成！')\n",
    "print(f\"所有圖片已整理到: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02cb1015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yahui\\Programming\\loan-predict\\loan-risk-predict_refactor\\temp\\ML\n"
     ]
    }
   ],
   "source": [
    "# cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2916abda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總共: 4803 個字(資料夾) / 總共: 250712個樣本\n",
      "平均每個字有: 52.19904226525089 個樣本\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "b=0\n",
    "\n",
    "for item in os.listdir(OutputFolder):\n",
    "  a += 1\n",
    "  for i in os.listdir(OutputFolder + '/' + item):\n",
    "    b +=1\n",
    "\n",
    "\n",
    "print('總共: ' + str(a) + ' 個字(資料夾) / 總共: ' + str(b) + '個樣本')\n",
    "print('平均每個字有: ' + str(b/a) + ' 個樣本')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "361f6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "\n",
    "space =  '    '\n",
    "branch = '│   '\n",
    "tee =    '├── '\n",
    "last =   '└── '\n",
    "\n",
    "def tree(dir_path: Path, level: int=-1, limit_to_directories: bool=False,\n",
    "         length_limit: int=1000):\n",
    "    \"\"\"Given a directory Path object print a visual tree structure\"\"\"\n",
    "    dir_path = Path(dir_path) # accept string coerceable to Path\n",
    "    files = 0\n",
    "    directories = 0\n",
    "    def inner(dir_path: Path, prefix: str='', level=-1):\n",
    "        nonlocal files, directories\n",
    "        if not level:\n",
    "            return # 0, stop iterating\n",
    "        if limit_to_directories:\n",
    "            contents = [d for d in dir_path.iterdir() if d.is_dir()]\n",
    "        else:\n",
    "            contents = list(dir_path.iterdir())\n",
    "        pointers = [tee] * (len(contents) - 1) + [last]\n",
    "        for pointer, path in zip(pointers, contents):\n",
    "            if path.is_dir():\n",
    "                yield prefix + pointer + path.name\n",
    "                directories += 1\n",
    "                extension = branch if pointer == tee else space\n",
    "                yield from inner(path, prefix=prefix+extension, level=level-1)\n",
    "            elif not limit_to_directories:\n",
    "                yield prefix + pointer + path.name\n",
    "                files += 1\n",
    "    print(dir_path.name)\n",
    "    iterator = inner(dir_path, level=level)\n",
    "    for line in islice(iterator, length_limit):\n",
    "        print(line)\n",
    "    if next(iterator, None):\n",
    "        print(f'... length_limit, {length_limit}, reached, counted:')\n",
    "    print(f'\\n{directories} directories' + (f', {files} files' if files else ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1681fe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML\n",
      "├── Handwritten_Data\n",
      "│   ├── 丁\n",
      "│   │   ├── 丁_0.png\n",
      "│   │   ├── 丁_1.png\n",
      "│   │   ├── 丁_10.png\n",
      "│   │   ├── 丁_11.png\n",
      "│   │   ├── 丁_12.png\n",
      "│   │   ├── 丁_13.png\n",
      "│   │   ├── 丁_14.png\n",
      "│   │   ├── 丁_15.png\n",
      "│   │   ├── 丁_16.png\n",
      "│   │   ├── 丁_17.png\n",
      "│   │   ├── 丁_18.png\n",
      "│   │   ├── 丁_19.png\n",
      "│   │   ├── 丁_2.png\n",
      "│   │   ├── 丁_20.png\n",
      "│   │   ├── 丁_21.png\n",
      "│   │   ├── 丁_22.png\n",
      "│   │   ├── 丁_23.png\n",
      "│   │   ├── 丁_24.png\n",
      "│   │   ├── 丁_25.png\n",
      "│   │   ├── 丁_26.png\n",
      "│   │   ├── 丁_27.png\n",
      "│   │   ├── 丁_28.png\n",
      "│   │   ├── 丁_29.png\n",
      "│   │   ├── 丁_3.png\n",
      "│   │   ├── 丁_30.png\n",
      "│   │   ├── 丁_31.png\n",
      "│   │   ├── 丁_32.png\n",
      "│   │   ├── 丁_33.png\n",
      "│   │   ├── 丁_34.png\n",
      "│   │   ├── 丁_35.png\n",
      "│   │   ├── 丁_36.png\n",
      "│   │   ├── 丁_37.png\n",
      "│   │   ├── 丁_38.png\n",
      "│   │   ├── 丁_39.png\n",
      "│   │   ├── 丁_4.png\n",
      "│   │   ├── 丁_40.png\n",
      "│   │   ├── 丁_41.png\n",
      "│   │   ├── 丁_42.png\n",
      "│   │   ├── 丁_43.png\n",
      "│   │   ├── 丁_44.png\n",
      "│   │   ├── 丁_45.png\n",
      "│   │   ├── 丁_46.png\n",
      "│   │   ├── 丁_47.png\n",
      "│   │   ├── 丁_48.png\n",
      "│   │   ├── 丁_49.png\n",
      "│   │   ├── 丁_5.png\n",
      "│   │   ├── 丁_50.png\n",
      "│   │   ├── 丁_6.png\n",
      "│   │   ├── 丁_7.png\n",
      "│   │   ├── 丁_8.png\n",
      "│   │   └── 丁_9.png\n",
      "│   ├── 七\n",
      "│   │   ├── 七_0.png\n",
      "│   │   ├── 七_1.png\n",
      "│   │   ├── 七_10.png\n",
      "│   │   ├── 七_11.png\n",
      "│   │   ├── 七_12.png\n",
      "│   │   ├── 七_13.png\n",
      "│   │   ├── 七_14.png\n",
      "│   │   ├── 七_15.png\n",
      "│   │   ├── 七_16.png\n",
      "│   │   ├── 七_17.png\n",
      "│   │   ├── 七_18.png\n",
      "│   │   ├── 七_19.png\n",
      "│   │   ├── 七_2.png\n",
      "│   │   ├── 七_20.png\n",
      "│   │   ├── 七_21.png\n",
      "│   │   ├── 七_22.png\n",
      "│   │   ├── 七_23.png\n",
      "│   │   ├── 七_24.png\n",
      "│   │   ├── 七_25.png\n",
      "│   │   ├── 七_26.png\n",
      "│   │   ├── 七_27.png\n",
      "│   │   ├── 七_28.png\n",
      "│   │   ├── 七_29.png\n",
      "│   │   ├── 七_3.png\n",
      "│   │   ├── 七_30.png\n",
      "│   │   ├── 七_31.png\n",
      "│   │   ├── 七_32.png\n",
      "│   │   ├── 七_33.png\n",
      "│   │   ├── 七_34.png\n",
      "│   │   ├── 七_35.png\n",
      "│   │   ├── 七_36.png\n",
      "│   │   ├── 七_37.png\n",
      "│   │   ├── 七_38.png\n",
      "│   │   ├── 七_39.png\n",
      "│   │   ├── 七_4.png\n",
      "│   │   ├── 七_40.png\n",
      "│   │   ├── 七_41.png\n",
      "│   │   ├── 七_42.png\n",
      "│   │   ├── 七_43.png\n",
      "│   │   ├── 七_44.png\n",
      "│   │   ├── 七_45.png\n",
      "│   │   ├── 七_46.png\n",
      "│   │   ├── 七_47.png\n",
      "│   │   ├── 七_48.png\n",
      "│   │   ├── 七_49.png\n",
      "│   │   ├── 七_5.png\n",
      "│   │   ├── 七_50.png\n",
      "│   │   ├── 七_6.png\n",
      "│   │   ├── 七_7.png\n",
      "│   │   ├── 七_8.png\n",
      "│   │   └── 七_9.png\n",
      "│   ├── 丈\n",
      "│   │   ├── 丈_0.png\n",
      "│   │   ├── 丈_1.png\n",
      "│   │   ├── 丈_10.png\n",
      "│   │   ├── 丈_11.png\n",
      "│   │   ├── 丈_12.png\n",
      "│   │   ├── 丈_13.png\n",
      "│   │   ├── 丈_14.png\n",
      "│   │   ├── 丈_15.png\n",
      "│   │   ├── 丈_16.png\n",
      "│   │   ├── 丈_17.png\n",
      "│   │   ├── 丈_18.png\n",
      "│   │   ├── 丈_19.png\n",
      "│   │   ├── 丈_2.png\n",
      "│   │   ├── 丈_20.png\n",
      "│   │   ├── 丈_21.png\n",
      "│   │   ├── 丈_22.png\n",
      "│   │   ├── 丈_23.png\n",
      "│   │   ├── 丈_24.png\n",
      "│   │   ├── 丈_25.png\n",
      "│   │   ├── 丈_26.png\n",
      "│   │   ├── 丈_27.png\n",
      "│   │   ├── 丈_28.png\n",
      "│   │   ├── 丈_29.png\n",
      "│   │   ├── 丈_3.png\n",
      "│   │   ├── 丈_30.png\n",
      "│   │   ├── 丈_31.png\n",
      "│   │   ├── 丈_32.png\n",
      "│   │   ├── 丈_33.png\n",
      "│   │   ├── 丈_34.png\n",
      "│   │   ├── 丈_35.png\n",
      "│   │   ├── 丈_36.png\n",
      "│   │   ├── 丈_37.png\n",
      "│   │   ├── 丈_38.png\n",
      "│   │   ├── 丈_39.png\n",
      "│   │   ├── 丈_4.png\n",
      "│   │   ├── 丈_40.png\n",
      "│   │   ├── 丈_41.png\n",
      "│   │   ├── 丈_42.png\n",
      "│   │   ├── 丈_43.png\n",
      "│   │   ├── 丈_44.png\n",
      "│   │   ├── 丈_45.png\n",
      "│   │   ├── 丈_46.png\n",
      "│   │   ├── 丈_47.png\n",
      "│   │   ├── 丈_48.png\n",
      "│   │   ├── 丈_49.png\n",
      "│   │   ├── 丈_5.png\n",
      "│   │   ├── 丈_50.png\n",
      "│   │   ├── 丈_6.png\n",
      "│   │   ├── 丈_7.png\n",
      "│   │   ├── 丈_8.png\n",
      "│   │   └── 丈_9.png\n",
      "│   ├── 三\n",
      "│   │   ├── 三_0.png\n",
      "│   │   ├── 三_1.png\n",
      "│   │   ├── 三_10.png\n",
      "│   │   ├── 三_11.png\n",
      "│   │   ├── 三_12.png\n",
      "│   │   ├── 三_13.png\n",
      "│   │   ├── 三_14.png\n",
      "│   │   ├── 三_15.png\n",
      "│   │   ├── 三_16.png\n",
      "│   │   ├── 三_17.png\n",
      "│   │   ├── 三_18.png\n",
      "│   │   ├── 三_19.png\n",
      "│   │   ├── 三_2.png\n",
      "│   │   ├── 三_20.png\n",
      "│   │   ├── 三_21.png\n",
      "│   │   ├── 三_22.png\n",
      "│   │   ├── 三_23.png\n",
      "│   │   ├── 三_24.png\n",
      "│   │   ├── 三_25.png\n",
      "│   │   ├── 三_26.png\n",
      "│   │   ├── 三_27.png\n",
      "│   │   ├── 三_28.png\n",
      "│   │   ├── 三_29.png\n",
      "│   │   ├── 三_3.png\n",
      "│   │   ├── 三_30.png\n",
      "│   │   ├── 三_31.png\n",
      "│   │   ├── 三_32.png\n",
      "│   │   ├── 三_33.png\n",
      "│   │   ├── 三_34.png\n",
      "│   │   ├── 三_35.png\n",
      "│   │   ├── 三_36.png\n",
      "│   │   ├── 三_37.png\n",
      "│   │   ├── 三_38.png\n",
      "│   │   ├── 三_39.png\n",
      "│   │   ├── 三_4.png\n",
      "│   │   ├── 三_40.png\n",
      "│   │   ├── 三_41.png\n",
      "│   │   ├── 三_42.png\n",
      "│   │   ├── 三_43.png\n",
      "│   │   ├── 三_44.png\n",
      "│   │   ├── 三_45.png\n",
      "│   │   ├── 三_46.png\n",
      "│   │   ├── 三_47.png\n",
      "│   │   ├── 三_48.png\n",
      "│   │   ├── 三_49.png\n",
      "│   │   ├── 三_5.png\n",
      "│   │   ├── 三_50.png\n",
      "│   │   ├── 三_6.png\n",
      "│   │   ├── 三_7.png\n",
      "│   │   ├── 三_8.png\n",
      "│   │   └── 三_9.png\n",
      "│   ├── 上\n",
      "│   │   ├── 上_0.png\n",
      "│   │   ├── 上_1.png\n",
      "│   │   ├── 上_10.png\n",
      "│   │   ├── 上_11.png\n",
      "│   │   ├── 上_12.png\n",
      "│   │   ├── 上_13.png\n",
      "│   │   ├── 上_14.png\n",
      "│   │   ├── 上_15.png\n",
      "│   │   ├── 上_16.png\n",
      "│   │   ├── 上_17.png\n",
      "│   │   ├── 上_18.png\n",
      "│   │   ├── 上_19.png\n",
      "│   │   ├── 上_2.png\n",
      "│   │   ├── 上_20.png\n",
      "│   │   ├── 上_21.png\n",
      "│   │   ├── 上_22.png\n",
      "│   │   ├── 上_23.png\n",
      "│   │   ├── 上_24.png\n",
      "│   │   ├── 上_25.png\n",
      "│   │   ├── 上_26.png\n",
      "│   │   ├── 上_27.png\n",
      "│   │   ├── 上_28.png\n",
      "│   │   ├── 上_29.png\n",
      "│   │   ├── 上_3.png\n",
      "│   │   ├── 上_30.png\n",
      "│   │   ├── 上_31.png\n",
      "│   │   ├── 上_32.png\n",
      "│   │   ├── 上_33.png\n",
      "│   │   ├── 上_34.png\n",
      "│   │   ├── 上_35.png\n",
      "│   │   ├── 上_36.png\n",
      "│   │   ├── 上_37.png\n",
      "│   │   ├── 上_38.png\n",
      "│   │   ├── 上_39.png\n",
      "│   │   ├── 上_4.png\n",
      "│   │   ├── 上_40.png\n",
      "│   │   ├── 上_41.png\n",
      "│   │   ├── 上_42.png\n",
      "│   │   ├── 上_43.png\n",
      "│   │   ├── 上_44.png\n",
      "│   │   ├── 上_45.png\n",
      "│   │   ├── 上_46.png\n",
      "│   │   ├── 上_47.png\n",
      "│   │   ├── 上_48.png\n",
      "│   │   ├── 上_49.png\n",
      "│   │   ├── 上_5.png\n",
      "│   │   ├── 上_50.png\n",
      "│   │   ├── 上_6.png\n",
      "│   │   ├── 上_7.png\n",
      "│   │   ├── 上_8.png\n",
      "│   │   └── 上_9.png\n",
      "│   ├── 下\n",
      "│   │   ├── 下_0.png\n",
      "│   │   ├── 下_1.png\n",
      "│   │   ├── 下_10.png\n",
      "│   │   ├── 下_11.png\n",
      "│   │   ├── 下_12.png\n",
      "│   │   ├── 下_13.png\n",
      "│   │   ├── 下_14.png\n",
      "│   │   ├── 下_15.png\n",
      "│   │   ├── 下_16.png\n",
      "│   │   ├── 下_17.png\n",
      "│   │   ├── 下_18.png\n",
      "│   │   ├── 下_19.png\n",
      "│   │   ├── 下_2.png\n",
      "│   │   ├── 下_20.png\n",
      "│   │   ├── 下_21.png\n",
      "│   │   ├── 下_22.png\n",
      "│   │   ├── 下_23.png\n",
      "│   │   ├── 下_24.png\n",
      "│   │   ├── 下_25.png\n",
      "│   │   ├── 下_26.png\n",
      "│   │   ├── 下_27.png\n",
      "│   │   ├── 下_28.png\n",
      "│   │   ├── 下_29.png\n",
      "│   │   ├── 下_3.png\n",
      "│   │   ├── 下_30.png\n",
      "│   │   ├── 下_31.png\n",
      "│   │   ├── 下_32.png\n",
      "│   │   ├── 下_33.png\n",
      "│   │   ├── 下_34.png\n",
      "│   │   ├── 下_35.png\n",
      "│   │   ├── 下_36.png\n",
      "│   │   ├── 下_37.png\n",
      "│   │   ├── 下_38.png\n",
      "│   │   ├── 下_39.png\n",
      "│   │   ├── 下_4.png\n",
      "│   │   ├── 下_40.png\n",
      "│   │   ├── 下_41.png\n",
      "│   │   ├── 下_42.png\n",
      "│   │   ├── 下_43.png\n",
      "│   │   ├── 下_44.png\n",
      "│   │   ├── 下_45.png\n",
      "│   │   ├── 下_46.png\n",
      "│   │   ├── 下_47.png\n",
      "│   │   ├── 下_48.png\n",
      "│   │   ├── 下_49.png\n",
      "│   │   ├── 下_5.png\n",
      "│   │   ├── 下_50.png\n",
      "│   │   ├── 下_6.png\n",
      "│   │   ├── 下_7.png\n",
      "│   │   ├── 下_8.png\n",
      "│   │   └── 下_9.png\n",
      "│   ├── 不\n",
      "│   │   ├── 不_0.png\n",
      "│   │   ├── 不_1.png\n",
      "│   │   ├── 不_10.png\n",
      "│   │   ├── 不_11.png\n",
      "│   │   ├── 不_12.png\n",
      "│   │   ├── 不_13.png\n",
      "│   │   ├── 不_14.png\n",
      "│   │   ├── 不_15.png\n",
      "│   │   ├── 不_16.png\n",
      "│   │   ├── 不_17.png\n",
      "│   │   ├── 不_18.png\n",
      "│   │   ├── 不_19.png\n",
      "│   │   ├── 不_2.png\n",
      "│   │   ├── 不_20.png\n",
      "│   │   ├── 不_21.png\n",
      "│   │   ├── 不_22.png\n",
      "│   │   ├── 不_23.png\n",
      "│   │   ├── 不_24.png\n",
      "│   │   ├── 不_25.png\n",
      "│   │   ├── 不_26.png\n",
      "│   │   ├── 不_27.png\n",
      "│   │   ├── 不_28.png\n",
      "│   │   ├── 不_29.png\n",
      "│   │   ├── 不_3.png\n",
      "│   │   ├── 不_30.png\n",
      "│   │   ├── 不_31.png\n",
      "│   │   ├── 不_32.png\n",
      "│   │   ├── 不_33.png\n",
      "│   │   ├── 不_34.png\n",
      "│   │   ├── 不_35.png\n",
      "│   │   ├── 不_36.png\n",
      "│   │   ├── 不_37.png\n",
      "│   │   ├── 不_38.png\n",
      "│   │   ├── 不_39.png\n",
      "│   │   ├── 不_4.png\n",
      "│   │   ├── 不_40.png\n",
      "│   │   ├── 不_41.png\n",
      "│   │   ├── 不_42.png\n",
      "│   │   ├── 不_43.png\n",
      "│   │   ├── 不_44.png\n",
      "│   │   ├── 不_45.png\n",
      "│   │   ├── 不_46.png\n",
      "│   │   ├── 不_47.png\n",
      "│   │   ├── 不_48.png\n",
      "│   │   ├── 不_49.png\n",
      "│   │   ├── 不_5.png\n",
      "│   │   ├── 不_50.png\n",
      "│   │   ├── 不_6.png\n",
      "│   │   ├── 不_7.png\n",
      "│   │   ├── 不_8.png\n",
      "│   │   └── 不_9.png\n",
      "│   ├── 丐\n",
      "│   │   ├── 丐_0.png\n",
      "│   │   ├── 丐_1.png\n",
      "│   │   ├── 丐_10.png\n",
      "│   │   ├── 丐_11.png\n",
      "│   │   ├── 丐_12.png\n",
      "│   │   ├── 丐_13.png\n",
      "│   │   ├── 丐_14.png\n",
      "│   │   ├── 丐_15.png\n",
      "│   │   ├── 丐_16.png\n",
      "│   │   ├── 丐_17.png\n",
      "│   │   ├── 丐_18.png\n",
      "│   │   ├── 丐_19.png\n",
      "│   │   ├── 丐_2.png\n",
      "│   │   ├── 丐_20.png\n",
      "│   │   ├── 丐_21.png\n",
      "│   │   ├── 丐_22.png\n",
      "│   │   ├── 丐_23.png\n",
      "│   │   ├── 丐_24.png\n",
      "│   │   ├── 丐_25.png\n",
      "│   │   ├── 丐_26.png\n",
      "│   │   ├── 丐_27.png\n",
      "│   │   ├── 丐_28.png\n",
      "│   │   ├── 丐_29.png\n",
      "│   │   ├── 丐_3.png\n",
      "│   │   ├── 丐_30.png\n",
      "│   │   ├── 丐_31.png\n",
      "│   │   ├── 丐_32.png\n",
      "│   │   ├── 丐_33.png\n",
      "│   │   ├── 丐_34.png\n",
      "│   │   ├── 丐_35.png\n",
      "│   │   ├── 丐_36.png\n",
      "│   │   ├── 丐_37.png\n",
      "│   │   ├── 丐_38.png\n",
      "│   │   ├── 丐_39.png\n",
      "│   │   ├── 丐_4.png\n",
      "│   │   ├── 丐_40.png\n",
      "│   │   ├── 丐_41.png\n",
      "│   │   ├── 丐_42.png\n",
      "│   │   ├── 丐_43.png\n",
      "│   │   ├── 丐_44.png\n",
      "│   │   ├── 丐_45.png\n",
      "│   │   ├── 丐_46.png\n",
      "│   │   ├── 丐_47.png\n",
      "│   │   ├── 丐_48.png\n",
      "│   │   ├── 丐_49.png\n",
      "│   │   ├── 丐_5.png\n",
      "│   │   ├── 丐_50.png\n",
      "│   │   ├── 丐_6.png\n",
      "│   │   ├── 丐_7.png\n",
      "│   │   ├── 丐_8.png\n",
      "│   │   └── 丐_9.png\n",
      "│   ├── 丑\n",
      "│   │   ├── 丑_0.png\n",
      "│   │   ├── 丑_1.png\n",
      "│   │   ├── 丑_10.png\n",
      "│   │   ├── 丑_11.png\n",
      "│   │   ├── 丑_12.png\n",
      "│   │   ├── 丑_13.png\n",
      "│   │   ├── 丑_14.png\n",
      "│   │   ├── 丑_15.png\n",
      "│   │   ├── 丑_16.png\n",
      "│   │   ├── 丑_17.png\n",
      "│   │   ├── 丑_18.png\n",
      "│   │   ├── 丑_19.png\n",
      "│   │   ├── 丑_2.png\n",
      "│   │   ├── 丑_20.png\n",
      "│   │   ├── 丑_21.png\n",
      "│   │   ├── 丑_22.png\n",
      "│   │   ├── 丑_23.png\n",
      "│   │   ├── 丑_24.png\n",
      "│   │   ├── 丑_25.png\n",
      "│   │   ├── 丑_26.png\n",
      "│   │   ├── 丑_27.png\n",
      "│   │   ├── 丑_28.png\n",
      "│   │   ├── 丑_29.png\n",
      "│   │   ├── 丑_3.png\n",
      "│   │   ├── 丑_30.png\n",
      "│   │   ├── 丑_31.png\n",
      "│   │   ├── 丑_32.png\n",
      "│   │   ├── 丑_33.png\n",
      "│   │   ├── 丑_34.png\n",
      "│   │   ├── 丑_35.png\n",
      "│   │   ├── 丑_36.png\n",
      "│   │   ├── 丑_37.png\n",
      "│   │   ├── 丑_38.png\n",
      "│   │   ├── 丑_39.png\n",
      "│   │   ├── 丑_4.png\n",
      "│   │   ├── 丑_40.png\n",
      "│   │   ├── 丑_41.png\n",
      "│   │   ├── 丑_42.png\n",
      "│   │   ├── 丑_43.png\n",
      "│   │   ├── 丑_44.png\n",
      "│   │   ├── 丑_45.png\n",
      "│   │   ├── 丑_46.png\n",
      "│   │   ├── 丑_47.png\n",
      "│   │   ├── 丑_48.png\n",
      "│   │   ├── 丑_49.png\n",
      "│   │   ├── 丑_5.png\n",
      "│   │   ├── 丑_50.png\n",
      "│   │   ├── 丑_6.png\n",
      "│   │   ├── 丑_7.png\n",
      "│   │   ├── 丑_8.png\n",
      "│   │   └── 丑_9.png\n",
      "│   ├── 且\n",
      "│   │   ├── 且_0.png\n",
      "│   │   ├── 且_1.png\n",
      "│   │   ├── 且_10.png\n",
      "│   │   ├── 且_11.png\n",
      "│   │   ├── 且_12.png\n",
      "│   │   ├── 且_13.png\n",
      "│   │   ├── 且_14.png\n",
      "│   │   ├── 且_15.png\n",
      "│   │   ├── 且_16.png\n",
      "│   │   ├── 且_17.png\n",
      "│   │   ├── 且_18.png\n",
      "│   │   ├── 且_19.png\n",
      "│   │   ├── 且_2.png\n",
      "│   │   ├── 且_20.png\n",
      "│   │   ├── 且_21.png\n",
      "│   │   ├── 且_22.png\n",
      "│   │   ├── 且_23.png\n",
      "│   │   ├── 且_24.png\n",
      "│   │   ├── 且_25.png\n",
      "│   │   ├── 且_26.png\n",
      "│   │   ├── 且_27.png\n",
      "│   │   ├── 且_28.png\n",
      "│   │   ├── 且_29.png\n",
      "│   │   ├── 且_3.png\n",
      "│   │   ├── 且_30.png\n",
      "│   │   ├── 且_31.png\n",
      "│   │   ├── 且_32.png\n",
      "│   │   ├── 且_33.png\n",
      "│   │   ├── 且_34.png\n",
      "│   │   ├── 且_35.png\n",
      "│   │   ├── 且_36.png\n",
      "│   │   ├── 且_37.png\n",
      "│   │   ├── 且_38.png\n",
      "│   │   ├── 且_39.png\n",
      "│   │   ├── 且_4.png\n",
      "│   │   ├── 且_40.png\n",
      "│   │   ├── 且_41.png\n",
      "│   │   ├── 且_42.png\n",
      "│   │   ├── 且_43.png\n",
      "│   │   ├── 且_44.png\n",
      "│   │   ├── 且_45.png\n",
      "│   │   ├── 且_46.png\n",
      "│   │   ├── 且_47.png\n",
      "│   │   ├── 且_48.png\n",
      "│   │   ├── 且_49.png\n",
      "│   │   ├── 且_5.png\n",
      "│   │   ├── 且_50.png\n",
      "│   │   ├── 且_6.png\n",
      "│   │   ├── 且_7.png\n",
      "│   │   ├── 且_8.png\n",
      "│   │   └── 且_9.png\n",
      "│   ├── 丕\n",
      "│   │   ├── 丕_0.png\n",
      "│   │   ├── 丕_1.png\n",
      "│   │   ├── 丕_10.png\n",
      "│   │   ├── 丕_11.png\n",
      "│   │   ├── 丕_12.png\n",
      "│   │   ├── 丕_13.png\n",
      "│   │   ├── 丕_14.png\n",
      "│   │   ├── 丕_15.png\n",
      "│   │   ├── 丕_16.png\n",
      "│   │   ├── 丕_17.png\n",
      "│   │   ├── 丕_18.png\n",
      "│   │   ├── 丕_19.png\n",
      "│   │   ├── 丕_2.png\n",
      "│   │   ├── 丕_20.png\n",
      "│   │   ├── 丕_21.png\n",
      "│   │   ├── 丕_22.png\n",
      "│   │   ├── 丕_23.png\n",
      "│   │   ├── 丕_24.png\n",
      "│   │   ├── 丕_25.png\n",
      "│   │   ├── 丕_26.png\n",
      "│   │   ├── 丕_27.png\n",
      "│   │   ├── 丕_28.png\n",
      "│   │   ├── 丕_29.png\n",
      "│   │   ├── 丕_3.png\n",
      "│   │   ├── 丕_30.png\n",
      "│   │   ├── 丕_31.png\n",
      "│   │   ├── 丕_32.png\n",
      "│   │   ├── 丕_33.png\n",
      "│   │   ├── 丕_34.png\n",
      "│   │   ├── 丕_35.png\n",
      "│   │   ├── 丕_36.png\n",
      "│   │   ├── 丕_37.png\n",
      "│   │   ├── 丕_38.png\n",
      "│   │   ├── 丕_39.png\n",
      "│   │   ├── 丕_4.png\n",
      "│   │   ├── 丕_40.png\n",
      "│   │   ├── 丕_41.png\n",
      "│   │   ├── 丕_42.png\n",
      "│   │   ├── 丕_43.png\n",
      "│   │   ├── 丕_44.png\n",
      "│   │   ├── 丕_45.png\n",
      "│   │   ├── 丕_46.png\n",
      "│   │   ├── 丕_47.png\n",
      "│   │   ├── 丕_48.png\n",
      "│   │   ├── 丕_49.png\n",
      "│   │   ├── 丕_5.png\n",
      "│   │   ├── 丕_50.png\n",
      "│   │   ├── 丕_6.png\n",
      "│   │   ├── 丕_7.png\n",
      "│   │   ├── 丕_8.png\n",
      "│   │   └── 丕_9.png\n",
      "│   ├── 世\n",
      "│   │   ├── 世_0.png\n",
      "│   │   ├── 世_1.png\n",
      "│   │   ├── 世_10.png\n",
      "│   │   ├── 世_11.png\n",
      "│   │   ├── 世_12.png\n",
      "│   │   ├── 世_13.png\n",
      "│   │   ├── 世_14.png\n",
      "│   │   ├── 世_15.png\n",
      "│   │   ├── 世_16.png\n",
      "│   │   ├── 世_17.png\n",
      "│   │   ├── 世_18.png\n",
      "│   │   ├── 世_19.png\n",
      "│   │   ├── 世_2.png\n",
      "│   │   ├── 世_20.png\n",
      "│   │   ├── 世_21.png\n",
      "│   │   ├── 世_22.png\n",
      "│   │   ├── 世_23.png\n",
      "│   │   ├── 世_24.png\n",
      "│   │   ├── 世_25.png\n",
      "│   │   ├── 世_26.png\n",
      "│   │   ├── 世_27.png\n",
      "│   │   ├── 世_28.png\n",
      "│   │   ├── 世_29.png\n",
      "│   │   ├── 世_3.png\n",
      "│   │   ├── 世_30.png\n",
      "│   │   ├── 世_31.png\n",
      "│   │   ├── 世_32.png\n",
      "│   │   ├── 世_33.png\n",
      "│   │   ├── 世_34.png\n",
      "│   │   ├── 世_35.png\n",
      "│   │   ├── 世_36.png\n",
      "│   │   ├── 世_37.png\n",
      "│   │   ├── 世_38.png\n",
      "│   │   ├── 世_39.png\n",
      "│   │   ├── 世_4.png\n",
      "│   │   ├── 世_40.png\n",
      "│   │   ├── 世_41.png\n",
      "│   │   ├── 世_42.png\n",
      "│   │   ├── 世_43.png\n",
      "│   │   ├── 世_44.png\n",
      "│   │   ├── 世_45.png\n",
      "│   │   ├── 世_46.png\n",
      "│   │   ├── 世_47.png\n",
      "│   │   ├── 世_48.png\n",
      "│   │   ├── 世_49.png\n",
      "│   │   ├── 世_5.png\n",
      "│   │   ├── 世_50.png\n",
      "│   │   ├── 世_6.png\n",
      "│   │   ├── 世_7.png\n",
      "│   │   ├── 世_8.png\n",
      "│   │   └── 世_9.png\n",
      "│   ├── 丘\n",
      "│   │   ├── 丘_0.png\n",
      "│   │   ├── 丘_1.png\n",
      "│   │   ├── 丘_10.png\n",
      "│   │   ├── 丘_11.png\n",
      "│   │   ├── 丘_12.png\n",
      "│   │   ├── 丘_13.png\n",
      "│   │   ├── 丘_14.png\n",
      "│   │   ├── 丘_15.png\n",
      "│   │   ├── 丘_16.png\n",
      "│   │   ├── 丘_17.png\n",
      "│   │   ├── 丘_18.png\n",
      "│   │   ├── 丘_19.png\n",
      "│   │   ├── 丘_2.png\n",
      "│   │   ├── 丘_20.png\n",
      "│   │   ├── 丘_21.png\n",
      "│   │   ├── 丘_22.png\n",
      "│   │   ├── 丘_23.png\n",
      "│   │   ├── 丘_24.png\n",
      "│   │   ├── 丘_25.png\n",
      "│   │   ├── 丘_26.png\n",
      "│   │   ├── 丘_27.png\n",
      "│   │   ├── 丘_28.png\n",
      "│   │   ├── 丘_29.png\n",
      "│   │   ├── 丘_3.png\n",
      "│   │   ├── 丘_30.png\n",
      "│   │   ├── 丘_31.png\n",
      "│   │   ├── 丘_32.png\n",
      "│   │   ├── 丘_33.png\n",
      "│   │   ├── 丘_34.png\n",
      "│   │   ├── 丘_35.png\n",
      "│   │   ├── 丘_36.png\n",
      "│   │   ├── 丘_37.png\n",
      "│   │   ├── 丘_38.png\n",
      "│   │   ├── 丘_39.png\n",
      "│   │   ├── 丘_4.png\n",
      "│   │   ├── 丘_40.png\n",
      "│   │   ├── 丘_41.png\n",
      "│   │   ├── 丘_42.png\n",
      "│   │   ├── 丘_43.png\n",
      "│   │   ├── 丘_44.png\n",
      "│   │   ├── 丘_45.png\n",
      "│   │   ├── 丘_46.png\n",
      "│   │   ├── 丘_47.png\n",
      "│   │   ├── 丘_48.png\n",
      "│   │   ├── 丘_49.png\n",
      "│   │   ├── 丘_5.png\n",
      "│   │   ├── 丘_50.png\n",
      "│   │   ├── 丘_6.png\n",
      "│   │   ├── 丘_7.png\n",
      "│   │   ├── 丘_8.png\n",
      "│   │   └── 丘_9.png\n",
      "│   ├── 丙\n",
      "│   │   ├── 丙_0.png\n",
      "│   │   ├── 丙_1.png\n",
      "│   │   ├── 丙_10.png\n",
      "│   │   ├── 丙_11.png\n",
      "│   │   ├── 丙_12.png\n",
      "│   │   ├── 丙_13.png\n",
      "│   │   ├── 丙_14.png\n",
      "│   │   ├── 丙_15.png\n",
      "│   │   ├── 丙_16.png\n",
      "│   │   ├── 丙_17.png\n",
      "│   │   ├── 丙_18.png\n",
      "│   │   ├── 丙_19.png\n",
      "│   │   ├── 丙_2.png\n",
      "│   │   ├── 丙_20.png\n",
      "│   │   ├── 丙_21.png\n",
      "│   │   ├── 丙_22.png\n",
      "│   │   ├── 丙_23.png\n",
      "│   │   ├── 丙_24.png\n",
      "│   │   ├── 丙_25.png\n",
      "│   │   ├── 丙_26.png\n",
      "│   │   ├── 丙_27.png\n",
      "│   │   ├── 丙_28.png\n",
      "│   │   ├── 丙_29.png\n",
      "│   │   ├── 丙_3.png\n",
      "│   │   ├── 丙_30.png\n",
      "│   │   ├── 丙_31.png\n",
      "│   │   ├── 丙_32.png\n",
      "│   │   ├── 丙_33.png\n",
      "│   │   ├── 丙_34.png\n",
      "│   │   ├── 丙_35.png\n",
      "│   │   ├── 丙_36.png\n",
      "│   │   ├── 丙_37.png\n",
      "│   │   ├── 丙_38.png\n",
      "│   │   ├── 丙_39.png\n",
      "│   │   ├── 丙_4.png\n",
      "│   │   ├── 丙_40.png\n",
      "│   │   ├── 丙_41.png\n",
      "│   │   ├── 丙_42.png\n",
      "│   │   ├── 丙_43.png\n",
      "│   │   ├── 丙_44.png\n",
      "│   │   ├── 丙_45.png\n",
      "│   │   ├── 丙_46.png\n",
      "│   │   ├── 丙_47.png\n",
      "│   │   ├── 丙_48.png\n",
      "│   │   ├── 丙_49.png\n",
      "│   │   ├── 丙_5.png\n",
      "│   │   ├── 丙_50.png\n",
      "│   │   ├── 丙_6.png\n",
      "│   │   ├── 丙_7.png\n",
      "│   │   ├── 丙_8.png\n",
      "│   │   └── 丙_9.png\n",
      "│   ├── 丞\n",
      "│   │   ├── 丞_0.png\n",
      "│   │   ├── 丞_1.png\n",
      "│   │   ├── 丞_10.png\n",
      "│   │   ├── 丞_11.png\n",
      "│   │   ├── 丞_12.png\n",
      "│   │   ├── 丞_13.png\n",
      "│   │   ├── 丞_14.png\n",
      "│   │   ├── 丞_15.png\n",
      "│   │   ├── 丞_16.png\n",
      "│   │   ├── 丞_17.png\n",
      "│   │   ├── 丞_18.png\n",
      "│   │   ├── 丞_19.png\n",
      "│   │   ├── 丞_2.png\n",
      "│   │   ├── 丞_20.png\n",
      "│   │   ├── 丞_21.png\n",
      "│   │   ├── 丞_22.png\n",
      "│   │   ├── 丞_23.png\n",
      "│   │   ├── 丞_24.png\n",
      "│   │   ├── 丞_25.png\n",
      "│   │   ├── 丞_26.png\n",
      "│   │   ├── 丞_27.png\n",
      "│   │   ├── 丞_28.png\n",
      "│   │   ├── 丞_29.png\n",
      "│   │   ├── 丞_3.png\n",
      "│   │   ├── 丞_30.png\n",
      "│   │   ├── 丞_31.png\n",
      "│   │   ├── 丞_32.png\n",
      "│   │   ├── 丞_33.png\n",
      "│   │   ├── 丞_34.png\n",
      "│   │   ├── 丞_35.png\n",
      "│   │   ├── 丞_36.png\n",
      "│   │   ├── 丞_37.png\n",
      "│   │   ├── 丞_38.png\n",
      "│   │   ├── 丞_39.png\n",
      "│   │   ├── 丞_4.png\n",
      "│   │   ├── 丞_40.png\n",
      "│   │   ├── 丞_41.png\n",
      "│   │   ├── 丞_42.png\n",
      "│   │   ├── 丞_43.png\n",
      "│   │   ├── 丞_44.png\n",
      "│   │   ├── 丞_45.png\n",
      "│   │   ├── 丞_46.png\n",
      "│   │   ├── 丞_47.png\n",
      "│   │   ├── 丞_48.png\n",
      "│   │   ├── 丞_49.png\n",
      "│   │   ├── 丞_5.png\n",
      "│   │   ├── 丞_50.png\n",
      "│   │   ├── 丞_6.png\n",
      "│   │   ├── 丞_7.png\n",
      "│   │   ├── 丞_8.png\n",
      "│   │   └── 丞_9.png\n",
      "│   ├── 丟\n",
      "│   │   ├── 丟_0.png\n",
      "│   │   ├── 丟_1.png\n",
      "│   │   ├── 丟_10.png\n",
      "│   │   ├── 丟_11.png\n",
      "│   │   ├── 丟_12.png\n",
      "│   │   ├── 丟_13.png\n",
      "│   │   ├── 丟_14.png\n",
      "│   │   ├── 丟_15.png\n",
      "│   │   ├── 丟_16.png\n",
      "│   │   ├── 丟_17.png\n",
      "│   │   ├── 丟_18.png\n",
      "│   │   ├── 丟_19.png\n",
      "│   │   ├── 丟_2.png\n",
      "│   │   ├── 丟_20.png\n",
      "│   │   ├── 丟_21.png\n",
      "│   │   ├── 丟_22.png\n",
      "│   │   ├── 丟_23.png\n",
      "│   │   ├── 丟_24.png\n",
      "│   │   ├── 丟_25.png\n",
      "│   │   ├── 丟_26.png\n",
      "│   │   ├── 丟_27.png\n",
      "│   │   ├── 丟_28.png\n",
      "│   │   ├── 丟_29.png\n",
      "│   │   ├── 丟_3.png\n",
      "│   │   ├── 丟_30.png\n",
      "│   │   ├── 丟_31.png\n",
      "│   │   ├── 丟_32.png\n",
      "│   │   ├── 丟_33.png\n",
      "│   │   ├── 丟_34.png\n",
      "│   │   ├── 丟_35.png\n",
      "│   │   ├── 丟_36.png\n",
      "│   │   ├── 丟_37.png\n",
      "│   │   ├── 丟_38.png\n",
      "│   │   ├── 丟_39.png\n",
      "│   │   ├── 丟_4.png\n",
      "│   │   ├── 丟_40.png\n",
      "│   │   ├── 丟_41.png\n",
      "│   │   ├── 丟_42.png\n",
      "│   │   ├── 丟_43.png\n",
      "│   │   ├── 丟_44.png\n",
      "│   │   ├── 丟_45.png\n",
      "│   │   ├── 丟_46.png\n",
      "│   │   ├── 丟_47.png\n",
      "│   │   ├── 丟_48.png\n",
      "│   │   ├── 丟_49.png\n",
      "│   │   ├── 丟_5.png\n",
      "│   │   ├── 丟_50.png\n",
      "│   │   ├── 丟_6.png\n",
      "│   │   ├── 丟_7.png\n",
      "│   │   ├── 丟_8.png\n",
      "│   │   └── 丟_9.png\n",
      "│   ├── 並\n",
      "│   │   ├── 並_0.png\n",
      "│   │   ├── 並_1.png\n",
      "│   │   ├── 並_10.png\n",
      "│   │   ├── 並_11.png\n",
      "│   │   ├── 並_12.png\n",
      "│   │   ├── 並_13.png\n",
      "│   │   ├── 並_14.png\n",
      "│   │   ├── 並_15.png\n",
      "│   │   ├── 並_16.png\n",
      "│   │   ├── 並_17.png\n",
      "│   │   ├── 並_18.png\n",
      "│   │   ├── 並_19.png\n",
      "│   │   ├── 並_2.png\n",
      "│   │   ├── 並_20.png\n",
      "│   │   ├── 並_21.png\n",
      "│   │   ├── 並_22.png\n",
      "│   │   ├── 並_23.png\n",
      "│   │   ├── 並_24.png\n",
      "│   │   ├── 並_25.png\n",
      "│   │   ├── 並_26.png\n",
      "│   │   ├── 並_27.png\n",
      "│   │   ├── 並_28.png\n",
      "│   │   ├── 並_29.png\n",
      "│   │   ├── 並_3.png\n",
      "│   │   ├── 並_30.png\n",
      "│   │   ├── 並_31.png\n",
      "│   │   ├── 並_32.png\n",
      "│   │   ├── 並_33.png\n",
      "│   │   ├── 並_34.png\n",
      "│   │   ├── 並_35.png\n",
      "│   │   ├── 並_36.png\n",
      "│   │   ├── 並_37.png\n",
      "│   │   ├── 並_38.png\n",
      "│   │   ├── 並_39.png\n",
      "│   │   ├── 並_4.png\n",
      "│   │   ├── 並_40.png\n",
      "│   │   ├── 並_41.png\n",
      "│   │   ├── 並_42.png\n",
      "│   │   ├── 並_43.png\n",
      "│   │   ├── 並_44.png\n",
      "│   │   ├── 並_45.png\n",
      "│   │   ├── 並_46.png\n",
      "│   │   ├── 並_47.png\n",
      "│   │   ├── 並_48.png\n",
      "│   │   ├── 並_49.png\n",
      "│   │   ├── 並_5.png\n",
      "│   │   ├── 並_50.png\n",
      "│   │   ├── 並_6.png\n",
      "│   │   ├── 並_7.png\n",
      "│   │   ├── 並_8.png\n",
      "│   │   └── 並_9.png\n",
      "│   ├── 丫\n",
      "│   │   ├── 丫_0.png\n",
      "│   │   ├── 丫_1.png\n",
      "│   │   ├── 丫_10.png\n",
      "│   │   ├── 丫_11.png\n",
      "│   │   ├── 丫_12.png\n",
      "│   │   ├── 丫_13.png\n",
      "│   │   ├── 丫_14.png\n",
      "│   │   ├── 丫_15.png\n",
      "│   │   ├── 丫_16.png\n",
      "│   │   ├── 丫_17.png\n",
      "│   │   ├── 丫_18.png\n",
      "│   │   ├── 丫_19.png\n",
      "│   │   ├── 丫_2.png\n",
      "│   │   ├── 丫_20.png\n",
      "│   │   ├── 丫_21.png\n",
      "│   │   ├── 丫_22.png\n",
      "│   │   ├── 丫_23.png\n",
      "│   │   ├── 丫_24.png\n",
      "│   │   ├── 丫_25.png\n",
      "│   │   ├── 丫_26.png\n",
      "│   │   ├── 丫_27.png\n",
      "│   │   ├── 丫_28.png\n",
      "│   │   ├── 丫_29.png\n",
      "│   │   ├── 丫_3.png\n",
      "│   │   ├── 丫_30.png\n",
      "│   │   ├── 丫_31.png\n",
      "│   │   ├── 丫_32.png\n",
      "│   │   ├── 丫_33.png\n",
      "│   │   ├── 丫_34.png\n",
      "│   │   ├── 丫_35.png\n",
      "│   │   ├── 丫_36.png\n",
      "│   │   ├── 丫_37.png\n",
      "│   │   ├── 丫_38.png\n",
      "│   │   ├── 丫_39.png\n",
      "│   │   ├── 丫_4.png\n",
      "│   │   ├── 丫_40.png\n",
      "│   │   ├── 丫_41.png\n",
      "│   │   ├── 丫_42.png\n",
      "│   │   ├── 丫_43.png\n",
      "│   │   ├── 丫_44.png\n",
      "│   │   ├── 丫_45.png\n",
      "│   │   ├── 丫_46.png\n",
      "│   │   ├── 丫_47.png\n",
      "│   │   ├── 丫_48.png\n",
      "│   │   ├── 丫_49.png\n",
      "│   │   ├── 丫_5.png\n",
      "│   │   ├── 丫_50.png\n",
      "│   │   ├── 丫_6.png\n",
      "│   │   ├── 丫_7.png\n",
      "│   │   ├── 丫_8.png\n",
      "│   │   └── 丫_9.png\n",
      "│   ├── 中\n",
      "│   │   ├── 中_0.png\n",
      "│   │   ├── 中_1.png\n",
      "│   │   ├── 中_10.png\n",
      "│   │   ├── 中_11.png\n",
      "│   │   ├── 中_12.png\n",
      "│   │   ├── 中_13.png\n",
      "│   │   ├── 中_14.png\n",
      "│   │   ├── 中_15.png\n",
      "│   │   ├── 中_16.png\n",
      "│   │   ├── 中_17.png\n",
      "│   │   ├── 中_18.png\n",
      "│   │   ├── 中_19.png\n",
      "│   │   ├── 中_2.png\n",
      "│   │   ├── 中_20.png\n",
      "│   │   ├── 中_21.png\n",
      "│   │   ├── 中_22.png\n",
      "│   │   ├── 中_23.png\n",
      "│   │   ├── 中_24.png\n",
      "│   │   ├── 中_25.png\n",
      "│   │   ├── 中_26.png\n",
      "│   │   ├── 中_27.png\n",
      "│   │   ├── 中_28.png\n",
      "│   │   ├── 中_29.png\n",
      "│   │   ├── 中_3.png\n",
      "│   │   ├── 中_30.png\n",
      "│   │   ├── 中_31.png\n",
      "│   │   ├── 中_32.png\n",
      "│   │   ├── 中_33.png\n",
      "│   │   ├── 中_34.png\n",
      "│   │   ├── 中_35.png\n",
      "│   │   ├── 中_36.png\n",
      "│   │   ├── 中_37.png\n",
      "│   │   ├── 中_38.png\n",
      "│   │   ├── 中_39.png\n",
      "│   │   ├── 中_4.png\n",
      "│   │   ├── 中_40.png\n",
      "│   │   ├── 中_41.png\n",
      "│   │   ├── 中_42.png\n",
      "│   │   ├── 中_43.png\n",
      "│   │   ├── 中_44.png\n",
      "│   │   ├── 中_45.png\n",
      "│   │   ├── 中_46.png\n",
      "│   │   ├── 中_47.png\n",
      "│   │   ├── 中_48.png\n",
      "│   │   ├── 中_49.png\n",
      "│   │   ├── 中_5.png\n",
      "│   │   ├── 中_50.png\n",
      "│   │   ├── 中_6.png\n",
      "│   │   ├── 中_7.png\n",
      "│   │   ├── 中_8.png\n",
      "│   │   └── 中_9.png\n",
      "│   ├── 串\n",
      "│   │   ├── 串_0.png\n",
      "│   │   ├── 串_1.png\n",
      "│   │   ├── 串_10.png\n",
      "│   │   ├── 串_11.png\n",
      "│   │   ├── 串_12.png\n",
      "│   │   ├── 串_13.png\n",
      "│   │   ├── 串_14.png\n",
      "│   │   ├── 串_15.png\n",
      "│   │   ├── 串_16.png\n",
      "│   │   ├── 串_17.png\n",
      "... length_limit, 1000, reached, counted:\n",
      "\n",
      "21 directories, 979 files\n"
     ]
    }
   ],
   "source": [
    "tree(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a72771b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'丁': 0, '七': 1, '丈': 2, '三': 3, '上': 4, '下': 5, '不': 6, '丐': 7, '丑': 8, '且': 9, '丕': 10, '世': 11, '丘': 12, '丙': 13, '丞': 14, '丟': 15, '並': 16, '丫': 17, '中': 18, '串': 19, '丸': 20, '丹': 21, '主': 22, '乃': 23, '久': 24, '么': 25, '之': 26, '乍': 27, '乎': 28, '乏': 29, '乒': 30, '乓': 31, '乖': 32, '乘': 33, '乙': 34, '九': 35, '乞': 36, '也': 37, '乩': 38, '乳': 39, '乾': 40, '亂': 41, '了': 42, '予': 43, '事': 44, '二': 45, '于': 46, '云': 47, '互': 48, '五': 49, '井': 50, '亙': 51, '些': 52, '亞': 53, '亟': 54, '亡': 55, '交': 56, '亥': 57, '亦': 58, '亨': 59, '享': 60, '京': 61, '亭': 62, '亮': 63, '人': 64, '什': 65, '仁': 66, '仃': 67, '仄': 68, '仆': 69, '仇': 70, '今': 71, '介': 72, '仍': 73, '仔': 74, '仕': 75, '他': 76, '仗': 77, '付': 78, '仙': 79, '仞': 80, '仟': 81, '代': 82, '令': 83, '以': 84, '仰': 85, '仲': 86, '仳': 87, '件': 88, '任': 89, '份': 90, '仿': 91, '企': 92, '伉': 93, '伊': 94, '伍': 95, '伏': 96, '伐': 97, '休': 98, '伕': 99, '伙': 100, '伯': 101, '估': 102, '伴': 103, '伶': 104, '伸': 105, '伺': 106, '似': 107, '伽': 108, '佃': 109, '但': 110, '佇': 111, '位': 112, '低': 113, '住': 114, '佐': 115, '佑': 116, '佔': 117, '何': 118, '佗': 119, '余': 120, '佛': 121, '作': 122, '佝': 123, '佞': 124, '你': 125, '佣': 126, '佩': 127, '佬': 128, '佯': 129, '佰': 130, '佳': 131, '併': 132, '佻': 133, '佾': 134, '使': 135, '侃': 136, '來': 137, '侈': 138, '例': 139, '侍': 140, '侏': 141, '侖': 142, '供': 143, '依': 144, '侮': 145, '侯': 146, '侵': 147, '侶': 148, '便': 149, '係': 150, '促': 151, '俄': 152, '俊': 153, '俎': 154, '俏': 155, '俐': 156, '俑': 157, '俗': 158, '俘': 159, '俚': 160, '保': 161, '俞': 162, '俟': 163, '俠': 164, '信': 165, '修': 166, '俯': 167, '俱': 168, '俳': 169, '俸': 170, '俺': 171, '俾': 172, '倀': 173, '倆': 174, '倉': 175, '個': 176, '倌': 177, '倍': 178, '倏': 179, '們': 180, '倒': 181, '倔': 182, '倖': 183, '倘': 184, '候': 185, '倚': 186, '借': 187, '倡': 188, '倣': 189, '倥': 190, '倦': 191, '倨': 192, '倩': 193, '倪': 194, '倫': 195, '倭': 196, '值': 197, '偃': 198, '假': 199, '偉': 200, '偌': 201, '偎': 202, '偏': 203, '偕': 204, '做': 205, '停': 206, '健': 207, '側': 208, '偵': 209, '偶': 210, '偷': 211, '偺': 212, '偽': 213, '傀': 214, '傅': 215, '傍': 216, '傑': 217, '傖': 218, '傘': 219, '備': 220, '傢': 221, '催': 222, '傭': 223, '傯': 224, '傲': 225, '傳': 226, '債': 227, '傷': 228, '傻': 229, '傾': 230, '僅': 231, '像': 232, '僑': 233, '僕': 234, '僖': 235, '僚': 236, '僥': 237, '僧': 238, '僭': 239, '僮': 240, '僱': 241, '僵': 242, '價': 243, '僻': 244, '儀': 245, '儂': 246, '億': 247, '儈': 248, '儉': 249, '儐': 250, '儒': 251, '儔': 252, '儘': 253, '償': 254, '儡': 255, '優': 256, '儲': 257, '儷': 258, '儼': 259, '兀': 260, '允': 261, '元': 262, '兄': 263, '充': 264, '兆': 265, '兇': 266, '先': 267, '光': 268, '克': 269, '兌': 270, '免': 271, '兒': 272, '兔': 273, '兕': 274, '兗': 275, '兜': 276, '兢': 277, '入': 278, '內': 279, '全': 280, '兩': 281, '八': 282, '公': 283, '六': 284, '兮': 285, '共': 286, '兵': 287, '其': 288, '具': 289, '典': 290, '兼': 291, '冀': 292, '冉': 293, '冊': 294, '再': 295, '冑': 296, '冒': 297, '冕': 298, '冗': 299, '冠': 300, '冢': 301, '冤': 302, '冥': 303, '冬': 304, '冰': 305, '冶': 306, '冷': 307, '冽': 308, '准': 309, '凋': 310, '凌': 311, '凍': 312, '凜': 313, '凝': 314, '几': 315, '凡': 316, '凰': 317, '凱': 318, '凳': 319, '凶': 320, '凸': 321, '凹': 322, '出': 323, '函': 324, '刀': 325, '刁': 326, '刃': 327, '分': 328, '切': 329, '刈': 330, '刊': 331, '刎': 332, '刑': 333, '划': 334, '列': 335, '初': 336, '判': 337, '別': 338, '刨': 339, '利': 340, '刪': 341, '刮': 342, '到': 343, '制': 344, '刷': 345, '券': 346, '刻': 347, '剁': 348, '剃': 349, '則': 350, '削': 351, '剋': 352, '剌': 353, '前': 354, '剎': 355, '剔': 356, '剖': 357, '剛': 358, '剜': 359, '剝': 360, '剩': 361, '剪': 362, '副': 363, '割': 364, '剴': 365, '創': 366, '剷': 367, '剽': 368, '剿': 369, '劃': 370, '劇': 371, '劈': 372, '劉': 373, '劍': 374, '劑': 375, '力': 376, '功': 377, '加': 378, '劣': 379, '助': 380, '努': 381, '劫': 382, '劬': 383, '劾': 384, '勁': 385, '勃': 386, '勇': 387, '勉': 388, '勒': 389, '動': 390, '勗': 391, '勘': 392, '務': 393, '勛': 394, '勝': 395, '勞': 396, '募': 397, '勢': 398, '勤': 399, '勦': 400, '勵': 401, '勸': 402, '勻': 403, '勾': 404, '勿': 405, '包': 406, '匆': 407, '匈': 408, '匍': 409, '匏': 410, '匐': 411, '匕': 412, '化': 413, '北': 414, '匙': 415, '匝': 416, '匠': 417, '匡': 418, '匣': 419, '匪': 420, '匯': 421, '匱': 422, '匹': 423, '匾': 424, '匿': 425, '區': 426, '十': 427, '千': 428, '卅': 429, '升': 430, '午': 431, '卉': 432, '半': 433, '卑': 434, '卒': 435, '卓': 436, '協': 437, '南': 438, '博': 439, '卜': 440, '卞': 441, '占': 442, '卡': 443, '卦': 444, '卮': 445, '卯': 446, '印': 447, '危': 448, '即': 449, '卵': 450, '卷': 451, '卸': 452, '卹': 453, '卻': 454, '卿': 455, '厄': 456, '厚': 457, '厝': 458, '原': 459, '厥': 460, '厭': 461, '厲': 462, '去': 463, '參': 464, '又': 465, '叉': 466, '及': 467, '友': 468, '反': 469, '叔': 470, '取': 471, '受': 472, '叛': 473, '叟': 474, '叢': 475, '口': 476, '古': 477, '句': 478, '另': 479, '叨': 480, '叩': 481, '只': 482, '叫': 483, '召': 484, '叭': 485, '叮': 486, '可': 487, '台': 488, '叱': 489, '史': 490, '右': 491, '叵': 492, '司': 493, '叼': 494, '吁': 495, '吃': 496, '各': 497, '吆': 498, '合': 499, '吉': 500, '吊': 501, '吋': 502, '同': 503, '名': 504, '后': 505, '吏': 506, '吐': 507, '向': 508, '吒': 509, '君': 510, '吝': 511, '吞': 512, '吟': 513, '吠': 514, '否': 515, '吧': 516, '吩': 517, '含': 518, '吭': 519, '吮': 520, '吱': 521, '吳': 522, '吵': 523, '吶': 524, '吸': 525, '吹': 526, '吻': 527, '吼': 528, '吾': 529, '呀': 530, '呂': 531, '呃': 532, '呆': 533, '呈': 534, '告': 535, '呎': 536, '呢': 537, '周': 538, '呱': 539, '味': 540, '呵': 541, '呶': 542, '呷': 543, '呸': 544, '呻': 545, '呼': 546, '命': 547, '咀': 548, '咄': 549, '咆': 550, '咋': 551, '和': 552, '咎': 553, '咐': 554, '咒': 555, '咕': 556, '咖': 557, '咚': 558, '咦': 559, '咨': 560, '咪': 561, '咫': 562, '咬': 563, '咯': 564, '咱': 565, '咳': 566, '咸': 567, '咻': 568, '咽': 569, '哀': 570, '品': 571, '哂': 572, '哄': 573, '哇': 574, '哈': 575, '哉': 576, '哎': 577, '員': 578, '哥': 579, '哦': 580, '哨': 581, '哩': 582, '哪': 583, '哭': 584, '哮': 585, '哲': 586, '哺': 587, '哼': 588, '唁': 589, '唆': 590, '唉': 591, '唐': 592, '唔': 593, '唧': 594, '唬': 595, '售': 596, '唯': 597, '唱': 598, '唳': 599, '唷': 600, '唸': 601, '唾': 602, '啃': 603, '啄': 604, '商': 605, '啊': 606, '問': 607, '啕': 608, '啖': 609, '啜': 610, '啞': 611, '啟': 612, '啡': 613, '啣': 614, '啤': 615, '啦': 616, '啪': 617, '啻': 618, '啼': 619, '啾': 620, '喀': 621, '喂': 622, '喃': 623, '善': 624, '喇': 625, '喉': 626, '喊': 627, '喋': 628, '喔': 629, '喘': 630, '喚': 631, '喜': 632, '喝': 633, '喟': 634, '喧': 635, '喪': 636, '喬': 637, '單': 638, '喱': 639, '喲': 640, '喳': 641, '喻': 642, '嗅': 643, '嗆': 644, '嗇': 645, '嗎': 646, '嗑': 647, '嗓': 648, '嗚': 649, '嗜': 650, '嗟': 651, '嗡': 652, '嗣': 653, '嗤': 654, '嗥': 655, '嗦': 656, '嗨': 657, '嗯': 658, '嗷': 659, '嗽': 660, '嗾': 661, '嘀': 662, '嘆': 663, '嘈': 664, '嘉': 665, '嘍': 666, '嘎': 667, '嘔': 668, '嘖': 669, '嘗': 670, '嘛': 671, '嘟': 672, '嘩': 673, '嘮': 674, '嘯': 675, '嘰': 676, '嘲': 677, '嘴': 678, '嘶': 679, '嘹': 680, '嘻': 681, '嘿': 682, '噎': 683, '噓': 684, '噗': 685, '噙': 686, '噢': 687, '噤': 688, '噥': 689, '器': 690, '噩': 691, '噪': 692, '噫': 693, '噬': 694, '噯': 695, '噱': 696, '噴': 697, '噸': 698, '噹': 699, '嚀': 700, '嚅': 701, '嚇': 702, '嚎': 703, '嚏': 704, '嚐': 705, '嚕': 706, '嚥': 707, '嚨': 708, '嚮': 709, '嚴': 710, '嚶': 711, '嚷': 712, '嚼': 713, '囀': 714, '囁': 715, '囂': 716, '囈': 717, '囉': 718, '囊': 719, '囌': 720, '囑': 721, '囚': 722, '四': 723, '回': 724, '因': 725, '囤': 726, '囪': 727, '困': 728, '固': 729, '圃': 730, '圈': 731, '國': 732, '圍': 733, '園': 734, '圓': 735, '圖': 736, '團': 737, '土': 738, '在': 739, '圬': 740, '圭': 741, '圯': 742, '地': 743, '圳': 744, '圾': 745, '址': 746, '均': 747, '坊': 748, '坍': 749, '坎': 750, '坏': 751, '坐': 752, '坑': 753, '坡': 754, '坤': 755, '坦': 756, '坩': 757, '坪': 758, '坷': 759, '坼': 760, '垂': 761, '垃': 762, '型': 763, '垠': 764, '垢': 765, '垣': 766, '垮': 767, '埂': 768, '埃': 769, '埋': 770, '城': 771, '埔': 772, '域': 773, '埠': 774, '埤': 775, '執': 776, '培': 777, '基': 778, '堂': 779, '堅': 780, '堆': 781, '堊': 782, '堡': 783, '堤': 784, '堪': 785, '堯': 786, '堰': 787, '報': 788, '場': 789, '堵': 790, '塊': 791, '塌': 792, '塑': 793, '塔': 794, '塗': 795, '塘': 796, '塚': 797, '塞': 798, '塢': 799, '填': 800, '塭': 801, '塵': 802, '塹': 803, '塾': 804, '墀': 805, '境': 806, '墅': 807, '墊': 808, '墓': 809, '墜': 810, '增': 811, '墟': 812, '墨': 813, '墮': 814, '墳': 815, '墾': 816, '壁': 817, '壅': 818, '壇': 819, '壑': 820, '壓': 821, '壕': 822, '壘': 823, '壙': 824, '壞': 825, '壟': 826, '壢': 827, '壤': 828, '壩': 829, '士': 830, '壬': 831, '壯': 832, '壹': 833, '壺': 834, '壽': 835, '夏': 836, '夔': 837, '夕': 838, '外': 839, '夙': 840, '多': 841, '夜': 842, '夠': 843, '夢': 844, '夤': 845, '夥': 846, '大': 847, '天': 848, '太': 849, '夫': 850, '夭': 851, '央': 852, '失': 853, '夷': 854, '夸': 855, '夾': 856, '奄': 857, '奇': 858, '奈': 859, '奉': 860, '奎': 861, '奏': 862, '奐': 863, '契': 864, '奔': 865, '奕': 866, '套': 867, '奘': 868, '奚': 869, '奠': 870, '奢': 871, '奧': 872, '奩': 873, '奪': 874, '奮': 875, '女': 876, '奴': 877, '奶': 878, '奸': 879, '她': 880, '好': 881, '妁': 882, '如': 883, '妃': 884, '妄': 885, '妊': 886, '妍': 887, '妒': 888, '妓': 889, '妖': 890, '妙': 891, '妝': 892, '妞': 893, '妣': 894, '妤': 895, '妥': 896, '妨': 897, '妮': 898, '妯': 899, '妳': 900, '妹': 901, '妻': 902, '妾': 903, '姆': 904, '姊': 905, '始': 906, '姍': 907, '姐': 908, '姑': 909, '姒': 910, '姓': 911, '委': 912, '姘': 913, '姚': 914, '姜': 915, '姣': 916, '姥': 917, '姦': 918, '姨': 919, '姪': 920, '姬': 921, '姻': 922, '姿': 923, '威': 924, '娃': 925, '娌': 926, '娑': 927, '娓': 928, '娘': 929, '娛': 930, '娜': 931, '娟': 932, '娠': 933, '娣': 934, '娥': 935, '娩': 936, '娶': 937, '娼': 938, '婀': 939, '婁': 940, '婆': 941, '婉': 942, '婊': 943, '婚': 944, '婢': 945, '婦': 946, '婪': 947, '婷': 948, '婿': 949, '媒': 950, '媚': 951, '媛': 952, '媲': 953, '媳': 954, '媼': 955, '媽': 956, '媾': 957, '嫁': 958, '嫂': 959, '嫉': 960, '嫌': 961, '嫖': 962, '嫗': 963, '嫘': 964, '嫡': 965, '嫣': 966, '嫦': 967, '嫩': 968, '嫵': 969, '嫻': 970, '嬉': 971, '嬋': 972, '嬌': 973, '嬝': 974, '嬤': 975, '嬪': 976, '嬰': 977, '嬴': 978, '嬸': 979, '孀': 980, '子': 981, '孑': 982, '孓': 983, '孔': 984, '孕': 985, '字': 986, '存': 987, '孚': 988, '孜': 989, '孝': 990, '孟': 991, '季': 992, '孤': 993, '孩': 994, '孫': 995, '孰': 996, '孱': 997, '孳': 998, '孵': 999, '學': 1000, '孺': 1001, '孽': 1002, '孿': 1003, '它': 1004, '宅': 1005, '宇': 1006, '守': 1007, '安': 1008, '宋': 1009, '完': 1010, '宏': 1011, '宗': 1012, '官': 1013, '宙': 1014, '定': 1015, '宛': 1016, '宜': 1017, '客': 1018, '宣': 1019, '室': 1020, '宥': 1021, '宦': 1022, '宮': 1023, '宰': 1024, '害': 1025, '宴': 1026, '宵': 1027, '家': 1028, '宸': 1029, '容': 1030, '宿': 1031, '寂': 1032, '寄': 1033, '寅': 1034, '密': 1035, '寇': 1036, '富': 1037, '寐': 1038, '寒': 1039, '寓': 1040, '寞': 1041, '察': 1042, '寡': 1043, '寢': 1044, '寤': 1045, '寥': 1046, '實': 1047, '寧': 1048, '寨': 1049, '審': 1050, '寫': 1051, '寬': 1052, '寮': 1053, '寵': 1054, '寶': 1055, '寸': 1056, '寺': 1057, '封': 1058, '射': 1059, '將': 1060, '專': 1061, '尉': 1062, '尊': 1063, '尋': 1064, '對': 1065, '導': 1066, '小': 1067, '少': 1068, '尖': 1069, '尚': 1070, '尤': 1071, '尬': 1072, '就': 1073, '尷': 1074, '尸': 1075, '尹': 1076, '尺': 1077, '尼': 1078, '尾': 1079, '尿': 1080, '局': 1081, '屁': 1082, '居': 1083, '屆': 1084, '屈': 1085, '屋': 1086, '屍': 1087, '屎': 1088, '屏': 1089, '屐': 1090, '屑': 1091, '展': 1092, '屜': 1093, '屠': 1094, '屢': 1095, '層': 1096, '履': 1097, '屬': 1098, '屯': 1099, '山': 1100, '屹': 1101, '岌': 1102, '岐': 1103, '岑': 1104, '岔': 1105, '岡': 1106, '岩': 1107, '岫': 1108, '岱': 1109, '岳': 1110, '岷': 1111, '岸': 1112, '峙': 1113, '峨': 1114, '峪': 1115, '峭': 1116, '峰': 1117, '島': 1118, '峻': 1119, '峽': 1120, '崁': 1121, '崆': 1122, '崇': 1123, '崎': 1124, '崑': 1125, '崔': 1126, '崖': 1127, '崙': 1128, '崛': 1129, '崢': 1130, '崩': 1131, '嵌': 1132, '嵐': 1133, '嵩': 1134, '嶄': 1135, '嶇': 1136, '嶝': 1137, '嶺': 1138, '嶼': 1139, '嶽': 1140, '巍': 1141, '巒': 1142, '巔': 1143, '巖': 1144, '川': 1145, '州': 1146, '巡': 1147, '巢': 1148, '工': 1149, '左': 1150, '巧': 1151, '巨': 1152, '巫': 1153, '差': 1154, '己': 1155, '已': 1156, '巳': 1157, '巴': 1158, '巷': 1159, '巽': 1160, '巾': 1161, '市': 1162, '布': 1163, '帆': 1164, '希': 1165, '帑': 1166, '帕': 1167, '帖': 1168, '帘': 1169, '帚': 1170, '帛': 1171, '帝': 1172, '帥': 1173, '師': 1174, '席': 1175, '帳': 1176, '帶': 1177, '帷': 1178, '常': 1179, '帽': 1180, '幀': 1181, '幅': 1182, '幌': 1183, '幔': 1184, '幗': 1185, '幛': 1186, '幟': 1187, '幢': 1188, '幣': 1189, '幫': 1190, '干': 1191, '平': 1192, '年': 1193, '并': 1194, '幸': 1195, '幹': 1196, '幻': 1197, '幼': 1198, '幽': 1199, '幾': 1200, '庇': 1201, '床': 1202, '序': 1203, '底': 1204, '庖': 1205, '店': 1206, '庚': 1207, '府': 1208, '庠': 1209, '度': 1210, '座': 1211, '庫': 1212, '庭': 1213, '庵': 1214, '庶': 1215, '康': 1216, '庸': 1217, '庾': 1218, '廁': 1219, '廂': 1220, '廈': 1221, '廉': 1222, '廊': 1223, '廓': 1224, '廖': 1225, '廚': 1226, '廝': 1227, '廟': 1228, '廠': 1229, '廢': 1230, '廣': 1231, '廬': 1232, '廳': 1233, '延': 1234, '廷': 1235, '建': 1236, '廿': 1237, '弁': 1238, '弄': 1239, '弈': 1240, '弊': 1241, '式': 1242, '弒': 1243, '弓': 1244, '弔': 1245, '引': 1246, '弗': 1247, '弘': 1248, '弛': 1249, '弟': 1250, '弦': 1251, '弧': 1252, '弩': 1253, '弭': 1254, '弱': 1255, '張': 1256, '強': 1257, '弼': 1258, '彆': 1259, '彈': 1260, '彌': 1261, '彎': 1262, '彗': 1263, '彙': 1264, '形': 1265, '彤': 1266, '彥': 1267, '彩': 1268, '彪': 1269, '彫': 1270, '彬': 1271, '彭': 1272, '彰': 1273, '影': 1274, '彷': 1275, '役': 1276, '彼': 1277, '彿': 1278, '往': 1279, '征': 1280, '待': 1281, '徇': 1282, '很': 1283, '徊': 1284, '律': 1285, '後': 1286, '徐': 1287, '徑': 1288, '徒': 1289, '得': 1290, '徘': 1291, '徙': 1292, '從': 1293, '御': 1294, '徨': 1295, '復': 1296, '循': 1297, '徬': 1298, '微': 1299, '徵': 1300, '德': 1301, '徹': 1302, '徽': 1303, '心': 1304, '必': 1305, '忌': 1306, '忍': 1307, '忖': 1308, '志': 1309, '忘': 1310, '忙': 1311, '忝': 1312, '忠': 1313, '快': 1314, '忱': 1315, '念': 1316, '忽': 1317, '忿': 1318, '怎': 1319, '怏': 1320, '怒': 1321, '怔': 1322, '怕': 1323, '怖': 1324, '思': 1325, '怠': 1326, '怡': 1327, '急': 1328, '性': 1329, '怨': 1330, '怪': 1331, '怯': 1332, '怵': 1333, '恃': 1334, '恆': 1335, '恍': 1336, '恐': 1337, '恕': 1338, '恙': 1339, '恢': 1340, '恣': 1341, '恤': 1342, '恥': 1343, '恨': 1344, '恩': 1345, '恪': 1346, '恫': 1347, '恬': 1348, '恭': 1349, '息': 1350, '恰': 1351, '恿': 1352, '悄': 1353, '悅': 1354, '悉': 1355, '悌': 1356, '悍': 1357, '悔': 1358, '悖': 1359, '悚': 1360, '悟': 1361, '悠': 1362, '患': 1363, '您': 1364, '悲': 1365, '悴': 1366, '悵': 1367, '悶': 1368, '悸': 1369, '悻': 1370, '悼': 1371, '悽': 1372, '情': 1373, '惆': 1374, '惋': 1375, '惑': 1376, '惕': 1377, '惘': 1378, '惚': 1379, '惜': 1380, '惟': 1381, '惠': 1382, '惡': 1383, '惦': 1384, '惰': 1385, '惱': 1386, '想': 1387, '惴': 1388, '惶': 1389, '惹': 1390, '惺': 1391, '惻': 1392, '愀': 1393, '愁': 1394, '愈': 1395, '愉': 1396, '愎': 1397, '意': 1398, '愕': 1399, '愚': 1400, '愛': 1401, '愜': 1402, '感': 1403, '愣': 1404, '愧': 1405, '愴': 1406, '愾': 1407, '愿': 1408, '慄': 1409, '慇': 1410, '慈': 1411, '態': 1412, '慌': 1413, '慍': 1414, '慎': 1415, '慕': 1416, '慘': 1417, '慚': 1418, '慝': 1419, '慟': 1420, '慢': 1421, '慣': 1422, '慧': 1423, '慨': 1424, '慫': 1425, '慮': 1426, '慰': 1427, '慶': 1428, '慷': 1429, '慼': 1430, '慾': 1431, '憂': 1432, '憊': 1433, '憎': 1434, '憐': 1435, '憑': 1436, '憔': 1437, '憚': 1438, '憤': 1439, '憧': 1440, '憩': 1441, '憫': 1442, '憬': 1443, '憲': 1444, '憶': 1445, '憾': 1446, '懂': 1447, '懇': 1448, '懈': 1449, '應': 1450, '懊': 1451, '懍': 1452, '懣': 1453, '懦': 1454, '懲': 1455, '懵': 1456, '懶': 1457, '懷': 1458, '懸': 1459, '懺': 1460, '懼': 1461, '懾': 1462, '懿': 1463, '戀': 1464, '戈': 1465, '戊': 1466, '戌': 1467, '戍': 1468, '戎': 1469, '成': 1470, '我': 1471, '戒': 1472, '戕': 1473, '或': 1474, '戚': 1475, '戛': 1476, '戟': 1477, '戡': 1478, '戢': 1479, '截': 1480, '戮': 1481, '戰': 1482, '戲': 1483, '戳': 1484, '戴': 1485, '戶': 1486, '戾': 1487, '房': 1488, '所': 1489, '扁': 1490, '扇': 1491, '扈': 1492, '扉': 1493, '手': 1494, '才': 1495, '扎': 1496, '扒': 1497, '打': 1498, '扔': 1499, '托': 1500, '扛': 1501, '扣': 1502, '扭': 1503, '扮': 1504, '扯': 1505, '扳': 1506, '扶': 1507, '批': 1508, '扼': 1509, '找': 1510, '承': 1511, '技': 1512, '抄': 1513, '抉': 1514, '把': 1515, '抑': 1516, '抒': 1517, '抓': 1518, '投': 1519, '抖': 1520, '抗': 1521, '折': 1522, '抨': 1523, '披': 1524, '抬': 1525, '抱': 1526, '抵': 1527, '抹': 1528, '押': 1529, '抽': 1530, '抿': 1531, '拂': 1532, '拄': 1533, '拆': 1534, '拇': 1535, '拈': 1536, '拉': 1537, '拋': 1538, '拌': 1539, '拍': 1540, '拎': 1541, '拐': 1542, '拒': 1543, '拓': 1544, '拔': 1545, '拖': 1546, '拗': 1547, '拘': 1548, '拙': 1549, '拚': 1550, '招': 1551, '拜': 1552, '括': 1553, '拭': 1554, '拮': 1555, '拯': 1556, '拱': 1557, '拳': 1558, '拴': 1559, '拷': 1560, '拼': 1561, '拽': 1562, '拾': 1563, '拿': 1564, '持': 1565, '指': 1566, '挈': 1567, '按': 1568, '挑': 1569, '挖': 1570, '挨': 1571, '挪': 1572, '挫': 1573, '振': 1574, '挺': 1575, '挽': 1576, '挾': 1577, '捂': 1578, '捆': 1579, '捉': 1580, '捎': 1581, '捏': 1582, '捐': 1583, '捕': 1584, '捧': 1585, '捨': 1586, '捩': 1587, '捫': 1588, '捱': 1589, '捲': 1590, '捶': 1591, '捷': 1592, '捻': 1593, '掀': 1594, '掃': 1595, '掄': 1596, '授': 1597, '掉': 1598, '掌': 1599, '掏': 1600, '排': 1601, '掖': 1602, '掘': 1603, '掙': 1604, '掛': 1605, '掠': 1606, '採': 1607, '探': 1608, '掣': 1609, '接': 1610, '控': 1611, '推': 1612, '掩': 1613, '措': 1614, '掬': 1615, '揀': 1616, '揆': 1617, '揉': 1618, '揍': 1619, '描': 1620, '提': 1621, '插': 1622, '揖': 1623, '揚': 1624, '換': 1625, '握': 1626, '揣': 1627, '揩': 1628, '揪': 1629, '揭': 1630, '揮': 1631, '援': 1632, '損': 1633, '搏': 1634, '搓': 1635, '搔': 1636, '搖': 1637, '搗': 1638, '搜': 1639, '搞': 1640, '搪': 1641, '搬': 1642, '搭': 1643, '搶': 1644, '搽': 1645, '搾': 1646, '摑': 1647, '摒': 1648, '摔': 1649, '摘': 1650, '摟': 1651, '摧': 1652, '摩': 1653, '摯': 1654, '摸': 1655, '摹': 1656, '摺': 1657, '撇': 1658, '撈': 1659, '撐': 1660, '撒': 1661, '撓': 1662, '撕': 1663, '撚': 1664, '撞': 1665, '撤': 1666, '撥': 1667, '撩': 1668, '撫': 1669, '撬': 1670, '播': 1671, '撮': 1672, '撰': 1673, '撲': 1674, '撻': 1675, '撼': 1676, '撿': 1677, '擁': 1678, '擂': 1679, '擄': 1680, '擅': 1681, '擇': 1682, '擊': 1683, '擋': 1684, '操': 1685, '擎': 1686, '擒': 1687, '擔': 1688, '擘': 1689, '據': 1690, '擠': 1691, '擦': 1692, '擬': 1693, '擰': 1694, '擱': 1695, '擲': 1696, '擴': 1697, '擺': 1698, '擻': 1699, '擾': 1700, '攀': 1701, '攆': 1702, '攏': 1703, '攔': 1704, '攘': 1705, '攙': 1706, '攜': 1707, '攝': 1708, '攣': 1709, '攤': 1710, '攪': 1711, '攫': 1712, '攬': 1713, '支': 1714, '收': 1715, '改': 1716, '攻': 1717, '放': 1718, '政': 1719, '故': 1720, '效': 1721, '敏': 1722, '救': 1723, '敖': 1724, '敗': 1725, '敘': 1726, '教': 1727, '敝': 1728, '敞': 1729, '敢': 1730, '散': 1731, '敦': 1732, '敬': 1733, '敲': 1734, '整': 1735, '敵': 1736, '敷': 1737, '數': 1738, '斂': 1739, '斃': 1740, '文': 1741, '斐': 1742, '斑': 1743, '斗': 1744, '料': 1745, '斜': 1746, '斟': 1747, '斡': 1748, '斤': 1749, '斥': 1750, '斧': 1751, '斫': 1752, '斬': 1753, '斯': 1754, '新': 1755, '斷': 1756, '方': 1757, '於': 1758, '施': 1759, '旁': 1760, '旅': 1761, '旋': 1762, '旌': 1763, '旎': 1764, '族': 1765, '旖': 1766, '旗': 1767, '既': 1768, '日': 1769, '旦': 1770, '旨': 1771, '早': 1772, '旬': 1773, '旭': 1774, '旱': 1775, '旺': 1776, '昀': 1777, '昂': 1778, '昆': 1779, '昌': 1780, '明': 1781, '昏': 1782, '易': 1783, '昔': 1784, '星': 1785, '映': 1786, '春': 1787, '昧': 1788, '昨': 1789, '昭': 1790, '是': 1791, '時': 1792, '晃': 1793, '晉': 1794, '晌': 1795, '晏': 1796, '晒': 1797, '晚': 1798, '晝': 1799, '晤': 1800, '晦': 1801, '晨': 1802, '普': 1803, '景': 1804, '晰': 1805, '晴': 1806, '晶': 1807, '智': 1808, '暇': 1809, '暈': 1810, '暉': 1811, '暑': 1812, '暖': 1813, '暗': 1814, '暢': 1815, '暨': 1816, '暫': 1817, '暮': 1818, '暴': 1819, '暹': 1820, '曆': 1821, '曉': 1822, '曖': 1823, '曙': 1824, '曝': 1825, '曠': 1826, '曦': 1827, '曰': 1828, '曲': 1829, '曳': 1830, '更': 1831, '曷': 1832, '書': 1833, '曹': 1834, '曼': 1835, '曾': 1836, '替': 1837, '最': 1838, '會': 1839, '月': 1840, '有': 1841, '朋': 1842, '服': 1843, '朔': 1844, '朕': 1845, '朗': 1846, '望': 1847, '朝': 1848, '期': 1849, '朦': 1850, '朧': 1851, '木': 1852, '未': 1853, '末': 1854, '本': 1855, '札': 1856, '朮': 1857, '朱': 1858, '朴': 1859, '朵': 1860, '朽': 1861, '杉': 1862, '李': 1863, '杏': 1864, '材': 1865, '村': 1866, '杖': 1867, '杜': 1868, '杞': 1869, '束': 1870, '杭': 1871, '杯': 1872, '杰': 1873, '東': 1874, '杳': 1875, '杵': 1876, '杷': 1877, '松': 1878, '板': 1879, '枇': 1880, '枉': 1881, '枋': 1882, '析': 1883, '枕': 1884, '林': 1885, '枚': 1886, '果': 1887, '枝': 1888, '枯': 1889, '枴': 1890, '架': 1891, '枸': 1892, '柄': 1893, '柏': 1894, '某': 1895, '柑': 1896, '染': 1897, '柔': 1898, '柚': 1899, '柞': 1900, '查': 1901, '柩': 1902, '柬': 1903, '柯': 1904, '柱': 1905, '柳': 1906, '柴': 1907, '柵': 1908, '柿': 1909, '栓': 1910, '栗': 1911, '校': 1912, '栩': 1913, '株': 1914, '核': 1915, '根': 1916, '格': 1917, '栽': 1918, '桀': 1919, '桂': 1920, '桃': 1921, '桅': 1922, '框': 1923, '案': 1924, '桌': 1925, '桐': 1926, '桑': 1927, '桓': 1928, '桔': 1929, '桶': 1930, '桿': 1931, '梁': 1932, '梃': 1933, '梅': 1934, '梆': 1935, '梓': 1936, '梔': 1937, '梗': 1938, '條': 1939, '梟': 1940, '梢': 1941, '梧': 1942, '梨': 1943, '梭': 1944, '梯': 1945, '械': 1946, '梱': 1947, '梳': 1948, '梵': 1949, '棄': 1950, '棉': 1951, '棋': 1952, '棍': 1953, '棒': 1954, '棕': 1955, '棗': 1956, '棘': 1957, '棚': 1958, '棟': 1959, '棠': 1960, '棣': 1961, '棧': 1962, '森': 1963, '棲': 1964, '棵': 1965, '棹': 1966, '棺': 1967, '椅': 1968, '植': 1969, '椎': 1970, '椒': 1971, '椰': 1972, '楊': 1973, '楓': 1974, '楔': 1975, '楚': 1976, '楞': 1977, '楠': 1978, '楨': 1979, '楫': 1980, '業': 1981, '極': 1982, '楷': 1983, '楹': 1984, '概': 1985, '榆': 1986, '榔': 1987, '榕': 1988, '榛': 1989, '榜': 1990, '榨': 1991, '榫': 1992, '榭': 1993, '榮': 1994, '榴': 1995, '榷': 1996, '榻': 1997, '槁': 1998, '構': 1999, '槌': 2000, '槍': 2001, '槐': 2002, '槓': 2003, '槨': 2004, '槳': 2005, '槽': 2006, '樁': 2007, '樂': 2008, '樅': 2009, '樊': 2010, '樓': 2011, '標': 2012, '樞': 2013, '樟': 2014, '模': 2015, '樣': 2016, '樵': 2017, '樸': 2018, '樹': 2019, '樺': 2020, '樽': 2021, '橄': 2022, '橇': 2023, '橋': 2024, '橘': 2025, '橙': 2026, '機': 2027, '橡': 2028, '橢': 2029, '橫': 2030, '檀': 2031, '檄': 2032, '檔': 2033, '檜': 2034, '檢': 2035, '檬': 2036, '檳': 2037, '檸': 2038, '檻': 2039, '櫂': 2040, '櫃': 2041, '櫓': 2042, '櫚': 2043, '櫛': 2044, '櫝': 2045, '櫥': 2046, '櫻': 2047, '欄': 2048, '權': 2049, '欖': 2050, '欠': 2051, '次': 2052, '欣': 2053, '欲': 2054, '欺': 2055, '欽': 2056, '款': 2057, '歇': 2058, '歉': 2059, '歌': 2060, '歐': 2061, '歙': 2062, '歟': 2063, '歡': 2064, '止': 2065, '正': 2066, '此': 2067, '步': 2068, '武': 2069, '歧': 2070, '歪': 2071, '歲': 2072, '歷': 2073, '歸': 2074, '歹': 2075, '死': 2076, '歿': 2077, '殃': 2078, '殆': 2079, '殉': 2080, '殊': 2081, '殖': 2082, '殘': 2083, '殤': 2084, '殮': 2085, '殯': 2086, '殲': 2087, '段': 2088, '殷': 2089, '殺': 2090, '殼': 2091, '殿': 2092, '毀': 2093, '毅': 2094, '毆': 2095, '毋': 2096, '母': 2097, '每': 2098, '毒': 2099, '毓': 2100, '比': 2101, '毗': 2102, '毛': 2103, '毫': 2104, '毯': 2105, '毽': 2106, '氏': 2107, '氐': 2108, '民': 2109, '氓': 2110, '氖': 2111, '氛': 2112, '氟': 2113, '氣': 2114, '氤': 2115, '氦': 2116, '氧': 2117, '氨': 2118, '氫': 2119, '氮': 2120, '氯': 2121, '氳': 2122, '水': 2123, '永': 2124, '氾': 2125, '汀': 2126, '汁': 2127, '求': 2128, '汐': 2129, '汕': 2130, '汗': 2131, '汙': 2132, '汝': 2133, '汞': 2134, '江': 2135, '池': 2136, '汨': 2137, '汪': 2138, '汰': 2139, '汲': 2140, '決': 2141, '汽': 2142, '汾': 2143, '沁': 2144, '沃': 2145, '沅': 2146, '沈': 2147, '沉': 2148, '沌': 2149, '沐': 2150, '沒': 2151, '沖': 2152, '沙': 2153, '沛': 2154, '沫': 2155, '沮': 2156, '沱': 2157, '河': 2158, '沸': 2159, '油': 2160, '治': 2161, '沼': 2162, '沽': 2163, '沾': 2164, '沿': 2165, '況': 2166, '泄': 2167, '泅': 2168, '泉': 2169, '泊': 2170, '泌': 2171, '泓': 2172, '法': 2173, '泗': 2174, '泛': 2175, '泡': 2176, '波': 2177, '泣': 2178, '泥': 2179, '注': 2180, '泰': 2181, '泱': 2182, '泳': 2183, '洋': 2184, '洌': 2185, '洗': 2186, '洛': 2187, '洞': 2188, '津': 2189, '洪': 2190, '洱': 2191, '洲': 2192, '洶': 2193, '活': 2194, '洽': 2195, '派': 2196, '流': 2197, '浙': 2198, '浚': 2199, '浦': 2200, '浩': 2201, '浪': 2202, '浬': 2203, '浮': 2204, '浴': 2205, '海': 2206, '浸': 2207, '涇': 2208, '消': 2209, '涉': 2210, '涎': 2211, '涓': 2212, '涕': 2213, '涮': 2214, '涯': 2215, '液': 2216, '涵': 2217, '涸': 2218, '涼': 2219, '淄': 2220, '淅': 2221, '淆': 2222, '淇': 2223, '淋': 2224, '淌': 2225, '淑': 2226, '淒': 2227, '淘': 2228, '淙': 2229, '淚': 2230, '淞': 2231, '淡': 2232, '淤': 2233, '淨': 2234, '淪': 2235, '淫': 2236, '淮': 2237, '深': 2238, '淳': 2239, '淵': 2240, '混': 2241, '淹': 2242, '淺': 2243, '添': 2244, '清': 2245, '渙': 2246, '渚': 2247, '減': 2248, '渝': 2249, '渠': 2250, '渡': 2251, '渣': 2252, '渤': 2253, '渥': 2254, '渦': 2255, '測': 2256, '渭': 2257, '港': 2258, '渲': 2259, '渴': 2260, '游': 2261, '渺': 2262, '渾': 2263, '湃': 2264, '湊': 2265, '湍': 2266, '湔': 2267, '湖': 2268, '湘': 2269, '湛': 2270, '湧': 2271, '湮': 2272, '湯': 2273, '溉': 2274, '源': 2275, '準': 2276, '溘': 2277, '溜': 2278, '溝': 2279, '溢': 2280, '溥': 2281, '溪': 2282, '溫': 2283, '溯': 2284, '溶': 2285, '溺': 2286, '溼': 2287, '滂': 2288, '滄': 2289, '滅': 2290, '滇': 2291, '滋': 2292, '滌': 2293, '滑': 2294, '滓': 2295, '滔': 2296, '滬': 2297, '滯': 2298, '滲': 2299, '滴': 2300, '滾': 2301, '滿': 2302, '漁': 2303, '漂': 2304, '漆': 2305, '漏': 2306, '漓': 2307, '演': 2308, '漕': 2309, '漠': 2310, '漢': 2311, '漣': 2312, '漩': 2313, '漪': 2314, '漫': 2315, '漬': 2316, '漯': 2317, '漱': 2318, '漲': 2319, '漳': 2320, '漸': 2321, '漾': 2322, '漿': 2323, '潑': 2324, '潔': 2325, '潘': 2326, '潛': 2327, '潤': 2328, '潦': 2329, '潭': 2330, '潮': 2331, '潰': 2332, '潸': 2333, '潺': 2334, '潼': 2335, '澀': 2336, '澄': 2337, '澆': 2338, '澈': 2339, '澎': 2340, '澗': 2341, '澡': 2342, '澤': 2343, '澧': 2344, '澱': 2345, '澳': 2346, '澹': 2347, '激': 2348, '濁': 2349, '濂': 2350, '濃': 2351, '濘': 2352, '濛': 2353, '濟': 2354, '濠': 2355, '濡': 2356, '濤': 2357, '濫': 2358, '濬': 2359, '濯': 2360, '濱': 2361, '濺': 2362, '濾': 2363, '瀆': 2364, '瀉': 2365, '瀋': 2366, '瀏': 2367, '瀑': 2368, '瀕': 2369, '瀚': 2370, '瀛': 2371, '瀝': 2372, '瀟': 2373, '瀨': 2374, '瀰': 2375, '瀾': 2376, '灌': 2377, '灑': 2378, '灘': 2379, '灣': 2380, '灤': 2381, '火': 2382, '灰': 2383, '灶': 2384, '灸': 2385, '灼': 2386, '災': 2387, '炊': 2388, '炎': 2389, '炒': 2390, '炕': 2391, '炙': 2392, '炫': 2393, '炬': 2394, '炭': 2395, '炮': 2396, '炯': 2397, '炳': 2398, '炸': 2399, '為': 2400, '烈': 2401, '烊': 2402, '烏': 2403, '烘': 2404, '烙': 2405, '烤': 2406, '烹': 2407, '烽': 2408, '焉': 2409, '焊': 2410, '焙': 2411, '焚': 2412, '無': 2413, '焦': 2414, '焰': 2415, '然': 2416, '煉': 2417, '煌': 2418, '煎': 2419, '煙': 2420, '煜': 2421, '煞': 2422, '煤': 2423, '煥': 2424, '煦': 2425, '照': 2426, '煩': 2427, '煬': 2428, '煮': 2429, '煽': 2430, '熄': 2431, '熊': 2432, '熔': 2433, '熙': 2434, '熟': 2435, '熨': 2436, '熬': 2437, '熱': 2438, '熹': 2439, '熾': 2440, '燃': 2441, '燄': 2442, '燈': 2443, '燉': 2444, '燎': 2445, '燐': 2446, '燒': 2447, '燕': 2448, '燙': 2449, '燜': 2450, '營': 2451, '燥': 2452, '燦': 2453, '燧': 2454, '燬': 2455, '燭': 2456, '燮': 2457, '燴': 2458, '燻': 2459, '爆': 2460, '爍': 2461, '爐': 2462, '爛': 2463, '爨': 2464, '爪': 2465, '爬': 2466, '爭': 2467, '爰': 2468, '爵': 2469, '父': 2470, '爸': 2471, '爹': 2472, '爺': 2473, '爻': 2474, '爽': 2475, '爾': 2476, '牆': 2477, '片': 2478, '版': 2479, '牌': 2480, '牒': 2481, '牖': 2482, '牘': 2483, '牙': 2484, '牛': 2485, '牝': 2486, '牟': 2487, '牠': 2488, '牡': 2489, '牢': 2490, '牧': 2491, '物': 2492, '牯': 2493, '牲': 2494, '牴': 2495, '特': 2496, '牽': 2497, '犀': 2498, '犁': 2499, '犄': 2500, '犒': 2501, '犖': 2502, '犛': 2503, '犢': 2504, '犧': 2505, '犬': 2506, '犯': 2507, '狀': 2508, '狂': 2509, '狄': 2510, '狎': 2511, '狐': 2512, '狗': 2513, '狙': 2514, '狠': 2515, '狡': 2516, '狩': 2517, '狷': 2518, '狸': 2519, '狹': 2520, '狼': 2521, '狽': 2522, '猓': 2523, '猖': 2524, '猙': 2525, '猛': 2526, '猜': 2527, '猥': 2528, '猩': 2529, '猴': 2530, '猶': 2531, '猷': 2532, '猾': 2533, '猿': 2534, '獄': 2535, '獅': 2536, '獎': 2537, '獐': 2538, '獗': 2539, '獨': 2540, '獰': 2541, '獲': 2542, '獵': 2543, '獷': 2544, '獸': 2545, '獺': 2546, '獻': 2547, '玀': 2548, '玄': 2549, '率': 2550, '玉': 2551, '王': 2552, '玖': 2553, '玟': 2554, '玨': 2555, '玩': 2556, '玫': 2557, '玲': 2558, '玳': 2559, '玷': 2560, '玻': 2561, '珀': 2562, '珊': 2563, '珍': 2564, '珠': 2565, '班': 2566, '珮': 2567, '現': 2568, '球': 2569, '琅': 2570, '理': 2571, '琉': 2572, '琊': 2573, '琍': 2574, '琢': 2575, '琥': 2576, '琪': 2577, '琳': 2578, '琴': 2579, '琵': 2580, '琶': 2581, '琺': 2582, '琿': 2583, '瑁': 2584, '瑕': 2585, '瑙': 2586, '瑚': 2587, '瑛': 2588, '瑜': 2589, '瑞': 2590, '瑟': 2591, '瑣': 2592, '瑤': 2593, '瑩': 2594, '瑪': 2595, '瑯': 2596, '瑰': 2597, '璃': 2598, '璋': 2599, '璜': 2600, '璣': 2601, '璦': 2602, '璧': 2603, '璩': 2604, '環': 2605, '璽': 2606, '瓊': 2607, '瓏': 2608, '瓜': 2609, '瓠': 2610, '瓢': 2611, '瓣': 2612, '瓦': 2613, '瓶': 2614, '瓷': 2615, '甄': 2616, '甌': 2617, '甕': 2618, '甘': 2619, '甚': 2620, '甜': 2621, '生': 2622, '產': 2623, '甥': 2624, '甦': 2625, '用': 2626, '甩': 2627, '甫': 2628, '甬': 2629, '甭': 2630, '田': 2631, '由': 2632, '甲': 2633, '申': 2634, '男': 2635, '甸': 2636, '甽': 2637, '界': 2638, '畏': 2639, '畔': 2640, '留': 2641, '畚': 2642, '畜': 2643, '畝': 2644, '畢': 2645, '略': 2646, '畦': 2647, '番': 2648, '畫': 2649, '異': 2650, '當': 2651, '畸': 2652, '疆': 2653, '疇': 2654, '疊': 2655, '疋': 2656, '疏': 2657, '疑': 2658, '疙': 2659, '疚': 2660, '疝': 2661, '疤': 2662, '疥': 2663, '疫': 2664, '疲': 2665, '疳': 2666, '疵': 2667, '疹': 2668, '疼': 2669, '疽': 2670, '疾': 2671, '病': 2672, '症': 2673, '痊': 2674, '痔': 2675, '痕': 2676, '痘': 2677, '痙': 2678, '痛': 2679, '痞': 2680, '痢': 2681, '痣': 2682, '痰': 2683, '痱': 2684, '痲': 2685, '痴': 2686, '痺': 2687, '痿': 2688, '瘀': 2689, '瘁': 2690, '瘉': 2691, '瘋': 2692, '瘍': 2693, '瘓': 2694, '瘟': 2695, '瘠': 2696, '瘡': 2697, '瘤': 2698, '瘦': 2699, '瘧': 2700, '瘩': 2701, '瘴': 2702, '瘸': 2703, '療': 2704, '癆': 2705, '癌': 2706, '癒': 2707, '癖': 2708, '癘': 2709, '癢': 2710, '癥': 2711, '癩': 2712, '癬': 2713, '癮': 2714, '癱': 2715, '癲': 2716, '癸': 2717, '登': 2718, '發': 2719, '白': 2720, '百': 2721, '皂': 2722, '的': 2723, '皆': 2724, '皇': 2725, '皈': 2726, '皎': 2727, '皓': 2728, '皖': 2729, '皚': 2730, '皮': 2731, '皰': 2732, '皴': 2733, '皺': 2734, '皿': 2735, '盂': 2736, '盃': 2737, '盆': 2738, '盈': 2739, '益': 2740, '盍': 2741, '盎': 2742, '盒': 2743, '盔': 2744, '盛': 2745, '盜': 2746, '盞': 2747, '盟': 2748, '盡': 2749, '監': 2750, '盤': 2751, '盥': 2752, '盧': 2753, '盪': 2754, '目': 2755, '盯': 2756, '盲': 2757, '直': 2758, '相': 2759, '盹': 2760, '盼': 2761, '盾': 2762, '省': 2763, '眉': 2764, '看': 2765, '真': 2766, '眠': 2767, '眨': 2768, '眩': 2769, '眶': 2770, '眷': 2771, '眸': 2772, '眺': 2773, '眼': 2774, '眾': 2775, '睏': 2776, '睛': 2777, '睜': 2778, '睞': 2779, '睡': 2780, '督': 2781, '睥': 2782, '睦': 2783, '睨': 2784, '睪': 2785, '睫': 2786, '睬': 2787, '睹': 2788, '睽': 2789, '睿': 2790, '瞄': 2791, '瞇': 2792, '瞌': 2793, '瞎': 2794, '瞑': 2795, '瞞': 2796, '瞟': 2797, '瞠': 2798, '瞥': 2799, '瞧': 2800, '瞪': 2801, '瞬': 2802, '瞭': 2803, '瞰': 2804, '瞳': 2805, '瞻': 2806, '瞽': 2807, '瞿': 2808, '矇': 2809, '矓': 2810, '矗': 2811, '矚': 2812, '矛': 2813, '矜': 2814, '矢': 2815, '矣': 2816, '知': 2817, '矩': 2818, '短': 2819, '矮': 2820, '矯': 2821, '石': 2822, '矽': 2823, '砂': 2824, '砌': 2825, '砍': 2826, '研': 2827, '砝': 2828, '砥': 2829, '砧': 2830, '砭': 2831, '砰': 2832, '破': 2833, '砷': 2834, '砸': 2835, '硃': 2836, '硝': 2837, '硫': 2838, '硬': 2839, '硯': 2840, '硼': 2841, '碉': 2842, '碌': 2843, '碎': 2844, '碑': 2845, '碗': 2846, '碘': 2847, '碟': 2848, '碧': 2849, '碩': 2850, '碰': 2851, '碳': 2852, '確': 2853, '碼': 2854, '碾': 2855, '磁': 2856, '磅': 2857, '磊': 2858, '磋': 2859, '磐': 2860, '磕': 2861, '磚': 2862, '磨': 2863, '磬': 2864, '磯': 2865, '磴': 2866, '磷': 2867, '磺': 2868, '礁': 2869, '礎': 2870, '礙': 2871, '礦': 2872, '礪': 2873, '礫': 2874, '礬': 2875, '示': 2876, '社': 2877, '祀': 2878, '祁': 2879, '祆': 2880, '祇': 2881, '祈': 2882, '祉': 2883, '祐': 2884, '祕': 2885, '祖': 2886, '祗': 2887, '祚': 2888, '祝': 2889, '神': 2890, '祟': 2891, '祠': 2892, '祥': 2893, '票': 2894, '祭': 2895, '祺': 2896, '祿': 2897, '禁': 2898, '禍': 2899, '禎': 2900, '福': 2901, '禦': 2902, '禧': 2903, '禪': 2904, '禮': 2905, '禱': 2906, '禹': 2907, '禽': 2908, '禾': 2909, '禿': 2910, '秀': 2911, '私': 2912, '秉': 2913, '秋': 2914, '科': 2915, '秒': 2916, '租': 2917, '秣': 2918, '秤': 2919, '秦': 2920, '秧': 2921, '秩': 2922, '移': 2923, '稀': 2924, '稅': 2925, '稈': 2926, '程': 2927, '稍': 2928, '稔': 2929, '稚': 2930, '稜': 2931, '稟': 2932, '稠': 2933, '種': 2934, '稱': 2935, '稷': 2936, '稻': 2937, '稼': 2938, '稽': 2939, '稿': 2940, '穀': 2941, '穆': 2942, '穌': 2943, '積': 2944, '穎': 2945, '穗': 2946, '穡': 2947, '穢': 2948, '穩': 2949, '穫': 2950, '穴': 2951, '究': 2952, '穹': 2953, '空': 2954, '穿': 2955, '突': 2956, '窄': 2957, '窈': 2958, '窒': 2959, '窕': 2960, '窖': 2961, '窗': 2962, '窘': 2963, '窟': 2964, '窠': 2965, '窩': 2966, '窪': 2967, '窮': 2968, '窯': 2969, '窺': 2970, '竄': 2971, '竅': 2972, '竇': 2973, '竊': 2974, '立': 2975, '站': 2976, '竟': 2977, '章': 2978, '竣': 2979, '童': 2980, '竭': 2981, '端': 2982, '競': 2983, '竹': 2984, '竺': 2985, '竽': 2986, '竿': 2987, '笆': 2988, '笑': 2989, '笙': 2990, '笛': 2991, '笞': 2992, '笠': 2993, '符': 2994, '笨': 2995, '第': 2996, '筆': 2997, '等': 2998, '筋': 2999, '筍': 3000, '筏': 3001, '筐': 3002, '筒': 3003, '答': 3004, '策': 3005, '筠': 3006, '筵': 3007, '筷': 3008, '箋': 3009, '箏': 3010, '箔': 3011, '箕': 3012, '算': 3013, '箝': 3014, '管': 3015, '箭': 3016, '箱': 3017, '箴': 3018, '節': 3019, '篁': 3020, '範': 3021, '篆': 3022, '篇': 3023, '築': 3024, '篙': 3025, '篛': 3026, '篡': 3027, '篤': 3028, '篩': 3029, '篷': 3030, '篾': 3031, '簇': 3032, '簍': 3033, '簑': 3034, '簞': 3035, '簡': 3036, '簣': 3037, '簧': 3038, '簪': 3039, '簫': 3040, '簷': 3041, '簸': 3042, '簽': 3043, '簾': 3044, '簿': 3045, '籃': 3046, '籌': 3047, '籍': 3048, '籐': 3049, '籟': 3050, '籠': 3051, '籤': 3052, '籬': 3053, '籮': 3054, '籲': 3055, '米': 3056, '粉': 3057, '粒': 3058, '粗': 3059, '粟': 3060, '粥': 3061, '粱': 3062, '粳': 3063, '粵': 3064, '粹': 3065, '粽': 3066, '精': 3067, '糊': 3068, '糕': 3069, '糖': 3070, '糙': 3071, '糜': 3072, '糞': 3073, '糟': 3074, '糠': 3075, '糢': 3076, '糧': 3077, '糯': 3078, '糸': 3079, '系': 3080, '糾': 3081, '紀': 3082, '紂': 3083, '約': 3084, '紅': 3085, '紇': 3086, '紉': 3087, '紊': 3088, '紋': 3089, '納': 3090, '紐': 3091, '純': 3092, '紕': 3093, '紗': 3094, '紙': 3095, '級': 3096, '紛': 3097, '紜': 3098, '素': 3099, '紡': 3100, '索': 3101, '紫': 3102, '紮': 3103, '累': 3104, '細': 3105, '紳': 3106, '紹': 3107, '紼': 3108, '絀': 3109, '終': 3110, '絃': 3111, '組': 3112, '絆': 3113, '結': 3114, '絕': 3115, '絞': 3116, '絡': 3117, '絢': 3118, '給': 3119, '絨': 3120, '絮': 3121, '統': 3122, '絲': 3123, '絹': 3124, '綁': 3125, '綏': 3126, '綑': 3127, '經': 3128, '綜': 3129, '綞': 3130, '綠': 3131, '綢': 3132, '維': 3133, '綰': 3134, '綱': 3135, '網': 3136, '綴': 3137, '綵': 3138, '綸': 3139, '綺': 3140, '綻': 3141, '綽': 3142, '綾': 3143, '綿': 3144, '緇': 3145, '緊': 3146, '緒': 3147, '緘': 3148, '線': 3149, '緝': 3150, '緞': 3151, '締': 3152, '緣': 3153, '編': 3154, '緩': 3155, '緬': 3156, '緯': 3157, '練': 3158, '緻': 3159, '縈': 3160, '縊': 3161, '縑': 3162, '縛': 3163, '縣': 3164, '縫': 3165, '縮': 3166, '縱': 3167, '縲': 3168, '縷': 3169, '總': 3170, '績': 3171, '繁': 3172, '繃': 3173, '繅': 3174, '繆': 3175, '織': 3176, '繕': 3177, '繚': 3178, '繞': 3179, '繡': 3180, '繩': 3181, '繪': 3182, '繫': 3183, '繭': 3184, '繹': 3185, '繼': 3186, '繽': 3187, '纂': 3188, '續': 3189, '纏': 3190, '纓': 3191, '纖': 3192, '纜': 3193, '缶': 3194, '缸': 3195, '缺': 3196, '缽': 3197, '罄': 3198, '罈': 3199, '罐': 3200, '罔': 3201, '罕': 3202, '罟': 3203, '罩': 3204, '罪': 3205, '置': 3206, '罰': 3207, '署': 3208, '罵': 3209, '罷': 3210, '罹': 3211, '羅': 3212, '羈': 3213, '羊': 3214, '羋': 3215, '羌': 3216, '美': 3217, '羔': 3218, '羚': 3219, '羞': 3220, '群': 3221, '羨': 3222, '義': 3223, '羯': 3224, '羲': 3225, '羶': 3226, '羸': 3227, '羹': 3228, '羽': 3229, '羿': 3230, '翁': 3231, '翅': 3232, '翌': 3233, '翎': 3234, '習': 3235, '翔': 3236, '翕': 3237, '翟': 3238, '翠': 3239, '翡': 3240, '翩': 3241, '翰': 3242, '翱': 3243, '翳': 3244, '翹': 3245, '翻': 3246, '翼': 3247, '耀': 3248, '老': 3249, '考': 3250, '者': 3251, '耆': 3252, '而': 3253, '耍': 3254, '耐': 3255, '耒': 3256, '耕': 3257, '耗': 3258, '耘': 3259, '耙': 3260, '耜': 3261, '耳': 3262, '耶': 3263, '耽': 3264, '耿': 3265, '聆': 3266, '聊': 3267, '聖': 3268, '聘': 3269, '聚': 3270, '聞': 3271, '聯': 3272, '聰': 3273, '聱': 3274, '聲': 3275, '聳': 3276, '聶': 3277, '職': 3278, '聽': 3279, '聾': 3280, '聿': 3281, '肄': 3282, '肅': 3283, '肆': 3284, '肇': 3285, '肉': 3286, '肋': 3287, '肌': 3288, '肓': 3289, '肖': 3290, '肘': 3291, '肚': 3292, '肛': 3293, '肝': 3294, '股': 3295, '肢': 3296, '肥': 3297, '肩': 3298, '肪': 3299, '肫': 3300, '肯': 3301, '肱': 3302, '育': 3303, '肴': 3304, '肺': 3305, '胃': 3306, '胄': 3307, '背': 3308, '胎': 3309, '胖': 3310, '胚': 3311, '胛': 3312, '胞': 3313, '胡': 3314, '胤': 3315, '胥': 3316, '胭': 3317, '胰': 3318, '胱': 3319, '胳': 3320, '胴': 3321, '胸': 3322, '能': 3323, '脂': 3324, '脅': 3325, '脆': 3326, '脈': 3327, '脊': 3328, '脖': 3329, '脣': 3330, '脩': 3331, '脫': 3332, '脯': 3333, '脹': 3334, '脾': 3335, '腆': 3336, '腋': 3337, '腎': 3338, '腐': 3339, '腑': 3340, '腔': 3341, '腕': 3342, '腥': 3343, '腦': 3344, '腫': 3345, '腮': 3346, '腰': 3347, '腱': 3348, '腳': 3349, '腸': 3350, '腹': 3351, '腺': 3352, '腿': 3353, '膀': 3354, '膈': 3355, '膊': 3356, '膏': 3357, '膚': 3358, '膛': 3359, '膜': 3360, '膝': 3361, '膠': 3362, '膨': 3363, '膩': 3364, '膳': 3365, '膺': 3366, '膽': 3367, '膾': 3368, '膿': 3369, '臀': 3370, '臂': 3371, '臃': 3372, '臆': 3373, '臉': 3374, '臍': 3375, '臏': 3376, '臘': 3377, '臚': 3378, '臟': 3379, '臣': 3380, '臥': 3381, '臧': 3382, '臨': 3383, '自': 3384, '臬': 3385, '臭': 3386, '至': 3387, '致': 3388, '臺': 3389, '臻': 3390, '臼': 3391, '臾': 3392, '舀': 3393, '舂': 3394, '舅': 3395, '與': 3396, '興': 3397, '舉': 3398, '舊': 3399, '舌': 3400, '舍': 3401, '舐': 3402, '舒': 3403, '舔': 3404, '舛': 3405, '舜': 3406, '舞': 3407, '舟': 3408, '舢': 3409, '舨': 3410, '航': 3411, '舫': 3412, '般': 3413, '舵': 3414, '舶': 3415, '舷': 3416, '船': 3417, '艇': 3418, '艘': 3419, '艙': 3420, '艦': 3421, '艮': 3422, '良': 3423, '艱': 3424, '色': 3425, '艾': 3426, '芋': 3427, '芍': 3428, '芒': 3429, '芙': 3430, '芝': 3431, '芟': 3432, '芥': 3433, '芬': 3434, '芭': 3435, '花': 3436, '芳': 3437, '芹': 3438, '芻': 3439, '芽': 3440, '苑': 3441, '苒': 3442, '苓': 3443, '苔': 3444, '苗': 3445, '苛': 3446, '苜': 3447, '苞': 3448, '苟': 3449, '苣': 3450, '若': 3451, '苦': 3452, '苧': 3453, '英': 3454, '茁': 3455, '茂': 3456, '范': 3457, '茄': 3458, '茅': 3459, '茉': 3460, '茗': 3461, '茫': 3462, '茱': 3463, '茲': 3464, '茴': 3465, '茵': 3466, '茶': 3467, '茸': 3468, '茹': 3469, '荀': 3470, '草': 3471, '荊': 3472, '荏': 3473, '荐': 3474, '荒': 3475, '荔': 3476, '荷': 3477, '荸': 3478, '荻': 3479, '荼': 3480, '莉': 3481, '莊': 3482, '莎': 3483, '莒': 3484, '莓': 3485, '莖': 3486, '莘': 3487, '莞': 3488, '莠': 3489, '莢': 3490, '莫': 3491, '莽': 3492, '菁': 3493, '菅': 3494, '菊': 3495, '菌': 3496, '菜': 3497, '菠': 3498, '菩': 3499, '華': 3500, '菰': 3501, '菱': 3502, '菲': 3503, '菴': 3504, '菸': 3505, '菽': 3506, '萃': 3507, '萄': 3508, '萊': 3509, '萋': 3510, '萌': 3511, '萍': 3512, '萎': 3513, '萬': 3514, '萱': 3515, '萵': 3516, '萸': 3517, '萼': 3518, '落': 3519, '葉': 3520, '著': 3521, '葛': 3522, '葡': 3523, '董': 3524, '葦': 3525, '葩': 3526, '葫': 3527, '葬': 3528, '葵': 3529, '葷': 3530, '蒂': 3531, '蒐': 3532, '蒙': 3533, '蒜': 3534, '蒞': 3535, '蒲': 3536, '蒸': 3537, '蒼': 3538, '蒿': 3539, '蓀': 3540, '蓄': 3541, '蓆': 3542, '蓉': 3543, '蓋': 3544, '蓓': 3545, '蓬': 3546, '蓮': 3547, '蓿': 3548, '蔑': 3549, '蔓': 3550, '蔔': 3551, '蔗': 3552, '蔚': 3553, '蔡': 3554, '蔣': 3555, '蔥': 3556, '蔬': 3557, '蔭': 3558, '蔽': 3559, '蕃': 3560, '蕈': 3561, '蕉': 3562, '蕊': 3563, '蕙': 3564, '蕨': 3565, '蕩': 3566, '蕪': 3567, '蕭': 3568, '蕾': 3569, '薄': 3570, '薇': 3571, '薑': 3572, '薔': 3573, '薛': 3574, '薜': 3575, '薩': 3576, '薪': 3577, '薯': 3578, '薰': 3579, '藉': 3580, '藍': 3581, '藏': 3582, '藐': 3583, '藕': 3584, '藝': 3585, '藤': 3586, '藥': 3587, '藩': 3588, '藪': 3589, '藹': 3590, '藺': 3591, '藻': 3592, '蘆': 3593, '蘇': 3594, '蘊': 3595, '蘋': 3596, '蘑': 3597, '蘗': 3598, '蘚': 3599, '蘭': 3600, '蘸': 3601, '蘿': 3602, '虎': 3603, '虐': 3604, '虔': 3605, '處': 3606, '虛': 3607, '虜': 3608, '虞': 3609, '號': 3610, '虧': 3611, '虫': 3612, '虱': 3613, '虹': 3614, '蚊': 3615, '蚌': 3616, '蚓': 3617, '蚣': 3618, '蚤': 3619, '蚩': 3620, '蚪': 3621, '蚯': 3622, '蚱': 3623, '蚵': 3624, '蚶': 3625, '蛀': 3626, '蛄': 3627, '蛆': 3628, '蛇': 3629, '蛋': 3630, '蛔': 3631, '蛙': 3632, '蛛': 3633, '蛟': 3634, '蛤': 3635, '蛭': 3636, '蛹': 3637, '蛻': 3638, '蛾': 3639, '蜀': 3640, '蜂': 3641, '蜃': 3642, '蜇': 3643, '蜈': 3644, '蜓': 3645, '蜘': 3646, '蜜': 3647, '蜢': 3648, '蜥': 3649, '蜴': 3650, '蜻': 3651, '蜿': 3652, '蝌': 3653, '蝕': 3654, '蝗': 3655, '蝙': 3656, '蝠': 3657, '蝦': 3658, '蝨': 3659, '蝴': 3660, '蝶': 3661, '蝸': 3662, '螂': 3663, '螃': 3664, '融': 3665, '螞': 3666, '螟': 3667, '螢': 3668, '螫': 3669, '螳': 3670, '螺': 3671, '螻': 3672, '蟀': 3673, '蟆': 3674, '蟈': 3675, '蟋': 3676, '蟑': 3677, '蟒': 3678, '蟬': 3679, '蟯': 3680, '蟲': 3681, '蟹': 3682, '蟻': 3683, '蠅': 3684, '蠍': 3685, '蠔': 3686, '蠕': 3687, '蠟': 3688, '蠡': 3689, '蠢': 3690, '蠣': 3691, '蠱': 3692, '蠶': 3693, '蠹': 3694, '蠻': 3695, '血': 3696, '行': 3697, '衍': 3698, '術': 3699, '街': 3700, '衙': 3701, '衛': 3702, '衝': 3703, '衡': 3704, '衢': 3705, '衣': 3706, '表': 3707, '衫': 3708, '衰': 3709, '衷': 3710, '袁': 3711, '袂': 3712, '袈': 3713, '袋': 3714, '袍': 3715, '袒': 3716, '袖': 3717, '袞': 3718, '被': 3719, '袱': 3720, '裁': 3721, '裂': 3722, '裊': 3723, '裔': 3724, '裕': 3725, '裘': 3726, '裙': 3727, '補': 3728, '裝': 3729, '裟': 3730, '裡': 3731, '裨': 3732, '裳': 3733, '裴': 3734, '裸': 3735, '裹': 3736, '製': 3737, '褂': 3738, '複': 3739, '褐': 3740, '褒': 3741, '褓': 3742, '褚': 3743, '褥': 3744, '褪': 3745, '褫': 3746, '褲': 3747, '褶': 3748, '褸': 3749, '褻': 3750, '襄': 3751, '襖': 3752, '襟': 3753, '襠': 3754, '襤': 3755, '襪': 3756, '襯': 3757, '襲': 3758, '西': 3759, '要': 3760, '覃': 3761, '覆': 3762, '見': 3763, '規': 3764, '覓': 3765, '視': 3766, '覦': 3767, '親': 3768, '覬': 3769, '覲': 3770, '覺': 3771, '覽': 3772, '觀': 3773, '角': 3774, '解': 3775, '觴': 3776, '觸': 3777, '言': 3778, '訂': 3779, '訃': 3780, '計': 3781, '訊': 3782, '訌': 3783, '討': 3784, '訐': 3785, '訓': 3786, '訕': 3787, '訖': 3788, '託': 3789, '記': 3790, '訛': 3791, '訝': 3792, '訟': 3793, '訣': 3794, '訥': 3795, '訪': 3796, '設': 3797, '許': 3798, '訴': 3799, '診': 3800, '註': 3801, '証': 3802, '詁': 3803, '詆': 3804, '詐': 3805, '詔': 3806, '評': 3807, '詛': 3808, '詞': 3809, '詠': 3810, '詢': 3811, '詣': 3812, '試': 3813, '詩': 3814, '詫': 3815, '詬': 3816, '詭': 3817, '詮': 3818, '詰': 3819, '話': 3820, '該': 3821, '詳': 3822, '詹': 3823, '詼': 3824, '誅': 3825, '誇': 3826, '誌': 3827, '認': 3828, '誑': 3829, '誓': 3830, '誕': 3831, '誘': 3832, '語': 3833, '誠': 3834, '誡': 3835, '誣': 3836, '誤': 3837, '誥': 3838, '誦': 3839, '誨': 3840, '說': 3841, '誰': 3842, '課': 3843, '誼': 3844, '調': 3845, '諂': 3846, '諄': 3847, '談': 3848, '諉': 3849, '請': 3850, '諍': 3851, '諒': 3852, '論': 3853, '諜': 3854, '諦': 3855, '諧': 3856, '諫': 3857, '諭': 3858, '諮': 3859, '諱': 3860, '諷': 3861, '諸': 3862, '諺': 3863, '諾': 3864, '謀': 3865, '謁': 3866, '謂': 3867, '謄': 3868, '謊': 3869, '謎': 3870, '謗': 3871, '謙': 3872, '講': 3873, '謝': 3874, '謠': 3875, '謨': 3876, '謬': 3877, '謹': 3878, '譁': 3879, '證': 3880, '譎': 3881, '譏': 3882, '識': 3883, '譚': 3884, '譜': 3885, '警': 3886, '譬': 3887, '譯': 3888, '議': 3889, '譴': 3890, '護': 3891, '譽': 3892, '讀': 3893, '變': 3894, '讒': 3895, '讓': 3896, '讖': 3897, '讚': 3898, '谷': 3899, '谿': 3900, '豁': 3901, '豆': 3902, '豈': 3903, '豉': 3904, '豌': 3905, '豎': 3906, '豐': 3907, '豔': 3908, '豕': 3909, '豚': 3910, '象': 3911, '豢': 3912, '豪': 3913, '豫': 3914, '豬': 3915, '豹': 3916, '豺': 3917, '貂': 3918, '貉': 3919, '貊': 3920, '貌': 3921, '貍': 3922, '貓': 3923, '貝': 3924, '貞': 3925, '負': 3926, '財': 3927, '貢': 3928, '貧': 3929, '貨': 3930, '販': 3931, '貪': 3932, '貫': 3933, '責': 3934, '貯': 3935, '貲': 3936, '貳': 3937, '貴': 3938, '貶': 3939, '買': 3940, '貸': 3941, '費': 3942, '貼': 3943, '貽': 3944, '貿': 3945, '賀': 3946, '賁': 3947, '賂': 3948, '賃': 3949, '賄': 3950, '賅': 3951, '資': 3952, '賈': 3953, '賊': 3954, '賑': 3955, '賒': 3956, '賓': 3957, '賜': 3958, '賞': 3959, '賠': 3960, '賢': 3961, '賣': 3962, '賤': 3963, '賦': 3964, '質': 3965, '賬': 3966, '賭': 3967, '賴': 3968, '賺': 3969, '購': 3970, '賽': 3971, '贅': 3972, '贈': 3973, '贊': 3974, '贍': 3975, '贏': 3976, '贓': 3977, '贖': 3978, '贗': 3979, '贛': 3980, '赤': 3981, '赦': 3982, '赧': 3983, '赫': 3984, '赭': 3985, '走': 3986, '赳': 3987, '赴': 3988, '起': 3989, '趁': 3990, '超': 3991, '越': 3992, '趕': 3993, '趙': 3994, '趟': 3995, '趣': 3996, '趨': 3997, '足': 3998, '趴': 3999, '趾': 4000, '跆': 4001, '跋': 4002, '跌': 4003, '跎': 4004, '跑': 4005, '跚': 4006, '跛': 4007, '距': 4008, '跟': 4009, '跡': 4010, '跨': 4011, '跪': 4012, '路': 4013, '跳': 4014, '跺': 4015, '跼': 4016, '踏': 4017, '踐': 4018, '踝': 4019, '踟': 4020, '踢': 4021, '踩': 4022, '踫': 4023, '踱': 4024, '踴': 4025, '踵': 4026, '踹': 4027, '蹂': 4028, '蹄': 4029, '蹈': 4030, '蹉': 4031, '蹊': 4032, '蹋': 4033, '蹙': 4034, '蹣': 4035, '蹤': 4036, '蹦': 4037, '蹬': 4038, '蹲': 4039, '蹶': 4040, '蹺': 4041, '蹼': 4042, '躁': 4043, '躂': 4044, '躅': 4045, '躇': 4046, '躉': 4047, '躊': 4048, '躍': 4049, '躑': 4050, '躡': 4051, '躪': 4052, '身': 4053, '躬': 4054, '躲': 4055, '躺': 4056, '軀': 4057, '車': 4058, '軋': 4059, '軌': 4060, '軍': 4061, '軒': 4062, '軔': 4063, '軛': 4064, '軟': 4065, '軸': 4066, '軻': 4067, '軼': 4068, '軾': 4069, '較': 4070, '載': 4071, '輊': 4072, '輒': 4073, '輓': 4074, '輔': 4075, '輕': 4076, '輛': 4077, '輜': 4078, '輝': 4079, '輟': 4080, '輦': 4081, '輩': 4082, '輪': 4083, '輯': 4084, '輸': 4085, '輻': 4086, '輾': 4087, '輿': 4088, '轂': 4089, '轄': 4090, '轅': 4091, '轉': 4092, '轍': 4093, '轎': 4094, '轔': 4095, '轟': 4096, '轡': 4097, '辛': 4098, '辜': 4099, '辟': 4100, '辣': 4101, '辦': 4102, '辨': 4103, '辭': 4104, '辮': 4105, '辯': 4106, '辰': 4107, '辱': 4108, '農': 4109, '迂': 4110, '迄': 4111, '迅': 4112, '迆': 4113, '迎': 4114, '近': 4115, '返': 4116, '迢': 4117, '迥': 4118, '迦': 4119, '迪': 4120, '迫': 4121, '迭': 4122, '述': 4123, '迴': 4124, '迷': 4125, '迺': 4126, '追': 4127, '退': 4128, '送': 4129, '逃': 4130, '逅': 4131, '逆': 4132, '逍': 4133, '透': 4134, '逐': 4135, '途': 4136, '逕': 4137, '逖': 4138, '逗': 4139, '這': 4140, '通': 4141, '逛': 4142, '逝': 4143, '逞': 4144, '速': 4145, '造': 4146, '逢': 4147, '連': 4148, '逮': 4149, '週': 4150, '進': 4151, '逵': 4152, '逸': 4153, '逼': 4154, '逾': 4155, '遁': 4156, '遂': 4157, '遇': 4158, '遊': 4159, '運': 4160, '遍': 4161, '過': 4162, '遏': 4163, '遐': 4164, '遑': 4165, '道': 4166, '達': 4167, '違': 4168, '遘': 4169, '遙': 4170, '遜': 4171, '遞': 4172, '遠': 4173, '遣': 4174, '遨': 4175, '適': 4176, '遭': 4177, '遮': 4178, '遲': 4179, '遴': 4180, '遵': 4181, '遷': 4182, '選': 4183, '遺': 4184, '遼': 4185, '遽': 4186, '避': 4187, '邀': 4188, '邁': 4189, '邂': 4190, '還': 4191, '邇': 4192, '邊': 4193, '邏': 4194, '邐': 4195, '邑': 4196, '邕': 4197, '邢': 4198, '那': 4199, '邦': 4200, '邪': 4201, '邱': 4202, '邵': 4203, '邸': 4204, '郁': 4205, '郊': 4206, '郎': 4207, '郡': 4208, '部': 4209, '郭': 4210, '郵': 4211, '都': 4212, '鄂': 4213, '鄉': 4214, '鄒': 4215, '鄙': 4216, '鄧': 4217, '鄭': 4218, '鄰': 4219, '鄱': 4220, '鄹': 4221, '酉': 4222, '酊': 4223, '酋': 4224, '酌': 4225, '配': 4226, '酒': 4227, '酗': 4228, '酣': 4229, '酥': 4230, '酩': 4231, '酪': 4232, '酬': 4233, '酵': 4234, '酷': 4235, '酸': 4236, '醃': 4237, '醇': 4238, '醉': 4239, '醋': 4240, '醒': 4241, '醜': 4242, '醞': 4243, '醣': 4244, '醫': 4245, '醬': 4246, '醺': 4247, '釀': 4248, '釁': 4249, '采': 4250, '釉': 4251, '釋': 4252, '里': 4253, '重': 4254, '野': 4255, '量': 4256, '釐': 4257, '金': 4258, '釗': 4259, '釘': 4260, '釜': 4261, '針': 4262, '釣': 4263, '釦': 4264, '釧': 4265, '釵': 4266, '鈉': 4267, '鈍': 4268, '鈐': 4269, '鈔': 4270, '鈕': 4271, '鈞': 4272, '鈣': 4273, '鈴': 4274, '鈷': 4275, '鈸': 4276, '鈽': 4277, '鈾': 4278, '鉀': 4279, '鉋': 4280, '鉑': 4281, '鉗': 4282, '鉛': 4283, '鉤': 4284, '鉸': 4285, '鉻': 4286, '銀': 4287, '銅': 4288, '銓': 4289, '銖': 4290, '銘': 4291, '銜': 4292, '銬': 4293, '銳': 4294, '銷': 4295, '銻': 4296, '銼': 4297, '鋁': 4298, '鋅': 4299, '鋒': 4300, '鋤': 4301, '鋪': 4302, '鋸': 4303, '鋼': 4304, '錄': 4305, '錐': 4306, '錘': 4307, '錚': 4308, '錠': 4309, '錢': 4310, '錦': 4311, '錨': 4312, '錫': 4313, '錯': 4314, '錳': 4315, '錶': 4316, '鍊': 4317, '鍋': 4318, '鍍': 4319, '鍛': 4320, '鍥': 4321, '鍬': 4322, '鍰': 4323, '鍵': 4324, '鍾': 4325, '鎂': 4326, '鎊': 4327, '鎔': 4328, '鎖': 4329, '鎢': 4330, '鎮': 4331, '鎳': 4332, '鏃': 4333, '鏈': 4334, '鏍': 4335, '鏑': 4336, '鏖': 4337, '鏗': 4338, '鏘': 4339, '鏜': 4340, '鏝': 4341, '鏟': 4342, '鏡': 4343, '鏢': 4344, '鏤': 4345, '鏽': 4346, '鐃': 4347, '鐘': 4348, '鐮': 4349, '鐲': 4350, '鐳': 4351, '鐵': 4352, '鐸': 4353, '鐺': 4354, '鑄': 4355, '鑑': 4356, '鑒': 4357, '鑠': 4358, '鑣': 4359, '鑰': 4360, '鑲': 4361, '鑼': 4362, '鑽': 4363, '鑾': 4364, '鑿': 4365, '長': 4366, '門': 4367, '閂': 4368, '閃': 4369, '閉': 4370, '開': 4371, '閏': 4372, '閑': 4373, '閒': 4374, '間': 4375, '閔': 4376, '閘': 4377, '閡': 4378, '閣': 4379, '閤': 4380, '閥': 4381, '閨': 4382, '閩': 4383, '閭': 4384, '閱': 4385, '閻': 4386, '闆': 4387, '闈': 4388, '闊': 4389, '闋': 4390, '闌': 4391, '闐': 4392, '闔': 4393, '闖': 4394, '關': 4395, '闡': 4396, '闢': 4397, '阜': 4398, '阡': 4399, '阪': 4400, '阮': 4401, '阱': 4402, '防': 4403, '阻': 4404, '阿': 4405, '陀': 4406, '附': 4407, '陋': 4408, '陌': 4409, '降': 4410, '限': 4411, '陛': 4412, '陝': 4413, '陡': 4414, '院': 4415, '陣': 4416, '除': 4417, '陪': 4418, '陰': 4419, '陲': 4420, '陳': 4421, '陴': 4422, '陵': 4423, '陶': 4424, '陷': 4425, '陸': 4426, '陽': 4427, '隅': 4428, '隆': 4429, '隊': 4430, '隋': 4431, '隍': 4432, '階': 4433, '隔': 4434, '隕': 4435, '隘': 4436, '隙': 4437, '際': 4438, '障': 4439, '隧': 4440, '隨': 4441, '險': 4442, '隱': 4443, '隴': 4444, '隸': 4445, '隻': 4446, '雀': 4447, '雁': 4448, '雄': 4449, '雅': 4450, '集': 4451, '雇': 4452, '雉': 4453, '雋': 4454, '雌': 4455, '雍': 4456, '雕': 4457, '雖': 4458, '雙': 4459, '雛': 4460, '雜': 4461, '雞': 4462, '離': 4463, '難': 4464, '雨': 4465, '雪': 4466, '雯': 4467, '雲': 4468, '零': 4469, '雷': 4470, '雹': 4471, '電': 4472, '需': 4473, '霄': 4474, '霆': 4475, '震': 4476, '霉': 4477, '霍': 4478, '霎': 4479, '霏': 4480, '霑': 4481, '霓': 4482, '霖': 4483, '霜': 4484, '霞': 4485, '霧': 4486, '霪': 4487, '露': 4488, '霸': 4489, '霹': 4490, '霽': 4491, '霾': 4492, '靂': 4493, '靄': 4494, '靈': 4495, '青': 4496, '靖': 4497, '靛': 4498, '靜': 4499, '非': 4500, '靠': 4501, '靡': 4502, '面': 4503, '靦': 4504, '靨': 4505, '革': 4506, '靴': 4507, '靶': 4508, '靼': 4509, '鞅': 4510, '鞋': 4511, '鞍': 4512, '鞏': 4513, '鞘': 4514, '鞠': 4515, '鞣': 4516, '鞦': 4517, '鞭': 4518, '韁': 4519, '韃': 4520, '韆': 4521, '韋': 4522, '韌': 4523, '韓': 4524, '韜': 4525, '韭': 4526, '音': 4527, '韶': 4528, '韻': 4529, '響': 4530, '頁': 4531, '頂': 4532, '頃': 4533, '項': 4534, '順': 4535, '須': 4536, '頊': 4537, '頌': 4538, '預': 4539, '頑': 4540, '頒': 4541, '頓': 4542, '頗': 4543, '領': 4544, '頡': 4545, '頤': 4546, '頭': 4547, '頰': 4548, '頷': 4549, '頸': 4550, '頹': 4551, '頻': 4552, '顆': 4553, '題': 4554, '額': 4555, '顎': 4556, '顏': 4557, '顓': 4558, '願': 4559, '顛': 4560, '類': 4561, '顧': 4562, '顫': 4563, '顯': 4564, '顰': 4565, '顱': 4566, '風': 4567, '颯': 4568, '颱': 4569, '颳': 4570, '颶': 4571, '颺': 4572, '颼': 4573, '飄': 4574, '飛': 4575, '食': 4576, '飢': 4577, '飧': 4578, '飩': 4579, '飪': 4580, '飭': 4581, '飯': 4582, '飲': 4583, '飴': 4584, '飼': 4585, '飽': 4586, '飾': 4587, '餃': 4588, '餅': 4589, '餉': 4590, '養': 4591, '餌': 4592, '餐': 4593, '餒': 4594, '餓': 4595, '餘': 4596, '餛': 4597, '餞': 4598, '餡': 4599, '館': 4600, '餵': 4601, '餽': 4602, '餾': 4603, '餿': 4604, '饅': 4605, '饑': 4606, '饒': 4607, '饜': 4608, '饞': 4609, '首': 4610, '香': 4611, '馥': 4612, '馨': 4613, '馬': 4614, '馭': 4615, '馮': 4616, '馱': 4617, '馳': 4618, '馴': 4619, '駁': 4620, '駐': 4621, '駑': 4622, '駒': 4623, '駕': 4624, '駙': 4625, '駛': 4626, '駝': 4627, '駟': 4628, '駢': 4629, '駭': 4630, '駱': 4631, '駿': 4632, '騁': 4633, '騎': 4634, '騖': 4635, '騙': 4636, '騫': 4637, '騰': 4638, '騷': 4639, '騾': 4640, '驀': 4641, '驃': 4642, '驅': 4643, '驕': 4644, '驗': 4645, '驚': 4646, '驛': 4647, '驟': 4648, '驢': 4649, '驥': 4650, '驪': 4651, '骨': 4652, '骯': 4653, '骰': 4654, '骷': 4655, '骸': 4656, '骼': 4657, '髏': 4658, '髒': 4659, '髓': 4660, '體': 4661, '高': 4662, '髦': 4663, '髭': 4664, '髮': 4665, '髯': 4666, '髻': 4667, '鬃': 4668, '鬆': 4669, '鬍': 4670, '鬚': 4671, '鬢': 4672, '鬥': 4673, '鬧': 4674, '鬨': 4675, '鬱': 4676, '鬲': 4677, '鬼': 4678, '魁': 4679, '魂': 4680, '魄': 4681, '魅': 4682, '魏': 4683, '魔': 4684, '魘': 4685, '魚': 4686, '魯': 4687, '魷': 4688, '鮑': 4689, '鮪': 4690, '鮫': 4691, '鮮': 4692, '鯉': 4693, '鯊': 4694, '鯧': 4695, '鯨': 4696, '鯽': 4697, '鰍': 4698, '鰓': 4699, '鰥': 4700, '鰭': 4701, '鰱': 4702, '鰻': 4703, '鰾': 4704, '鱉': 4705, '鱔': 4706, '鱖': 4707, '鱗': 4708, '鱷': 4709, '鱸': 4710, '鳥': 4711, '鳩': 4712, '鳳': 4713, '鳴': 4714, '鳶': 4715, '鴆': 4716, '鴉': 4717, '鴒': 4718, '鴕': 4719, '鴛': 4720, '鴣': 4721, '鴦': 4722, '鴨': 4723, '鴻': 4724, '鴿': 4725, '鵑': 4726, '鵝': 4727, '鵠': 4728, '鵡': 4729, '鵪': 4730, '鵬': 4731, '鵲': 4732, '鶉': 4733, '鶯': 4734, '鶴': 4735, '鷂': 4736, '鷓': 4737, '鷗': 4738, '鷥': 4739, '鷹': 4740, '鷺': 4741, '鸚': 4742, '鸞': 4743, '鹹': 4744, '鹼': 4745, '鹽': 4746, '鹿': 4747, '麂': 4748, '麋': 4749, '麒': 4750, '麓': 4751, '麗': 4752, '麝': 4753, '麟': 4754, '麥': 4755, '麩': 4756, '麴': 4757, '麵': 4758, '麻': 4759, '麼': 4760, '麾': 4761, '黃': 4762, '黍': 4763, '黎': 4764, '黏': 4765, '黑': 4766, '黔': 4767, '默': 4768, '黛': 4769, '黜': 4770, '黝': 4771, '點': 4772, '黠': 4773, '黨': 4774, '黯': 4775, '黴': 4776, '黷': 4777, '鼇': 4778, '鼎': 4779, '鼓': 4780, '鼕': 4781, '鼙': 4782, '鼠': 4783, '鼬': 4784, '鼴': 4785, '鼻': 4786, '鼾': 4787, '齊': 4788, '齋': 4789, '齒': 4790, '齜': 4791, '齟': 4792, '齡': 4793, '齣': 4794, '齦': 4795, '齪': 4796, '齬': 4797, '齲': 4798, '齷': 4799, '龍': 4800, '龐': 4801, '龔': 4802}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 資料轉換（預處理 + 增強）\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),         # EfficientNet 建議輸入大小\n",
    "    transforms.Grayscale(num_output_channels=3),  # 轉為 RGB 格式（3 通道）\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "# 載入資料集\n",
    "dataset_path = \"./Handwritten_Data\"  # 假設所有字符資料夾都在這個路徑下\n",
    "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# 印出類別對應表\n",
    "print(full_dataset.class_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ddccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Save the class mapping to a JSON file\n",
    "with open('class_mapping.json', 'w') as f:\n",
    "    json.dump(full_dataset.class_to_idx, f)\n",
    "\n",
    "# train / validation\n",
    "from torch.utils.data import random_split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# 建立 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f3e5754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from efficientnet_pytorch) (2.7.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from torch->efficientnet_pytorch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from torch->efficientnet_pytorch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from torch->efficientnet_pytorch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from torch->efficientnet_pytorch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from torch->efficientnet_pytorch) (79.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from sympy>=1.13.3->torch->efficientnet_pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
      "Building wheels for collected packages: efficientnet_pytorch\n",
      "  Building wheel for efficientnet_pytorch (pyproject.toml): started\n",
      "  Building wheel for efficientnet_pytorch (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16519 sha256=dfe68ef8d02db1fe4724103b4e4e29a704678ceaa3e7a3dac437bdb5eee8f8be\n",
      "  Stored in directory: c:\\users\\yahui\\appdata\\local\\pip\\cache\\wheels\\9c\\3f\\43\\e6271c7026fe08c185da2be23c98c8e87477d3db63f41f32ad\n",
      "Successfully built efficientnet_pytorch\n",
      "Installing collected packages: efficientnet_pytorch\n",
      "Successfully installed efficientnet_pytorch-0.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b693540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 可用，使用 GPU 訓練\n",
      "當前 GPU 名稱: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU 數量: 1\n",
      "當前使用的 GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA 版本: 11.8\n",
      "PyTorch 版本: 2.7.0+cu118\n",
      "使用設備: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Step 1: Setup parameters and check CUDA\n",
    "def check_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        current_device = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(current_device)\n",
    "        print(f\"CUDA 可用，使用 GPU 訓練\")\n",
    "        print(f\"當前 GPU 名稱: {device_name}\")\n",
    "        print(f\"GPU 數量: {device_count}\")\n",
    "        print(f\"當前使用的 GPU: {device_name}\")\n",
    "        print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "        print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"CUDA 不可用，將使用 CPU 訓練\")\n",
    "        return False\n",
    "\n",
    "use_gpu = check_cuda()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 設定本地路徑\n",
    "data_dir = 'Handwritten_Data' # 你的資料路徑\n",
    "batch_size = 64 if use_gpu else 16  # GPU 可以用更大的 batch size\n",
    "lr = 0.001  # 稍微降低學習率以獲得更穩定的訓練\n",
    "momentum = 0.9\n",
    "num_epochs = 3  # 增加訓練輪數\n",
    "input_size = 224\n",
    "net_name = 'efficientnet-b0'\n",
    "\n",
    "# 設定 CUDA 優化參數\n",
    "if use_gpu:\n",
    "    torch.backends.cudnn.benchmark = True  # 加速 convolution 運算\n",
    "    torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5ef0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define data loading functions\n",
    "def loaddata(data_dir, batch_size, shuffle=True):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # 原始無轉換資料集\n",
    "    raw_dataset = datasets.ImageFolder(root=data_dir, transform=transforms.ToTensor())\n",
    "\n",
    "    train_size = int(0.8 * len(raw_dataset))\n",
    "    val_size = len(raw_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(raw_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    class DatasetWrapper(torch.utils.data.Dataset):\n",
    "        def __init__(self, subset, transform):\n",
    "            self.subset = subset\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.subset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image, label = self.subset.dataset[self.subset.indices[idx]]\n",
    "            image = transforms.ToPILImage()(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "    train_dataset_wrapped = DatasetWrapper(train_dataset, data_transforms['train'])\n",
    "    val_dataset_wrapped = DatasetWrapper(val_dataset, data_transforms['val'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset_wrapped, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=use_gpu)\n",
    "    val_loader = DataLoader(val_dataset_wrapped, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=use_gpu)\n",
    "\n",
    "    return train_loader, val_loader, train_size, val_size, len(raw_dataset.classes), raw_dataset, train_dataset_wrapped\n",
    "\n",
    "# 2. 顯示原圖與轉換後的圖\n",
    "def show_images(raw_data, transformed_data, label_map=None, num_images=5, figsize_per_image=(1.5, 1.5)):\n",
    "    num_cols = num_images\n",
    "    plt.figure(figsize=(figsize_per_image[0] * num_cols, figsize_per_image[1] * 2))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        orig_img, label = raw_data[i]\n",
    "        trans_img, _ = transformed_data[i]\n",
    "\n",
    "        label_text = label_map[label] if label_map else str(label)\n",
    "\n",
    "        # 原圖\n",
    "        plt.subplot(2, num_cols, i + 1)\n",
    "        if isinstance(orig_img, torch.Tensor):\n",
    "            img_np = orig_img.permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            img_np = orig_img\n",
    "        plt.imshow(img_np)\n",
    "        plt.title(f\"Original\\n{label_text}\", fontsize=10)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # 轉換圖（需反正規化）\n",
    "        if isinstance(trans_img, torch.Tensor):\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "            trans_img = trans_img * std + mean\n",
    "            trans_img = trans_img.clamp(0, 1)\n",
    "            img_np = trans_img.permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            img_np = trans_img\n",
    "\n",
    "        plt.subplot(2, num_cols, num_cols + i + 1)\n",
    "        plt.imshow(img_np)\n",
    "        plt.title(\"Transformed\", fontsize=10)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc2c55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs=10):\n",
    "    since = time.time()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # 記錄用\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    # 👉 新增指標記錄\n",
    "    train_precisions, val_precisions = [], []\n",
    "    train_recalls, val_recalls = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "\n",
    "    print(f\"開始訓練，使用設備: {device}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 40)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'當前學習率: {current_lr:.6f}')\n",
    "\n",
    "        # 訓練階段\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # 👉 預測與標籤收集\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            all_train_preds.append(preds.cpu())\n",
    "            all_train_labels.append(labels.cpu())\n",
    "\n",
    "            if i % 50 == 0 or i == len(train_loader) - 1:\n",
    "                print(f'  批次 [{i+1}/{len(train_loader)}] Loss: {loss.item():.4f}')\n",
    "\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        print(f'訓練 Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc.cpu().item())\n",
    "\n",
    "        # 👉 統計訓練 Precision/Recall/F1\n",
    "        train_preds = torch.cat(all_train_preds)\n",
    "        train_labels = torch.cat(all_train_labels)\n",
    "        train_precisions.append(precision_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "        train_recalls.append(recall_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "        train_f1s.append(f1_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "\n",
    "        # 驗證階段\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                all_val_preds.append(preds.cpu())\n",
    "                all_val_labels.append(labels.cpu())\n",
    "\n",
    "        epoch_loss = running_loss / val_size\n",
    "        epoch_acc = running_corrects.double() / val_size\n",
    "        print(f'驗證 Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        val_losses.append(epoch_loss)\n",
    "        val_accs.append(epoch_acc.cpu().item())\n",
    "\n",
    "        # 👉 統計驗證 Precision/Recall/F1\n",
    "        val_preds = torch.cat(all_val_preds)\n",
    "        val_labels = torch.cat(all_val_labels)\n",
    "        val_precisions.append(precision_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        val_recalls.append(recall_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        val_f1s.append(f1_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            checkpoint_path = os.path.join(data_dir, 'best_model_checkpoint.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': epoch_loss,\n",
    "                'acc': epoch_acc,\n",
    "            }, checkpoint_path)\n",
    "            print(f'新的最佳模型已儲存，準確率: {best_acc:.4f}')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'訓練完成，耗時 {time_elapsed // 60:.0f}分 {time_elapsed % 60:.0f}秒')\n",
    "    print(f'最佳驗證準確率: {best_acc:.4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    save_dir = os.path.join(data_dir, 'model')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = os.path.join('./best', 'best.pth')\n",
    "    torch.save(model, model_path)\n",
    "    print(f'最終模型已儲存至: {model_path}')\n",
    "\n",
    "    # 👉 繪圖（Loss, Accuracy, Precision, Recall, F1）\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='訓練 Loss')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='驗證 Loss')\n",
    "    plt.title('Loss 變化')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, num_epochs+1), train_accs, 'b-', label='訓練準確率')\n",
    "    plt.plot(range(1, num_epochs+1), val_accs, 'r-', label='驗證準確率')\n",
    "    plt.title('Accuracy 變化')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, num_epochs+1), train_precisions, 'b--', label='訓練 Precision')\n",
    "    plt.plot(range(1, num_epochs+1), val_precisions, 'r--', label='驗證 Precision')\n",
    "    plt.plot(range(1, num_epochs+1), train_recalls, 'b:', label='訓練 Recall')\n",
    "    plt.plot(range(1, num_epochs+1), val_recalls, 'r:', label='驗證 Recall')\n",
    "    plt.title('Precision / Recall 變化')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, num_epochs+1), train_f1s, 'b-', label='訓練 F1')\n",
    "    plt.plot(range(1, num_epochs+1), val_f1s, 'r-', label='驗證 F1')\n",
    "    plt.title('F1-score 變化')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    curve_path = os.path.join(save_dir, 'training_curves_all_metrics.png')\n",
    "    plt.savefig(curve_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'訓練曲線（包含 F1、Precision、Recall）已儲存至: {curve_path}')\n",
    "\n",
    "    return model, best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a456b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始手寫字符辨識訓練\n",
      "============================================================\n",
      "載入資料...\n",
      "資料載入失敗: too many values to unpack (expected 6)\n"
     ]
    }
   ],
   "source": [
    "# # Step 5: Main function\n",
    "# def main():\n",
    "#     \"\"\"主程式\"\"\"\n",
    "#     print(\"開始手寫字符辨識訓練\")\n",
    "#     print(\"=\" * 60)\n",
    "    \n",
    "#     # 檢查資料路徑\n",
    "#     if not os.path.exists(data_dir):\n",
    "#         print(f\"資料夾不存在: {data_dir}\")\n",
    "#         print(\"請確認資料夾路徑是否正確\")\n",
    "#         return\n",
    "    \n",
    "#     print(\"載入資料...\")\n",
    "#     try:\n",
    "#         train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(data_dir, batch_size)\n",
    "#         print(f\"資料載入成功\")\n",
    "#         print(f\"   訓練樣本: {train_size}\")\n",
    "#         print(f\"   驗證樣本: {val_size}\")\n",
    "#         print(f\"   字符類別: {class_num}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"資料載入失敗: {e}\")\n",
    "#         return\n",
    "\n",
    "#     print(\"\\n建立模型...\")\n",
    "#     try:\n",
    "#         # 載入預訓練的 EfficientNet\n",
    "#         model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        \n",
    "#         # 修改最後一層以適應我們的分類任務\n",
    "#         num_ftrs = model._fc.in_features\n",
    "#         model._fc = nn.Linear(num_ftrs, class_num)\n",
    "        \n",
    "#         # 移動模型到 GPU\n",
    "#         model = model.to(device)\n",
    "#         print(f\"模型建立成功，參數數量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "#         # 顯示 GPU 記憶體使用量\n",
    "#         if use_gpu:\n",
    "#             print(f\"GPU 記憶體使用: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"模型建立失敗: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # 損失函數和優化器\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)  # 使用 AdamW\n",
    "#     scheduler = get_lr_scheduler(optimizer)\n",
    "\n",
    "#     print(f\"\\n 開始訓練...\")\n",
    "#     print(f\"   批次大小: {batch_size}\")\n",
    "#     print(f\"   學習率: {lr}\")\n",
    "#     print(f\"   訓練輪數: {num_epochs}\")\n",
    "#     print(f\"   使用設備: {device}\")\n",
    "    \n",
    "#     try:\n",
    "#         model, best_acc = train_model(\n",
    "#             model, criterion, optimizer, scheduler,\n",
    "#             train_loader, val_loader,\n",
    "#             train_size, val_size,\n",
    "#             num_epochs=num_epochs\n",
    "#         )\n",
    "        \n",
    "#         print(f\"\\n🎉 訓練完成！最佳準確率: {best_acc:.4f}\")\n",
    "        \n",
    "#         # 測試幾個樣本\n",
    "#         print(\"\\n測試幾個樣本...\")\n",
    "#         test_samples(model, val_loader, class_names, device)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"訓練過程中發生錯誤: {e}\")\n",
    "#         return\n",
    "\n",
    "# def test_samples(model, val_loader, class_names, device, num_samples=10):\n",
    "#     \"\"\"測試幾個樣本\"\"\"\n",
    "#     model.eval()\n",
    "#     samples_tested = 0\n",
    "#     correct_predictions = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "#             # 顯示前幾個預測結果\n",
    "#             for i in range(min(len(preds), num_samples - samples_tested)):\n",
    "#                 pred_class = class_names[preds[i]]\n",
    "#                 true_class = class_names[labels[i]]\n",
    "#                 is_correct = preds[i] == labels[i]\n",
    "                \n",
    "#                 status = \"YYYYYY\" if is_correct else \"NNNNNN\"\n",
    "#                 print(f\"{status} 預測: {pred_class} | 實際: {true_class}\")\n",
    "                \n",
    "#                 if is_correct:\n",
    "#                     correct_predictions += 1\n",
    "#                 samples_tested += 1\n",
    "                \n",
    "#                 if samples_tested >= num_samples:\n",
    "#                     break\n",
    "            \n",
    "#             if samples_tested >= num_samples:\n",
    "#                 break\n",
    "    \n",
    "#     accuracy = correct_predictions / samples_tested\n",
    "#     print(f\"\\n測試樣本準確率: {accuracy:.4f} ({correct_predictions}/{samples_tested})\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 可用，使用 GPU 訓練\n",
      "當前 GPU 名稱: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU 數量: 1\n",
      "當前使用的 GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA 版本: 11.8\n",
      "PyTorch 版本: 2.7.0+cu118\n",
      "使用設備: cuda\n",
      "Starting handwritten character recognition training\n",
      "============================================================\n",
      "Loading data...\n",
      "Data loaded successfully\n",
      "   Training samples: 200569\n",
      "   Validation samples: 50143\n",
      "   Character classes: 4803\n",
      "   Class names: ['丁', '七', '丈', '三', '上', '下', '不', '丐', '丑', '且', '丕', '世', '丘', '丙', '丞', '丟', '並', '丫', '中', '串', '丸', '丹', '主', '乃', '久', '么', '之', '乍', '乎', '乏', '乒', '乓', '乖', '乘', '乙', '九', '乞', '也', '乩', '乳', '乾', '亂', '了', '予', '事', '二', '于', '云', '互', '五', '井', '亙', '些', '亞', '亟', '亡', '交', '亥', '亦', '亨', '享', '京', '亭', '亮', '人', '什', '仁', '仃', '仄', '仆', '仇', '今', '介', '仍', '仔', '仕', '他', '仗', '付', '仙', '仞', '仟', '代', '令', '以', '仰', '仲', '仳', '件', '任', '份', '仿', '企', '伉', '伊', '伍', '伏', '伐', '休', '伕', '伙', '伯', '估', '伴', '伶', '伸', '伺', '似', '伽', '佃', '但', '佇', '位', '低', '住', '佐', '佑', '佔', '何', '佗', '余', '佛', '作', '佝', '佞', '你', '佣', '佩', '佬', '佯', '佰', '佳', '併', '佻', '佾', '使', '侃', '來', '侈', '例', '侍', '侏', '侖', '供', '依', '侮', '侯', '侵', '侶', '便', '係', '促', '俄', '俊', '俎', '俏', '俐', '俑', '俗', '俘', '俚', '保', '俞', '俟', '俠', '信', '修', '俯', '俱', '俳', '俸', '俺', '俾', '倀', '倆', '倉', '個', '倌', '倍', '倏', '們', '倒', '倔', '倖', '倘', '候', '倚', '借', '倡', '倣', '倥', '倦', '倨', '倩', '倪', '倫', '倭', '值', '偃', '假', '偉', '偌', '偎', '偏', '偕', '做', '停', '健', '側', '偵', '偶', '偷', '偺', '偽', '傀', '傅', '傍', '傑', '傖', '傘', '備', '傢', '催', '傭', '傯', '傲', '傳', '債', '傷', '傻', '傾', '僅', '像', '僑', '僕', '僖', '僚', '僥', '僧', '僭', '僮', '僱', '僵', '價', '僻', '儀', '儂', '億', '儈', '儉', '儐', '儒', '儔', '儘', '償', '儡', '優', '儲', '儷', '儼', '兀', '允', '元', '兄', '充', '兆', '兇', '先', '光', '克', '兌', '免', '兒', '兔', '兕', '兗', '兜', '兢', '入', '內', '全', '兩', '八', '公', '六', '兮', '共', '兵', '其', '具', '典', '兼', '冀', '冉', '冊', '再', '冑', '冒', '冕', '冗', '冠', '冢', '冤', '冥', '冬', '冰', '冶', '冷', '冽', '准', '凋', '凌', '凍', '凜', '凝', '几', '凡', '凰', '凱', '凳', '凶', '凸', '凹', '出', '函', '刀', '刁', '刃', '分', '切', '刈', '刊', '刎', '刑', '划', '列', '初', '判', '別', '刨', '利', '刪', '刮', '到', '制', '刷', '券', '刻', '剁', '剃', '則', '削', '剋', '剌', '前', '剎', '剔', '剖', '剛', '剜', '剝', '剩', '剪', '副', '割', '剴', '創', '剷', '剽', '剿', '劃', '劇', '劈', '劉', '劍', '劑', '力', '功', '加', '劣', '助', '努', '劫', '劬', '劾', '勁', '勃', '勇', '勉', '勒', '動', '勗', '勘', '務', '勛', '勝', '勞', '募', '勢', '勤', '勦', '勵', '勸', '勻', '勾', '勿', '包', '匆', '匈', '匍', '匏', '匐', '匕', '化', '北', '匙', '匝', '匠', '匡', '匣', '匪', '匯', '匱', '匹', '匾', '匿', '區', '十', '千', '卅', '升', '午', '卉', '半', '卑', '卒', '卓', '協', '南', '博', '卜', '卞', '占', '卡', '卦', '卮', '卯', '印', '危', '即', '卵', '卷', '卸', '卹', '卻', '卿', '厄', '厚', '厝', '原', '厥', '厭', '厲', '去', '參', '又', '叉', '及', '友', '反', '叔', '取', '受', '叛', '叟', '叢', '口', '古', '句', '另', '叨', '叩', '只', '叫', '召', '叭', '叮', '可', '台', '叱', '史', '右', '叵', '司', '叼', '吁', '吃', '各', '吆', '合', '吉', '吊', '吋', '同', '名', '后', '吏', '吐', '向', '吒', '君', '吝', '吞', '吟', '吠', '否', '吧', '吩', '含', '吭', '吮', '吱', '吳', '吵', '吶', '吸', '吹', '吻', '吼', '吾', '呀', '呂', '呃', '呆', '呈', '告', '呎', '呢', '周', '呱', '味', '呵', '呶', '呷', '呸', '呻', '呼', '命', '咀', '咄', '咆', '咋', '和', '咎', '咐', '咒', '咕', '咖', '咚', '咦', '咨', '咪', '咫', '咬', '咯', '咱', '咳', '咸', '咻', '咽', '哀', '品', '哂', '哄', '哇', '哈', '哉', '哎', '員', '哥', '哦', '哨', '哩', '哪', '哭', '哮', '哲', '哺', '哼', '唁', '唆', '唉', '唐', '唔', '唧', '唬', '售', '唯', '唱', '唳', '唷', '唸', '唾', '啃', '啄', '商', '啊', '問', '啕', '啖', '啜', '啞', '啟', '啡', '啣', '啤', '啦', '啪', '啻', '啼', '啾', '喀', '喂', '喃', '善', '喇', '喉', '喊', '喋', '喔', '喘', '喚', '喜', '喝', '喟', '喧', '喪', '喬', '單', '喱', '喲', '喳', '喻', '嗅', '嗆', '嗇', '嗎', '嗑', '嗓', '嗚', '嗜', '嗟', '嗡', '嗣', '嗤', '嗥', '嗦', '嗨', '嗯', '嗷', '嗽', '嗾', '嘀', '嘆', '嘈', '嘉', '嘍', '嘎', '嘔', '嘖', '嘗', '嘛', '嘟', '嘩', '嘮', '嘯', '嘰', '嘲', '嘴', '嘶', '嘹', '嘻', '嘿', '噎', '噓', '噗', '噙', '噢', '噤', '噥', '器', '噩', '噪', '噫', '噬', '噯', '噱', '噴', '噸', '噹', '嚀', '嚅', '嚇', '嚎', '嚏', '嚐', '嚕', '嚥', '嚨', '嚮', '嚴', '嚶', '嚷', '嚼', '囀', '囁', '囂', '囈', '囉', '囊', '囌', '囑', '囚', '四', '回', '因', '囤', '囪', '困', '固', '圃', '圈', '國', '圍', '園', '圓', '圖', '團', '土', '在', '圬', '圭', '圯', '地', '圳', '圾', '址', '均', '坊', '坍', '坎', '坏', '坐', '坑', '坡', '坤', '坦', '坩', '坪', '坷', '坼', '垂', '垃', '型', '垠', '垢', '垣', '垮', '埂', '埃', '埋', '城', '埔', '域', '埠', '埤', '執', '培', '基', '堂', '堅', '堆', '堊', '堡', '堤', '堪', '堯', '堰', '報', '場', '堵', '塊', '塌', '塑', '塔', '塗', '塘', '塚', '塞', '塢', '填', '塭', '塵', '塹', '塾', '墀', '境', '墅', '墊', '墓', '墜', '增', '墟', '墨', '墮', '墳', '墾', '壁', '壅', '壇', '壑', '壓', '壕', '壘', '壙', '壞', '壟', '壢', '壤', '壩', '士', '壬', '壯', '壹', '壺', '壽', '夏', '夔', '夕', '外', '夙', '多', '夜', '夠', '夢', '夤', '夥', '大', '天', '太', '夫', '夭', '央', '失', '夷', '夸', '夾', '奄', '奇', '奈', '奉', '奎', '奏', '奐', '契', '奔', '奕', '套', '奘', '奚', '奠', '奢', '奧', '奩', '奪', '奮', '女', '奴', '奶', '奸', '她', '好', '妁', '如', '妃', '妄', '妊', '妍', '妒', '妓', '妖', '妙', '妝', '妞', '妣', '妤', '妥', '妨', '妮', '妯', '妳', '妹', '妻', '妾', '姆', '姊', '始', '姍', '姐', '姑', '姒', '姓', '委', '姘', '姚', '姜', '姣', '姥', '姦', '姨', '姪', '姬', '姻', '姿', '威', '娃', '娌', '娑', '娓', '娘', '娛', '娜', '娟', '娠', '娣', '娥', '娩', '娶', '娼', '婀', '婁', '婆', '婉', '婊', '婚', '婢', '婦', '婪', '婷', '婿', '媒', '媚', '媛', '媲', '媳', '媼', '媽', '媾', '嫁', '嫂', '嫉', '嫌', '嫖', '嫗', '嫘', '嫡', '嫣', '嫦', '嫩', '嫵', '嫻', '嬉', '嬋', '嬌', '嬝', '嬤', '嬪', '嬰', '嬴', '嬸', '孀', '子', '孑', '孓', '孔', '孕', '字', '存', '孚', '孜', '孝', '孟', '季', '孤', '孩', '孫', '孰', '孱', '孳', '孵', '學', '孺', '孽', '孿', '它', '宅', '宇', '守', '安', '宋', '完', '宏', '宗', '官', '宙', '定', '宛', '宜', '客', '宣', '室', '宥', '宦', '宮', '宰', '害', '宴', '宵', '家', '宸', '容', '宿', '寂', '寄', '寅', '密', '寇', '富', '寐', '寒', '寓', '寞', '察', '寡', '寢', '寤', '寥', '實', '寧', '寨', '審', '寫', '寬', '寮', '寵', '寶', '寸', '寺', '封', '射', '將', '專', '尉', '尊', '尋', '對', '導', '小', '少', '尖', '尚', '尤', '尬', '就', '尷', '尸', '尹', '尺', '尼', '尾', '尿', '局', '屁', '居', '屆', '屈', '屋', '屍', '屎', '屏', '屐', '屑', '展', '屜', '屠', '屢', '層', '履', '屬', '屯', '山', '屹', '岌', '岐', '岑', '岔', '岡', '岩', '岫', '岱', '岳', '岷', '岸', '峙', '峨', '峪', '峭', '峰', '島', '峻', '峽', '崁', '崆', '崇', '崎', '崑', '崔', '崖', '崙', '崛', '崢', '崩', '嵌', '嵐', '嵩', '嶄', '嶇', '嶝', '嶺', '嶼', '嶽', '巍', '巒', '巔', '巖', '川', '州', '巡', '巢', '工', '左', '巧', '巨', '巫', '差', '己', '已', '巳', '巴', '巷', '巽', '巾', '市', '布', '帆', '希', '帑', '帕', '帖', '帘', '帚', '帛', '帝', '帥', '師', '席', '帳', '帶', '帷', '常', '帽', '幀', '幅', '幌', '幔', '幗', '幛', '幟', '幢', '幣', '幫', '干', '平', '年', '并', '幸', '幹', '幻', '幼', '幽', '幾', '庇', '床', '序', '底', '庖', '店', '庚', '府', '庠', '度', '座', '庫', '庭', '庵', '庶', '康', '庸', '庾', '廁', '廂', '廈', '廉', '廊', '廓', '廖', '廚', '廝', '廟', '廠', '廢', '廣', '廬', '廳', '延', '廷', '建', '廿', '弁', '弄', '弈', '弊', '式', '弒', '弓', '弔', '引', '弗', '弘', '弛', '弟', '弦', '弧', '弩', '弭', '弱', '張', '強', '弼', '彆', '彈', '彌', '彎', '彗', '彙', '形', '彤', '彥', '彩', '彪', '彫', '彬', '彭', '彰', '影', '彷', '役', '彼', '彿', '往', '征', '待', '徇', '很', '徊', '律', '後', '徐', '徑', '徒', '得', '徘', '徙', '從', '御', '徨', '復', '循', '徬', '微', '徵', '德', '徹', '徽', '心', '必', '忌', '忍', '忖', '志', '忘', '忙', '忝', '忠', '快', '忱', '念', '忽', '忿', '怎', '怏', '怒', '怔', '怕', '怖', '思', '怠', '怡', '急', '性', '怨', '怪', '怯', '怵', '恃', '恆', '恍', '恐', '恕', '恙', '恢', '恣', '恤', '恥', '恨', '恩', '恪', '恫', '恬', '恭', '息', '恰', '恿', '悄', '悅', '悉', '悌', '悍', '悔', '悖', '悚', '悟', '悠', '患', '您', '悲', '悴', '悵', '悶', '悸', '悻', '悼', '悽', '情', '惆', '惋', '惑', '惕', '惘', '惚', '惜', '惟', '惠', '惡', '惦', '惰', '惱', '想', '惴', '惶', '惹', '惺', '惻', '愀', '愁', '愈', '愉', '愎', '意', '愕', '愚', '愛', '愜', '感', '愣', '愧', '愴', '愾', '愿', '慄', '慇', '慈', '態', '慌', '慍', '慎', '慕', '慘', '慚', '慝', '慟', '慢', '慣', '慧', '慨', '慫', '慮', '慰', '慶', '慷', '慼', '慾', '憂', '憊', '憎', '憐', '憑', '憔', '憚', '憤', '憧', '憩', '憫', '憬', '憲', '憶', '憾', '懂', '懇', '懈', '應', '懊', '懍', '懣', '懦', '懲', '懵', '懶', '懷', '懸', '懺', '懼', '懾', '懿', '戀', '戈', '戊', '戌', '戍', '戎', '成', '我', '戒', '戕', '或', '戚', '戛', '戟', '戡', '戢', '截', '戮', '戰', '戲', '戳', '戴', '戶', '戾', '房', '所', '扁', '扇', '扈', '扉', '手', '才', '扎', '扒', '打', '扔', '托', '扛', '扣', '扭', '扮', '扯', '扳', '扶', '批', '扼', '找', '承', '技', '抄', '抉', '把', '抑', '抒', '抓', '投', '抖', '抗', '折', '抨', '披', '抬', '抱', '抵', '抹', '押', '抽', '抿', '拂', '拄', '拆', '拇', '拈', '拉', '拋', '拌', '拍', '拎', '拐', '拒', '拓', '拔', '拖', '拗', '拘', '拙', '拚', '招', '拜', '括', '拭', '拮', '拯', '拱', '拳', '拴', '拷', '拼', '拽', '拾', '拿', '持', '指', '挈', '按', '挑', '挖', '挨', '挪', '挫', '振', '挺', '挽', '挾', '捂', '捆', '捉', '捎', '捏', '捐', '捕', '捧', '捨', '捩', '捫', '捱', '捲', '捶', '捷', '捻', '掀', '掃', '掄', '授', '掉', '掌', '掏', '排', '掖', '掘', '掙', '掛', '掠', '採', '探', '掣', '接', '控', '推', '掩', '措', '掬', '揀', '揆', '揉', '揍', '描', '提', '插', '揖', '揚', '換', '握', '揣', '揩', '揪', '揭', '揮', '援', '損', '搏', '搓', '搔', '搖', '搗', '搜', '搞', '搪', '搬', '搭', '搶', '搽', '搾', '摑', '摒', '摔', '摘', '摟', '摧', '摩', '摯', '摸', '摹', '摺', '撇', '撈', '撐', '撒', '撓', '撕', '撚', '撞', '撤', '撥', '撩', '撫', '撬', '播', '撮', '撰', '撲', '撻', '撼', '撿', '擁', '擂', '擄', '擅', '擇', '擊', '擋', '操', '擎', '擒', '擔', '擘', '據', '擠', '擦', '擬', '擰', '擱', '擲', '擴', '擺', '擻', '擾', '攀', '攆', '攏', '攔', '攘', '攙', '攜', '攝', '攣', '攤', '攪', '攫', '攬', '支', '收', '改', '攻', '放', '政', '故', '效', '敏', '救', '敖', '敗', '敘', '教', '敝', '敞', '敢', '散', '敦', '敬', '敲', '整', '敵', '敷', '數', '斂', '斃', '文', '斐', '斑', '斗', '料', '斜', '斟', '斡', '斤', '斥', '斧', '斫', '斬', '斯', '新', '斷', '方', '於', '施', '旁', '旅', '旋', '旌', '旎', '族', '旖', '旗', '既', '日', '旦', '旨', '早', '旬', '旭', '旱', '旺', '昀', '昂', '昆', '昌', '明', '昏', '易', '昔', '星', '映', '春', '昧', '昨', '昭', '是', '時', '晃', '晉', '晌', '晏', '晒', '晚', '晝', '晤', '晦', '晨', '普', '景', '晰', '晴', '晶', '智', '暇', '暈', '暉', '暑', '暖', '暗', '暢', '暨', '暫', '暮', '暴', '暹', '曆', '曉', '曖', '曙', '曝', '曠', '曦', '曰', '曲', '曳', '更', '曷', '書', '曹', '曼', '曾', '替', '最', '會', '月', '有', '朋', '服', '朔', '朕', '朗', '望', '朝', '期', '朦', '朧', '木', '未', '末', '本', '札', '朮', '朱', '朴', '朵', '朽', '杉', '李', '杏', '材', '村', '杖', '杜', '杞', '束', '杭', '杯', '杰', '東', '杳', '杵', '杷', '松', '板', '枇', '枉', '枋', '析', '枕', '林', '枚', '果', '枝', '枯', '枴', '架', '枸', '柄', '柏', '某', '柑', '染', '柔', '柚', '柞', '查', '柩', '柬', '柯', '柱', '柳', '柴', '柵', '柿', '栓', '栗', '校', '栩', '株', '核', '根', '格', '栽', '桀', '桂', '桃', '桅', '框', '案', '桌', '桐', '桑', '桓', '桔', '桶', '桿', '梁', '梃', '梅', '梆', '梓', '梔', '梗', '條', '梟', '梢', '梧', '梨', '梭', '梯', '械', '梱', '梳', '梵', '棄', '棉', '棋', '棍', '棒', '棕', '棗', '棘', '棚', '棟', '棠', '棣', '棧', '森', '棲', '棵', '棹', '棺', '椅', '植', '椎', '椒', '椰', '楊', '楓', '楔', '楚', '楞', '楠', '楨', '楫', '業', '極', '楷', '楹', '概', '榆', '榔', '榕', '榛', '榜', '榨', '榫', '榭', '榮', '榴', '榷', '榻', '槁', '構', '槌', '槍', '槐', '槓', '槨', '槳', '槽', '樁', '樂', '樅', '樊', '樓', '標', '樞', '樟', '模', '樣', '樵', '樸', '樹', '樺', '樽', '橄', '橇', '橋', '橘', '橙', '機', '橡', '橢', '橫', '檀', '檄', '檔', '檜', '檢', '檬', '檳', '檸', '檻', '櫂', '櫃', '櫓', '櫚', '櫛', '櫝', '櫥', '櫻', '欄', '權', '欖', '欠', '次', '欣', '欲', '欺', '欽', '款', '歇', '歉', '歌', '歐', '歙', '歟', '歡', '止', '正', '此', '步', '武', '歧', '歪', '歲', '歷', '歸', '歹', '死', '歿', '殃', '殆', '殉', '殊', '殖', '殘', '殤', '殮', '殯', '殲', '段', '殷', '殺', '殼', '殿', '毀', '毅', '毆', '毋', '母', '每', '毒', '毓', '比', '毗', '毛', '毫', '毯', '毽', '氏', '氐', '民', '氓', '氖', '氛', '氟', '氣', '氤', '氦', '氧', '氨', '氫', '氮', '氯', '氳', '水', '永', '氾', '汀', '汁', '求', '汐', '汕', '汗', '汙', '汝', '汞', '江', '池', '汨', '汪', '汰', '汲', '決', '汽', '汾', '沁', '沃', '沅', '沈', '沉', '沌', '沐', '沒', '沖', '沙', '沛', '沫', '沮', '沱', '河', '沸', '油', '治', '沼', '沽', '沾', '沿', '況', '泄', '泅', '泉', '泊', '泌', '泓', '法', '泗', '泛', '泡', '波', '泣', '泥', '注', '泰', '泱', '泳', '洋', '洌', '洗', '洛', '洞', '津', '洪', '洱', '洲', '洶', '活', '洽', '派', '流', '浙', '浚', '浦', '浩', '浪', '浬', '浮', '浴', '海', '浸', '涇', '消', '涉', '涎', '涓', '涕', '涮', '涯', '液', '涵', '涸', '涼', '淄', '淅', '淆', '淇', '淋', '淌', '淑', '淒', '淘', '淙', '淚', '淞', '淡', '淤', '淨', '淪', '淫', '淮', '深', '淳', '淵', '混', '淹', '淺', '添', '清', '渙', '渚', '減', '渝', '渠', '渡', '渣', '渤', '渥', '渦', '測', '渭', '港', '渲', '渴', '游', '渺', '渾', '湃', '湊', '湍', '湔', '湖', '湘', '湛', '湧', '湮', '湯', '溉', '源', '準', '溘', '溜', '溝', '溢', '溥', '溪', '溫', '溯', '溶', '溺', '溼', '滂', '滄', '滅', '滇', '滋', '滌', '滑', '滓', '滔', '滬', '滯', '滲', '滴', '滾', '滿', '漁', '漂', '漆', '漏', '漓', '演', '漕', '漠', '漢', '漣', '漩', '漪', '漫', '漬', '漯', '漱', '漲', '漳', '漸', '漾', '漿', '潑', '潔', '潘', '潛', '潤', '潦', '潭', '潮', '潰', '潸', '潺', '潼', '澀', '澄', '澆', '澈', '澎', '澗', '澡', '澤', '澧', '澱', '澳', '澹', '激', '濁', '濂', '濃', '濘', '濛', '濟', '濠', '濡', '濤', '濫', '濬', '濯', '濱', '濺', '濾', '瀆', '瀉', '瀋', '瀏', '瀑', '瀕', '瀚', '瀛', '瀝', '瀟', '瀨', '瀰', '瀾', '灌', '灑', '灘', '灣', '灤', '火', '灰', '灶', '灸', '灼', '災', '炊', '炎', '炒', '炕', '炙', '炫', '炬', '炭', '炮', '炯', '炳', '炸', '為', '烈', '烊', '烏', '烘', '烙', '烤', '烹', '烽', '焉', '焊', '焙', '焚', '無', '焦', '焰', '然', '煉', '煌', '煎', '煙', '煜', '煞', '煤', '煥', '煦', '照', '煩', '煬', '煮', '煽', '熄', '熊', '熔', '熙', '熟', '熨', '熬', '熱', '熹', '熾', '燃', '燄', '燈', '燉', '燎', '燐', '燒', '燕', '燙', '燜', '營', '燥', '燦', '燧', '燬', '燭', '燮', '燴', '燻', '爆', '爍', '爐', '爛', '爨', '爪', '爬', '爭', '爰', '爵', '父', '爸', '爹', '爺', '爻', '爽', '爾', '牆', '片', '版', '牌', '牒', '牖', '牘', '牙', '牛', '牝', '牟', '牠', '牡', '牢', '牧', '物', '牯', '牲', '牴', '特', '牽', '犀', '犁', '犄', '犒', '犖', '犛', '犢', '犧', '犬', '犯', '狀', '狂', '狄', '狎', '狐', '狗', '狙', '狠', '狡', '狩', '狷', '狸', '狹', '狼', '狽', '猓', '猖', '猙', '猛', '猜', '猥', '猩', '猴', '猶', '猷', '猾', '猿', '獄', '獅', '獎', '獐', '獗', '獨', '獰', '獲', '獵', '獷', '獸', '獺', '獻', '玀', '玄', '率', '玉', '王', '玖', '玟', '玨', '玩', '玫', '玲', '玳', '玷', '玻', '珀', '珊', '珍', '珠', '班', '珮', '現', '球', '琅', '理', '琉', '琊', '琍', '琢', '琥', '琪', '琳', '琴', '琵', '琶', '琺', '琿', '瑁', '瑕', '瑙', '瑚', '瑛', '瑜', '瑞', '瑟', '瑣', '瑤', '瑩', '瑪', '瑯', '瑰', '璃', '璋', '璜', '璣', '璦', '璧', '璩', '環', '璽', '瓊', '瓏', '瓜', '瓠', '瓢', '瓣', '瓦', '瓶', '瓷', '甄', '甌', '甕', '甘', '甚', '甜', '生', '產', '甥', '甦', '用', '甩', '甫', '甬', '甭', '田', '由', '甲', '申', '男', '甸', '甽', '界', '畏', '畔', '留', '畚', '畜', '畝', '畢', '略', '畦', '番', '畫', '異', '當', '畸', '疆', '疇', '疊', '疋', '疏', '疑', '疙', '疚', '疝', '疤', '疥', '疫', '疲', '疳', '疵', '疹', '疼', '疽', '疾', '病', '症', '痊', '痔', '痕', '痘', '痙', '痛', '痞', '痢', '痣', '痰', '痱', '痲', '痴', '痺', '痿', '瘀', '瘁', '瘉', '瘋', '瘍', '瘓', '瘟', '瘠', '瘡', '瘤', '瘦', '瘧', '瘩', '瘴', '瘸', '療', '癆', '癌', '癒', '癖', '癘', '癢', '癥', '癩', '癬', '癮', '癱', '癲', '癸', '登', '發', '白', '百', '皂', '的', '皆', '皇', '皈', '皎', '皓', '皖', '皚', '皮', '皰', '皴', '皺', '皿', '盂', '盃', '盆', '盈', '益', '盍', '盎', '盒', '盔', '盛', '盜', '盞', '盟', '盡', '監', '盤', '盥', '盧', '盪', '目', '盯', '盲', '直', '相', '盹', '盼', '盾', '省', '眉', '看', '真', '眠', '眨', '眩', '眶', '眷', '眸', '眺', '眼', '眾', '睏', '睛', '睜', '睞', '睡', '督', '睥', '睦', '睨', '睪', '睫', '睬', '睹', '睽', '睿', '瞄', '瞇', '瞌', '瞎', '瞑', '瞞', '瞟', '瞠', '瞥', '瞧', '瞪', '瞬', '瞭', '瞰', '瞳', '瞻', '瞽', '瞿', '矇', '矓', '矗', '矚', '矛', '矜', '矢', '矣', '知', '矩', '短', '矮', '矯', '石', '矽', '砂', '砌', '砍', '研', '砝', '砥', '砧', '砭', '砰', '破', '砷', '砸', '硃', '硝', '硫', '硬', '硯', '硼', '碉', '碌', '碎', '碑', '碗', '碘', '碟', '碧', '碩', '碰', '碳', '確', '碼', '碾', '磁', '磅', '磊', '磋', '磐', '磕', '磚', '磨', '磬', '磯', '磴', '磷', '磺', '礁', '礎', '礙', '礦', '礪', '礫', '礬', '示', '社', '祀', '祁', '祆', '祇', '祈', '祉', '祐', '祕', '祖', '祗', '祚', '祝', '神', '祟', '祠', '祥', '票', '祭', '祺', '祿', '禁', '禍', '禎', '福', '禦', '禧', '禪', '禮', '禱', '禹', '禽', '禾', '禿', '秀', '私', '秉', '秋', '科', '秒', '租', '秣', '秤', '秦', '秧', '秩', '移', '稀', '稅', '稈', '程', '稍', '稔', '稚', '稜', '稟', '稠', '種', '稱', '稷', '稻', '稼', '稽', '稿', '穀', '穆', '穌', '積', '穎', '穗', '穡', '穢', '穩', '穫', '穴', '究', '穹', '空', '穿', '突', '窄', '窈', '窒', '窕', '窖', '窗', '窘', '窟', '窠', '窩', '窪', '窮', '窯', '窺', '竄', '竅', '竇', '竊', '立', '站', '竟', '章', '竣', '童', '竭', '端', '競', '竹', '竺', '竽', '竿', '笆', '笑', '笙', '笛', '笞', '笠', '符', '笨', '第', '筆', '等', '筋', '筍', '筏', '筐', '筒', '答', '策', '筠', '筵', '筷', '箋', '箏', '箔', '箕', '算', '箝', '管', '箭', '箱', '箴', '節', '篁', '範', '篆', '篇', '築', '篙', '篛', '篡', '篤', '篩', '篷', '篾', '簇', '簍', '簑', '簞', '簡', '簣', '簧', '簪', '簫', '簷', '簸', '簽', '簾', '簿', '籃', '籌', '籍', '籐', '籟', '籠', '籤', '籬', '籮', '籲', '米', '粉', '粒', '粗', '粟', '粥', '粱', '粳', '粵', '粹', '粽', '精', '糊', '糕', '糖', '糙', '糜', '糞', '糟', '糠', '糢', '糧', '糯', '糸', '系', '糾', '紀', '紂', '約', '紅', '紇', '紉', '紊', '紋', '納', '紐', '純', '紕', '紗', '紙', '級', '紛', '紜', '素', '紡', '索', '紫', '紮', '累', '細', '紳', '紹', '紼', '絀', '終', '絃', '組', '絆', '結', '絕', '絞', '絡', '絢', '給', '絨', '絮', '統', '絲', '絹', '綁', '綏', '綑', '經', '綜', '綞', '綠', '綢', '維', '綰', '綱', '網', '綴', '綵', '綸', '綺', '綻', '綽', '綾', '綿', '緇', '緊', '緒', '緘', '線', '緝', '緞', '締', '緣', '編', '緩', '緬', '緯', '練', '緻', '縈', '縊', '縑', '縛', '縣', '縫', '縮', '縱', '縲', '縷', '總', '績', '繁', '繃', '繅', '繆', '織', '繕', '繚', '繞', '繡', '繩', '繪', '繫', '繭', '繹', '繼', '繽', '纂', '續', '纏', '纓', '纖', '纜', '缶', '缸', '缺', '缽', '罄', '罈', '罐', '罔', '罕', '罟', '罩', '罪', '置', '罰', '署', '罵', '罷', '罹', '羅', '羈', '羊', '羋', '羌', '美', '羔', '羚', '羞', '群', '羨', '義', '羯', '羲', '羶', '羸', '羹', '羽', '羿', '翁', '翅', '翌', '翎', '習', '翔', '翕', '翟', '翠', '翡', '翩', '翰', '翱', '翳', '翹', '翻', '翼', '耀', '老', '考', '者', '耆', '而', '耍', '耐', '耒', '耕', '耗', '耘', '耙', '耜', '耳', '耶', '耽', '耿', '聆', '聊', '聖', '聘', '聚', '聞', '聯', '聰', '聱', '聲', '聳', '聶', '職', '聽', '聾', '聿', '肄', '肅', '肆', '肇', '肉', '肋', '肌', '肓', '肖', '肘', '肚', '肛', '肝', '股', '肢', '肥', '肩', '肪', '肫', '肯', '肱', '育', '肴', '肺', '胃', '胄', '背', '胎', '胖', '胚', '胛', '胞', '胡', '胤', '胥', '胭', '胰', '胱', '胳', '胴', '胸', '能', '脂', '脅', '脆', '脈', '脊', '脖', '脣', '脩', '脫', '脯', '脹', '脾', '腆', '腋', '腎', '腐', '腑', '腔', '腕', '腥', '腦', '腫', '腮', '腰', '腱', '腳', '腸', '腹', '腺', '腿', '膀', '膈', '膊', '膏', '膚', '膛', '膜', '膝', '膠', '膨', '膩', '膳', '膺', '膽', '膾', '膿', '臀', '臂', '臃', '臆', '臉', '臍', '臏', '臘', '臚', '臟', '臣', '臥', '臧', '臨', '自', '臬', '臭', '至', '致', '臺', '臻', '臼', '臾', '舀', '舂', '舅', '與', '興', '舉', '舊', '舌', '舍', '舐', '舒', '舔', '舛', '舜', '舞', '舟', '舢', '舨', '航', '舫', '般', '舵', '舶', '舷', '船', '艇', '艘', '艙', '艦', '艮', '良', '艱', '色', '艾', '芋', '芍', '芒', '芙', '芝', '芟', '芥', '芬', '芭', '花', '芳', '芹', '芻', '芽', '苑', '苒', '苓', '苔', '苗', '苛', '苜', '苞', '苟', '苣', '若', '苦', '苧', '英', '茁', '茂', '范', '茄', '茅', '茉', '茗', '茫', '茱', '茲', '茴', '茵', '茶', '茸', '茹', '荀', '草', '荊', '荏', '荐', '荒', '荔', '荷', '荸', '荻', '荼', '莉', '莊', '莎', '莒', '莓', '莖', '莘', '莞', '莠', '莢', '莫', '莽', '菁', '菅', '菊', '菌', '菜', '菠', '菩', '華', '菰', '菱', '菲', '菴', '菸', '菽', '萃', '萄', '萊', '萋', '萌', '萍', '萎', '萬', '萱', '萵', '萸', '萼', '落', '葉', '著', '葛', '葡', '董', '葦', '葩', '葫', '葬', '葵', '葷', '蒂', '蒐', '蒙', '蒜', '蒞', '蒲', '蒸', '蒼', '蒿', '蓀', '蓄', '蓆', '蓉', '蓋', '蓓', '蓬', '蓮', '蓿', '蔑', '蔓', '蔔', '蔗', '蔚', '蔡', '蔣', '蔥', '蔬', '蔭', '蔽', '蕃', '蕈', '蕉', '蕊', '蕙', '蕨', '蕩', '蕪', '蕭', '蕾', '薄', '薇', '薑', '薔', '薛', '薜', '薩', '薪', '薯', '薰', '藉', '藍', '藏', '藐', '藕', '藝', '藤', '藥', '藩', '藪', '藹', '藺', '藻', '蘆', '蘇', '蘊', '蘋', '蘑', '蘗', '蘚', '蘭', '蘸', '蘿', '虎', '虐', '虔', '處', '虛', '虜', '虞', '號', '虧', '虫', '虱', '虹', '蚊', '蚌', '蚓', '蚣', '蚤', '蚩', '蚪', '蚯', '蚱', '蚵', '蚶', '蛀', '蛄', '蛆', '蛇', '蛋', '蛔', '蛙', '蛛', '蛟', '蛤', '蛭', '蛹', '蛻', '蛾', '蜀', '蜂', '蜃', '蜇', '蜈', '蜓', '蜘', '蜜', '蜢', '蜥', '蜴', '蜻', '蜿', '蝌', '蝕', '蝗', '蝙', '蝠', '蝦', '蝨', '蝴', '蝶', '蝸', '螂', '螃', '融', '螞', '螟', '螢', '螫', '螳', '螺', '螻', '蟀', '蟆', '蟈', '蟋', '蟑', '蟒', '蟬', '蟯', '蟲', '蟹', '蟻', '蠅', '蠍', '蠔', '蠕', '蠟', '蠡', '蠢', '蠣', '蠱', '蠶', '蠹', '蠻', '血', '行', '衍', '術', '街', '衙', '衛', '衝', '衡', '衢', '衣', '表', '衫', '衰', '衷', '袁', '袂', '袈', '袋', '袍', '袒', '袖', '袞', '被', '袱', '裁', '裂', '裊', '裔', '裕', '裘', '裙', '補', '裝', '裟', '裡', '裨', '裳', '裴', '裸', '裹', '製', '褂', '複', '褐', '褒', '褓', '褚', '褥', '褪', '褫', '褲', '褶', '褸', '褻', '襄', '襖', '襟', '襠', '襤', '襪', '襯', '襲', '西', '要', '覃', '覆', '見', '規', '覓', '視', '覦', '親', '覬', '覲', '覺', '覽', '觀', '角', '解', '觴', '觸', '言', '訂', '訃', '計', '訊', '訌', '討', '訐', '訓', '訕', '訖', '託', '記', '訛', '訝', '訟', '訣', '訥', '訪', '設', '許', '訴', '診', '註', '証', '詁', '詆', '詐', '詔', '評', '詛', '詞', '詠', '詢', '詣', '試', '詩', '詫', '詬', '詭', '詮', '詰', '話', '該', '詳', '詹', '詼', '誅', '誇', '誌', '認', '誑', '誓', '誕', '誘', '語', '誠', '誡', '誣', '誤', '誥', '誦', '誨', '說', '誰', '課', '誼', '調', '諂', '諄', '談', '諉', '請', '諍', '諒', '論', '諜', '諦', '諧', '諫', '諭', '諮', '諱', '諷', '諸', '諺', '諾', '謀', '謁', '謂', '謄', '謊', '謎', '謗', '謙', '講', '謝', '謠', '謨', '謬', '謹', '譁', '證', '譎', '譏', '識', '譚', '譜', '警', '譬', '譯', '議', '譴', '護', '譽', '讀', '變', '讒', '讓', '讖', '讚', '谷', '谿', '豁', '豆', '豈', '豉', '豌', '豎', '豐', '豔', '豕', '豚', '象', '豢', '豪', '豫', '豬', '豹', '豺', '貂', '貉', '貊', '貌', '貍', '貓', '貝', '貞', '負', '財', '貢', '貧', '貨', '販', '貪', '貫', '責', '貯', '貲', '貳', '貴', '貶', '買', '貸', '費', '貼', '貽', '貿', '賀', '賁', '賂', '賃', '賄', '賅', '資', '賈', '賊', '賑', '賒', '賓', '賜', '賞', '賠', '賢', '賣', '賤', '賦', '質', '賬', '賭', '賴', '賺', '購', '賽', '贅', '贈', '贊', '贍', '贏', '贓', '贖', '贗', '贛', '赤', '赦', '赧', '赫', '赭', '走', '赳', '赴', '起', '趁', '超', '越', '趕', '趙', '趟', '趣', '趨', '足', '趴', '趾', '跆', '跋', '跌', '跎', '跑', '跚', '跛', '距', '跟', '跡', '跨', '跪', '路', '跳', '跺', '跼', '踏', '踐', '踝', '踟', '踢', '踩', '踫', '踱', '踴', '踵', '踹', '蹂', '蹄', '蹈', '蹉', '蹊', '蹋', '蹙', '蹣', '蹤', '蹦', '蹬', '蹲', '蹶', '蹺', '蹼', '躁', '躂', '躅', '躇', '躉', '躊', '躍', '躑', '躡', '躪', '身', '躬', '躲', '躺', '軀', '車', '軋', '軌', '軍', '軒', '軔', '軛', '軟', '軸', '軻', '軼', '軾', '較', '載', '輊', '輒', '輓', '輔', '輕', '輛', '輜', '輝', '輟', '輦', '輩', '輪', '輯', '輸', '輻', '輾', '輿', '轂', '轄', '轅', '轉', '轍', '轎', '轔', '轟', '轡', '辛', '辜', '辟', '辣', '辦', '辨', '辭', '辮', '辯', '辰', '辱', '農', '迂', '迄', '迅', '迆', '迎', '近', '返', '迢', '迥', '迦', '迪', '迫', '迭', '述', '迴', '迷', '迺', '追', '退', '送', '逃', '逅', '逆', '逍', '透', '逐', '途', '逕', '逖', '逗', '這', '通', '逛', '逝', '逞', '速', '造', '逢', '連', '逮', '週', '進', '逵', '逸', '逼', '逾', '遁', '遂', '遇', '遊', '運', '遍', '過', '遏', '遐', '遑', '道', '達', '違', '遘', '遙', '遜', '遞', '遠', '遣', '遨', '適', '遭', '遮', '遲', '遴', '遵', '遷', '選', '遺', '遼', '遽', '避', '邀', '邁', '邂', '還', '邇', '邊', '邏', '邐', '邑', '邕', '邢', '那', '邦', '邪', '邱', '邵', '邸', '郁', '郊', '郎', '郡', '部', '郭', '郵', '都', '鄂', '鄉', '鄒', '鄙', '鄧', '鄭', '鄰', '鄱', '鄹', '酉', '酊', '酋', '酌', '配', '酒', '酗', '酣', '酥', '酩', '酪', '酬', '酵', '酷', '酸', '醃', '醇', '醉', '醋', '醒', '醜', '醞', '醣', '醫', '醬', '醺', '釀', '釁', '采', '釉', '釋', '里', '重', '野', '量', '釐', '金', '釗', '釘', '釜', '針', '釣', '釦', '釧', '釵', '鈉', '鈍', '鈐', '鈔', '鈕', '鈞', '鈣', '鈴', '鈷', '鈸', '鈽', '鈾', '鉀', '鉋', '鉑', '鉗', '鉛', '鉤', '鉸', '鉻', '銀', '銅', '銓', '銖', '銘', '銜', '銬', '銳', '銷', '銻', '銼', '鋁', '鋅', '鋒', '鋤', '鋪', '鋸', '鋼', '錄', '錐', '錘', '錚', '錠', '錢', '錦', '錨', '錫', '錯', '錳', '錶', '鍊', '鍋', '鍍', '鍛', '鍥', '鍬', '鍰', '鍵', '鍾', '鎂', '鎊', '鎔', '鎖', '鎢', '鎮', '鎳', '鏃', '鏈', '鏍', '鏑', '鏖', '鏗', '鏘', '鏜', '鏝', '鏟', '鏡', '鏢', '鏤', '鏽', '鐃', '鐘', '鐮', '鐲', '鐳', '鐵', '鐸', '鐺', '鑄', '鑑', '鑒', '鑠', '鑣', '鑰', '鑲', '鑼', '鑽', '鑾', '鑿', '長', '門', '閂', '閃', '閉', '開', '閏', '閑', '閒', '間', '閔', '閘', '閡', '閣', '閤', '閥', '閨', '閩', '閭', '閱', '閻', '闆', '闈', '闊', '闋', '闌', '闐', '闔', '闖', '關', '闡', '闢', '阜', '阡', '阪', '阮', '阱', '防', '阻', '阿', '陀', '附', '陋', '陌', '降', '限', '陛', '陝', '陡', '院', '陣', '除', '陪', '陰', '陲', '陳', '陴', '陵', '陶', '陷', '陸', '陽', '隅', '隆', '隊', '隋', '隍', '階', '隔', '隕', '隘', '隙', '際', '障', '隧', '隨', '險', '隱', '隴', '隸', '隻', '雀', '雁', '雄', '雅', '集', '雇', '雉', '雋', '雌', '雍', '雕', '雖', '雙', '雛', '雜', '雞', '離', '難', '雨', '雪', '雯', '雲', '零', '雷', '雹', '電', '需', '霄', '霆', '震', '霉', '霍', '霎', '霏', '霑', '霓', '霖', '霜', '霞', '霧', '霪', '露', '霸', '霹', '霽', '霾', '靂', '靄', '靈', '青', '靖', '靛', '靜', '非', '靠', '靡', '面', '靦', '靨', '革', '靴', '靶', '靼', '鞅', '鞋', '鞍', '鞏', '鞘', '鞠', '鞣', '鞦', '鞭', '韁', '韃', '韆', '韋', '韌', '韓', '韜', '韭', '音', '韶', '韻', '響', '頁', '頂', '頃', '項', '順', '須', '頊', '頌', '預', '頑', '頒', '頓', '頗', '領', '頡', '頤', '頭', '頰', '頷', '頸', '頹', '頻', '顆', '題', '額', '顎', '顏', '顓', '願', '顛', '類', '顧', '顫', '顯', '顰', '顱', '風', '颯', '颱', '颳', '颶', '颺', '颼', '飄', '飛', '食', '飢', '飧', '飩', '飪', '飭', '飯', '飲', '飴', '飼', '飽', '飾', '餃', '餅', '餉', '養', '餌', '餐', '餒', '餓', '餘', '餛', '餞', '餡', '館', '餵', '餽', '餾', '餿', '饅', '饑', '饒', '饜', '饞', '首', '香', '馥', '馨', '馬', '馭', '馮', '馱', '馳', '馴', '駁', '駐', '駑', '駒', '駕', '駙', '駛', '駝', '駟', '駢', '駭', '駱', '駿', '騁', '騎', '騖', '騙', '騫', '騰', '騷', '騾', '驀', '驃', '驅', '驕', '驗', '驚', '驛', '驟', '驢', '驥', '驪', '骨', '骯', '骰', '骷', '骸', '骼', '髏', '髒', '髓', '體', '高', '髦', '髭', '髮', '髯', '髻', '鬃', '鬆', '鬍', '鬚', '鬢', '鬥', '鬧', '鬨', '鬱', '鬲', '鬼', '魁', '魂', '魄', '魅', '魏', '魔', '魘', '魚', '魯', '魷', '鮑', '鮪', '鮫', '鮮', '鯉', '鯊', '鯧', '鯨', '鯽', '鰍', '鰓', '鰥', '鰭', '鰱', '鰻', '鰾', '鱉', '鱔', '鱖', '鱗', '鱷', '鱸', '鳥', '鳩', '鳳', '鳴', '鳶', '鴆', '鴉', '鴒', '鴕', '鴛', '鴣', '鴦', '鴨', '鴻', '鴿', '鵑', '鵝', '鵠', '鵡', '鵪', '鵬', '鵲', '鶉', '鶯', '鶴', '鷂', '鷓', '鷗', '鷥', '鷹', '鷺', '鸚', '鸞', '鹹', '鹼', '鹽', '鹿', '麂', '麋', '麒', '麓', '麗', '麝', '麟', '麥', '麩', '麴', '麵', '麻', '麼', '麾', '黃', '黍', '黎', '黏', '黑', '黔', '默', '黛', '黜', '黝', '點', '黠', '黨', '黯', '黴', '黷', '鼇', '鼎', '鼓', '鼕', '鼙', '鼠', '鼬', '鼴', '鼻', '鼾', '齊', '齋', '齒', '齜', '齟', '齡', '齣', '齦', '齪', '齬', '齲', '齷', '龍', '龐', '龔']\n",
      "\n",
      "Building model...\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Model built successfully, number of parameters: 10,160,191\n",
      "GPU memory usage: 288.1 MB\n",
      "\n",
      "Starting training...\n",
      "   Batch size: 128\n",
      "   Learning rate: 0.001\n",
      "   Number of epochs: 10\n",
      "   Using device: cuda\n",
      "開始訓練，使用設備: cuda\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/10 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------------------------------------\n",
      "Current Learning Rate: 0.001000\n",
      "  Batch [1/1567] Loss: 8.4923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/10 [06:06<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 447\u001b[39m\n\u001b[32m    444\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 429\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Using device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     model, best_acc = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🎉 Training completed! Best accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# Test a few samples\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 146\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs)\u001b[39m\n\u001b[32m    143\u001b[39m all_train_preds = []\n\u001b[32m    144\u001b[39m all_train_labels = []\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mloaddata.<locals>.DatasetWrapper.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     image, label = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     94\u001b[39m     image = transforms.ToPILImage()(image)\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m \u001b[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m path, target = \u001b[38;5;28mself\u001b[39m.samples[index]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    247\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.transform(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[39m, in \u001b[36mdefault_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torchvision\\datasets\\folder.py:262\u001b[39m, in \u001b[36mpil_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpil_loader\u001b[39m(path: Union[\u001b[38;5;28mstr\u001b[39m, Path]) -> Image.Image:\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    263\u001b[39m         img = Image.open(f)\n\u001b[32m    264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: Setup parameters and check CUDA\n",
    "def check_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        current_device = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(current_device)\n",
    "        print(f\"CUDA 可用，使用 GPU 訓練\")\n",
    "        print(f\"當前 GPU 名稱: {device_name}\")\n",
    "        print(f\"GPU 數量: {device_count}\")\n",
    "        print(f\"當前使用的 GPU: {device_name}\")\n",
    "        print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "        print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"CUDA 不可用，將使用 CPU 訓練\")\n",
    "        return False\n",
    "\n",
    "use_gpu = check_cuda()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 設定本地路徑\n",
    "data_dir = 'Handwritten_Data' # 你的資料路徑\n",
    "batch_size = 128 if use_gpu else 16  # GPU 可以用更大的 batch size\n",
    "lr = 0.001  # 稍微降低學習率以獲得更穩定的訓練\n",
    "momentum = 0.9\n",
    "num_epochs = 10  # 增加訓練輪數\n",
    "input_size = 224\n",
    "net_name = 'efficientnet-b0'\n",
    "\n",
    "# 設定 CUDA 優化參數\n",
    "if use_gpu:\n",
    "    torch.backends.cudnn.benchmark = True  # 加速 convolution 運算\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Step 2: Define data loading functions\n",
    "def loaddata(data_dir, batch_size, shuffle=True, subsample_rate=0.01):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # 原始無轉換資料集\n",
    "    raw_dataset = datasets.ImageFolder(root=data_dir, transform=transforms.ToTensor())\n",
    "\n",
    "    # 實現子樣本率\n",
    "    if subsample_rate < 1:\n",
    "        total_size = len(raw_dataset)\n",
    "        sample_size = int(total_size * subsample_rate)\n",
    "        train_size = int(sample_size * 0.8)\n",
    "        val_size = sample_size - train_size\n",
    "        train_dataset, val_dataset = random_split(raw_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "    else:\n",
    "        train_size = int(0.8 * len(raw_dataset))\n",
    "        val_size = len(raw_dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(raw_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    class DatasetWrapper(torch.utils.data.Dataset):\n",
    "        def __init__(self, subset, transform):\n",
    "            self.subset = subset\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.subset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image, label = self.subset.dataset[self.subset.dinices[idx]]\n",
    "            image = transforms.ToPILImage()(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "    train_dataset_wrapped = DatasetWrapper(train_dataset, data_transforms['train'])\n",
    "    val_dataset_wrapped = DatasetWrapper(val_dataset, data_transforms['val'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset_wrapped, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=use_gpu)\n",
    "    val_loader = DataLoader(val_dataset_wrapped, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=use_gpu)\n",
    "\n",
    "    return train_loader, val_loader, train_size, val_size, len(raw_dataset.classes), raw_dataset.classes\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr_scheduler(optimizer):\n",
    "    \"\"\"建立學習率調度器\"\"\"\n",
    "    return optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "# Training function with metrics\n",
    "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs=10):\n",
    "    since = time.time()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # 記錄用\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    # 新增指標記錄\n",
    "    train_precisions, val_precisions = [], []\n",
    "    train_recalls, val_recalls = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "\n",
    "    print(f\"開始訓練，使用設備: {device}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\", unit=\"epoch\"):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 40)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current Learning Rate: {current_lr:.6f}')\n",
    "\n",
    "        # 訓練階段\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # 預測與標籤收集\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            all_train_preds.append(preds.cpu())\n",
    "            all_train_labels.append(labels.cpu())\n",
    "\n",
    "            if i % 50 == 0 or i == len(train_loader) - 1:\n",
    "                print(f'  Batch [{i+1}/{len(train_loader)}] Loss: {loss.item():.4f}')\n",
    "\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc.cpu().item())\n",
    "\n",
    "        # 統計訓練 Precision/Recall/F1\n",
    "        train_preds = torch.cat(all_train_preds)\n",
    "        train_labels = torch.cat(all_train_labels)\n",
    "        train_precisions.append(precision_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "        train_recalls.append(recall_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "        train_f1s.append(f1_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "\n",
    "        # 驗證階段\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                all_val_preds.append(preds.cpu())\n",
    "                all_val_labels.append(labels.cpu())\n",
    "\n",
    "        epoch_loss = running_loss / val_size\n",
    "        epoch_acc = running_corrects.double() / val_size\n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        val_losses.append(epoch_loss)\n",
    "        val_accs.append(epoch_acc.cpu().item())\n",
    "\n",
    "        # 統計驗證 Precision/Recall/F1\n",
    "        val_preds = torch.cat(all_val_preds)\n",
    "        val_labels = torch.cat(all_val_labels)\n",
    "        val_precisions.append(precision_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        val_recalls.append(recall_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        val_f1s.append(f1_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            \n",
    "            # Create necessary directories\n",
    "            os.makedirs('./best', exist_ok=True)\n",
    "            checkpoint_path = os.path.join(data_dir, 'best_model_checkpoint.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': epoch_loss,\n",
    "                'acc': epoch_acc,\n",
    "            }, checkpoint_path)\n",
    "            print(f'New best model saved, accuracy: {best_acc:.4f}')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training completed, time elapsed: {time_elapsed // 60:.0f}min {time_elapsed % 60:.0f}sec')\n",
    "    print(f'Best validation accuracy: {best_acc:.4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    save_dir = os.path.join(data_dir, 'model')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs('./best', exist_ok=True)\n",
    "    model_path = os.path.join('./best', 'best.pth')\n",
    "    torch.save(model, model_path)\n",
    "    print(f'Final model saved to: {model_path}')\n",
    "\n",
    "    # Plotting (Loss, Accuracy, Precision, Recall, F1)\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title('Loss Change')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, num_epochs+1), train_accs, 'b-', label='Training Accuracy')\n",
    "    plt.plot(range(1, num_epochs+1), val_accs, 'r-', label='Validation Accuracy')\n",
    "    plt.title('Accuracy Change')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, num_epochs+1), train_precisions, 'b--', label='Training Precision')\n",
    "    plt.plot(range(1, num_epochs+1), val_precisions, 'r--', label='Validation Precision')\n",
    "    plt.plot(range(1, num_epochs+1), train_recalls, 'b:', label='Training Recall')\n",
    "    plt.plot(range(1, num_epochs+1), val_recalls, 'r:', label='Validation Recall')\n",
    "    plt.title('Precision / Recall Change')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, num_epochs+1), train_f1s, 'b-', label='Training F1')\n",
    "    plt.plot(range(1, num_epochs+1), val_f1s, 'r-', label='Validation F1')\n",
    "    plt.title('F1-score Change')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    curve_path = os.path.join(save_dir, 'training_curves_all_metrics.png')\n",
    "    plt.savefig(curve_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Training curves (including F1, Precision, Recall) saved to: {curve_path}')\n",
    "\n",
    "    return model, best_acc\n",
    "\n",
    "# 2. Display original and transformed images\n",
    "def show_images(raw_data, transformed_data, label_map=None, num_images=5, figsize_per_image=(1.5, 1.5)):\n",
    "    num_cols = num_images\n",
    "    plt.figure(figsize=(figsize_per_image[0] * num_cols, figsize_per_image[1] * 2))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        orig_img, label = raw_data[i]\n",
    "        trans_img, _ = transformed_data[i]\n",
    "\n",
    "        label_text = label_map[label] if label_map else str(label)\n",
    "\n",
    "        # Original image\n",
    "        plt.subplot(2, num_cols, i + 1)\n",
    "        if isinstance(orig_img, torch.Tensor):\n",
    "            img_np = orig_img.permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            img_np = orig_img\n",
    "        plt.imshow(img_np)\n",
    "        plt.title(f\"Original\\n{label_text}\", fontsize=10)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Transformed image (needs to be denormalized)\n",
    "        if isinstance(trans_img, torch.Tensor):\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "            trans_img = trans_img * std + mean\n",
    "            trans_img = trans_img.clamp(0, 1)\n",
    "            img_np = trans_img.permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            img_np = trans_img\n",
    "\n",
    "        plt.subplot(2, num_cols, num_cols + i + 1)\n",
    "        plt.imshow(img_np)\n",
    "        plt.title(\"Transformed\", fontsize=10)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test_samples(model, val_loader, class_names, device, num_samples=10):\n",
    "    \"\"\"Test a few samples\"\"\"\n",
    "    model.eval()\n",
    "    samples_tested = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Display the first few prediction results\n",
    "            for i in range(min(len(preds), num_samples - samples_tested)):\n",
    "                pred_class = class_names[preds[i]]\n",
    "                true_class = class_names[labels[i]]\n",
    "                is_correct = preds[i] == labels[i]\n",
    "                \n",
    "                status = \"YYYYYY\" if is_correct else \"NNNNNN\"\n",
    "                print(f\"{status} Prediction: {pred_class} | Actual: {true_class}\")\n",
    "                \n",
    "                if is_correct:\n",
    "                    correct_predictions += 1\n",
    "                samples_tested += 1\n",
    "                \n",
    "                if samples_tested >= num_samples:\n",
    "                    break\n",
    "            \n",
    "            if samples_tested >= num_samples:\n",
    "                break\n",
    "    \n",
    "    accuracy = correct_predictions / samples_tested\n",
    "    print(f\"\\nTest sample accuracy: {accuracy:.4f} ({correct_predictions}/{samples_tested})\")\n",
    "\n",
    "# Step 5: Main function\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"Starting handwritten character recognition training\")\n",
    "    print(\"=\" * 60)\n",
    "    # subsample_rate = 0.001\n",
    "    subsample_rate = 1\n",
    "    \n",
    "    # Check data path\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Folder does not exist: {data_dir}\")\n",
    "        print(\"Please ensure the data folder path is correct\")\n",
    "        return\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        # Correctly receive 6 return values\n",
    "        train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(data_dir, batch_size, subsample_rate=subsample_rate)\n",
    "        # train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(data_dir, batch_size)\n",
    "        print(f\"Data loaded successfully\")\n",
    "        print(f\"   Training samples: {train_size}\")\n",
    "        print(f\"   Validation samples: {val_size}\")\n",
    "        print(f\"   Character classes: {class_num}\")\n",
    "        print(f\"   Class names: {class_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Data loading failed: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nBuilding model...\")\n",
    "    try:\n",
    "        # Load pre-trained EfficientNet\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        \n",
    "        # Modify the last layer to adapt to our classification task\n",
    "        num_ftrs = model._fc.in_features\n",
    "        model._fc = nn.Linear(num_ftrs, class_num)\n",
    "        \n",
    "        # Move model to GPU\n",
    "        model = model.to(device)\n",
    "        print(f\"Model built successfully, number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        # Display GPU memory usage\n",
    "        if use_gpu:\n",
    "            print(f\"GPU memory usage: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Model building failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)  # Use AdamW\n",
    "    scheduler = get_lr_scheduler(optimizer)\n",
    "\n",
    "    print(f\"\\nStarting training...\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Learning rate: {lr}\")\n",
    "    print(f\"   Number of epochs: {num_epochs}\")\n",
    "    print(f\"   Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        model, best_acc = train_model(\n",
    "            model, criterion, optimizer, scheduler,\n",
    "            train_loader, val_loader,\n",
    "            train_size, val_size,\n",
    "            num_epochs=num_epochs\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🎉 Training completed! Best accuracy: {best_acc:.4f}\")\n",
    "        \n",
    "        # Test a few samples\n",
    "        print(\"\\nTesting a few samples...\")\n",
    "        test_samples(model, val_loader, class_names, device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during training: {e}\")\n",
    "        return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14c093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from optuna) (25.0)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.41-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.2.3-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\yahui\\appdata\\local\\pypoetry\\cache\\virtualenvs\\loan-risk-predictor-hbt6gynv-py3.12\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "Downloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading greenlet-3.2.3-cp312-cp312-win_amd64.whl (297 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "Successfully installed Mako-1.3.10 alembic-1.16.1 colorlog-6.9.0 greenlet-3.2.3 optuna-4.3.0 sqlalchemy-2.0.41\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e2aded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 可用，使用 GPU 訓練\n",
      "當前 GPU 名稱: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU 數量: 1\n",
      "當前使用的 GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA 版本: 11.8\n",
      "PyTorch 版本: 2.7.0+cu118\n",
      "使用設備: cuda\n",
      "開始手寫字符辨識訓練 - 使用 Optuna 優化\n",
      "============================================================\n",
      "載入資料...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-10 19:22:50,165] A new study created in memory with name: handwriting_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料載入成功\n",
      "   訓練樣本: 200569\n",
      "   驗證樣本: 50143\n",
      "   字符類別: 4803\n",
      "   類別名稱: ['丁', '七', '丈', '三', '上', '下', '不', '丐', '丑', '且', '丕', '世', '丘', '丙', '丞', '丟', '並', '丫', '中', '串', '丸', '丹', '主', '乃', '久', '么', '之', '乍', '乎', '乏', '乒', '乓', '乖', '乘', '乙', '九', '乞', '也', '乩', '乳', '乾', '亂', '了', '予', '事', '二', '于', '云', '互', '五', '井', '亙', '些', '亞', '亟', '亡', '交', '亥', '亦', '亨', '享', '京', '亭', '亮', '人', '什', '仁', '仃', '仄', '仆', '仇', '今', '介', '仍', '仔', '仕', '他', '仗', '付', '仙', '仞', '仟', '代', '令', '以', '仰', '仲', '仳', '件', '任', '份', '仿', '企', '伉', '伊', '伍', '伏', '伐', '休', '伕', '伙', '伯', '估', '伴', '伶', '伸', '伺', '似', '伽', '佃', '但', '佇', '位', '低', '住', '佐', '佑', '佔', '何', '佗', '余', '佛', '作', '佝', '佞', '你', '佣', '佩', '佬', '佯', '佰', '佳', '併', '佻', '佾', '使', '侃', '來', '侈', '例', '侍', '侏', '侖', '供', '依', '侮', '侯', '侵', '侶', '便', '係', '促', '俄', '俊', '俎', '俏', '俐', '俑', '俗', '俘', '俚', '保', '俞', '俟', '俠', '信', '修', '俯', '俱', '俳', '俸', '俺', '俾', '倀', '倆', '倉', '個', '倌', '倍', '倏', '們', '倒', '倔', '倖', '倘', '候', '倚', '借', '倡', '倣', '倥', '倦', '倨', '倩', '倪', '倫', '倭', '值', '偃', '假', '偉', '偌', '偎', '偏', '偕', '做', '停', '健', '側', '偵', '偶', '偷', '偺', '偽', '傀', '傅', '傍', '傑', '傖', '傘', '備', '傢', '催', '傭', '傯', '傲', '傳', '債', '傷', '傻', '傾', '僅', '像', '僑', '僕', '僖', '僚', '僥', '僧', '僭', '僮', '僱', '僵', '價', '僻', '儀', '儂', '億', '儈', '儉', '儐', '儒', '儔', '儘', '償', '儡', '優', '儲', '儷', '儼', '兀', '允', '元', '兄', '充', '兆', '兇', '先', '光', '克', '兌', '免', '兒', '兔', '兕', '兗', '兜', '兢', '入', '內', '全', '兩', '八', '公', '六', '兮', '共', '兵', '其', '具', '典', '兼', '冀', '冉', '冊', '再', '冑', '冒', '冕', '冗', '冠', '冢', '冤', '冥', '冬', '冰', '冶', '冷', '冽', '准', '凋', '凌', '凍', '凜', '凝', '几', '凡', '凰', '凱', '凳', '凶', '凸', '凹', '出', '函', '刀', '刁', '刃', '分', '切', '刈', '刊', '刎', '刑', '划', '列', '初', '判', '別', '刨', '利', '刪', '刮', '到', '制', '刷', '券', '刻', '剁', '剃', '則', '削', '剋', '剌', '前', '剎', '剔', '剖', '剛', '剜', '剝', '剩', '剪', '副', '割', '剴', '創', '剷', '剽', '剿', '劃', '劇', '劈', '劉', '劍', '劑', '力', '功', '加', '劣', '助', '努', '劫', '劬', '劾', '勁', '勃', '勇', '勉', '勒', '動', '勗', '勘', '務', '勛', '勝', '勞', '募', '勢', '勤', '勦', '勵', '勸', '勻', '勾', '勿', '包', '匆', '匈', '匍', '匏', '匐', '匕', '化', '北', '匙', '匝', '匠', '匡', '匣', '匪', '匯', '匱', '匹', '匾', '匿', '區', '十', '千', '卅', '升', '午', '卉', '半', '卑', '卒', '卓', '協', '南', '博', '卜', '卞', '占', '卡', '卦', '卮', '卯', '印', '危', '即', '卵', '卷', '卸', '卹', '卻', '卿', '厄', '厚', '厝', '原', '厥', '厭', '厲', '去', '參', '又', '叉', '及', '友', '反', '叔', '取', '受', '叛', '叟', '叢', '口', '古', '句', '另', '叨', '叩', '只', '叫', '召', '叭', '叮', '可', '台', '叱', '史', '右', '叵', '司', '叼', '吁', '吃', '各', '吆', '合', '吉', '吊', '吋', '同', '名', '后', '吏', '吐', '向', '吒', '君', '吝', '吞', '吟', '吠', '否', '吧', '吩', '含', '吭', '吮', '吱', '吳', '吵', '吶', '吸', '吹', '吻', '吼', '吾', '呀', '呂', '呃', '呆', '呈', '告', '呎', '呢', '周', '呱', '味', '呵', '呶', '呷', '呸', '呻', '呼', '命', '咀', '咄', '咆', '咋', '和', '咎', '咐', '咒', '咕', '咖', '咚', '咦', '咨', '咪', '咫', '咬', '咯', '咱', '咳', '咸', '咻', '咽', '哀', '品', '哂', '哄', '哇', '哈', '哉', '哎', '員', '哥', '哦', '哨', '哩', '哪', '哭', '哮', '哲', '哺', '哼', '唁', '唆', '唉', '唐', '唔', '唧', '唬', '售', '唯', '唱', '唳', '唷', '唸', '唾', '啃', '啄', '商', '啊', '問', '啕', '啖', '啜', '啞', '啟', '啡', '啣', '啤', '啦', '啪', '啻', '啼', '啾', '喀', '喂', '喃', '善', '喇', '喉', '喊', '喋', '喔', '喘', '喚', '喜', '喝', '喟', '喧', '喪', '喬', '單', '喱', '喲', '喳', '喻', '嗅', '嗆', '嗇', '嗎', '嗑', '嗓', '嗚', '嗜', '嗟', '嗡', '嗣', '嗤', '嗥', '嗦', '嗨', '嗯', '嗷', '嗽', '嗾', '嘀', '嘆', '嘈', '嘉', '嘍', '嘎', '嘔', '嘖', '嘗', '嘛', '嘟', '嘩', '嘮', '嘯', '嘰', '嘲', '嘴', '嘶', '嘹', '嘻', '嘿', '噎', '噓', '噗', '噙', '噢', '噤', '噥', '器', '噩', '噪', '噫', '噬', '噯', '噱', '噴', '噸', '噹', '嚀', '嚅', '嚇', '嚎', '嚏', '嚐', '嚕', '嚥', '嚨', '嚮', '嚴', '嚶', '嚷', '嚼', '囀', '囁', '囂', '囈', '囉', '囊', '囌', '囑', '囚', '四', '回', '因', '囤', '囪', '困', '固', '圃', '圈', '國', '圍', '園', '圓', '圖', '團', '土', '在', '圬', '圭', '圯', '地', '圳', '圾', '址', '均', '坊', '坍', '坎', '坏', '坐', '坑', '坡', '坤', '坦', '坩', '坪', '坷', '坼', '垂', '垃', '型', '垠', '垢', '垣', '垮', '埂', '埃', '埋', '城', '埔', '域', '埠', '埤', '執', '培', '基', '堂', '堅', '堆', '堊', '堡', '堤', '堪', '堯', '堰', '報', '場', '堵', '塊', '塌', '塑', '塔', '塗', '塘', '塚', '塞', '塢', '填', '塭', '塵', '塹', '塾', '墀', '境', '墅', '墊', '墓', '墜', '增', '墟', '墨', '墮', '墳', '墾', '壁', '壅', '壇', '壑', '壓', '壕', '壘', '壙', '壞', '壟', '壢', '壤', '壩', '士', '壬', '壯', '壹', '壺', '壽', '夏', '夔', '夕', '外', '夙', '多', '夜', '夠', '夢', '夤', '夥', '大', '天', '太', '夫', '夭', '央', '失', '夷', '夸', '夾', '奄', '奇', '奈', '奉', '奎', '奏', '奐', '契', '奔', '奕', '套', '奘', '奚', '奠', '奢', '奧', '奩', '奪', '奮', '女', '奴', '奶', '奸', '她', '好', '妁', '如', '妃', '妄', '妊', '妍', '妒', '妓', '妖', '妙', '妝', '妞', '妣', '妤', '妥', '妨', '妮', '妯', '妳', '妹', '妻', '妾', '姆', '姊', '始', '姍', '姐', '姑', '姒', '姓', '委', '姘', '姚', '姜', '姣', '姥', '姦', '姨', '姪', '姬', '姻', '姿', '威', '娃', '娌', '娑', '娓', '娘', '娛', '娜', '娟', '娠', '娣', '娥', '娩', '娶', '娼', '婀', '婁', '婆', '婉', '婊', '婚', '婢', '婦', '婪', '婷', '婿', '媒', '媚', '媛', '媲', '媳', '媼', '媽', '媾', '嫁', '嫂', '嫉', '嫌', '嫖', '嫗', '嫘', '嫡', '嫣', '嫦', '嫩', '嫵', '嫻', '嬉', '嬋', '嬌', '嬝', '嬤', '嬪', '嬰', '嬴', '嬸', '孀', '子', '孑', '孓', '孔', '孕', '字', '存', '孚', '孜', '孝', '孟', '季', '孤', '孩', '孫', '孰', '孱', '孳', '孵', '學', '孺', '孽', '孿', '它', '宅', '宇', '守', '安', '宋', '完', '宏', '宗', '官', '宙', '定', '宛', '宜', '客', '宣', '室', '宥', '宦', '宮', '宰', '害', '宴', '宵', '家', '宸', '容', '宿', '寂', '寄', '寅', '密', '寇', '富', '寐', '寒', '寓', '寞', '察', '寡', '寢', '寤', '寥', '實', '寧', '寨', '審', '寫', '寬', '寮', '寵', '寶', '寸', '寺', '封', '射', '將', '專', '尉', '尊', '尋', '對', '導', '小', '少', '尖', '尚', '尤', '尬', '就', '尷', '尸', '尹', '尺', '尼', '尾', '尿', '局', '屁', '居', '屆', '屈', '屋', '屍', '屎', '屏', '屐', '屑', '展', '屜', '屠', '屢', '層', '履', '屬', '屯', '山', '屹', '岌', '岐', '岑', '岔', '岡', '岩', '岫', '岱', '岳', '岷', '岸', '峙', '峨', '峪', '峭', '峰', '島', '峻', '峽', '崁', '崆', '崇', '崎', '崑', '崔', '崖', '崙', '崛', '崢', '崩', '嵌', '嵐', '嵩', '嶄', '嶇', '嶝', '嶺', '嶼', '嶽', '巍', '巒', '巔', '巖', '川', '州', '巡', '巢', '工', '左', '巧', '巨', '巫', '差', '己', '已', '巳', '巴', '巷', '巽', '巾', '市', '布', '帆', '希', '帑', '帕', '帖', '帘', '帚', '帛', '帝', '帥', '師', '席', '帳', '帶', '帷', '常', '帽', '幀', '幅', '幌', '幔', '幗', '幛', '幟', '幢', '幣', '幫', '干', '平', '年', '并', '幸', '幹', '幻', '幼', '幽', '幾', '庇', '床', '序', '底', '庖', '店', '庚', '府', '庠', '度', '座', '庫', '庭', '庵', '庶', '康', '庸', '庾', '廁', '廂', '廈', '廉', '廊', '廓', '廖', '廚', '廝', '廟', '廠', '廢', '廣', '廬', '廳', '延', '廷', '建', '廿', '弁', '弄', '弈', '弊', '式', '弒', '弓', '弔', '引', '弗', '弘', '弛', '弟', '弦', '弧', '弩', '弭', '弱', '張', '強', '弼', '彆', '彈', '彌', '彎', '彗', '彙', '形', '彤', '彥', '彩', '彪', '彫', '彬', '彭', '彰', '影', '彷', '役', '彼', '彿', '往', '征', '待', '徇', '很', '徊', '律', '後', '徐', '徑', '徒', '得', '徘', '徙', '從', '御', '徨', '復', '循', '徬', '微', '徵', '德', '徹', '徽', '心', '必', '忌', '忍', '忖', '志', '忘', '忙', '忝', '忠', '快', '忱', '念', '忽', '忿', '怎', '怏', '怒', '怔', '怕', '怖', '思', '怠', '怡', '急', '性', '怨', '怪', '怯', '怵', '恃', '恆', '恍', '恐', '恕', '恙', '恢', '恣', '恤', '恥', '恨', '恩', '恪', '恫', '恬', '恭', '息', '恰', '恿', '悄', '悅', '悉', '悌', '悍', '悔', '悖', '悚', '悟', '悠', '患', '您', '悲', '悴', '悵', '悶', '悸', '悻', '悼', '悽', '情', '惆', '惋', '惑', '惕', '惘', '惚', '惜', '惟', '惠', '惡', '惦', '惰', '惱', '想', '惴', '惶', '惹', '惺', '惻', '愀', '愁', '愈', '愉', '愎', '意', '愕', '愚', '愛', '愜', '感', '愣', '愧', '愴', '愾', '愿', '慄', '慇', '慈', '態', '慌', '慍', '慎', '慕', '慘', '慚', '慝', '慟', '慢', '慣', '慧', '慨', '慫', '慮', '慰', '慶', '慷', '慼', '慾', '憂', '憊', '憎', '憐', '憑', '憔', '憚', '憤', '憧', '憩', '憫', '憬', '憲', '憶', '憾', '懂', '懇', '懈', '應', '懊', '懍', '懣', '懦', '懲', '懵', '懶', '懷', '懸', '懺', '懼', '懾', '懿', '戀', '戈', '戊', '戌', '戍', '戎', '成', '我', '戒', '戕', '或', '戚', '戛', '戟', '戡', '戢', '截', '戮', '戰', '戲', '戳', '戴', '戶', '戾', '房', '所', '扁', '扇', '扈', '扉', '手', '才', '扎', '扒', '打', '扔', '托', '扛', '扣', '扭', '扮', '扯', '扳', '扶', '批', '扼', '找', '承', '技', '抄', '抉', '把', '抑', '抒', '抓', '投', '抖', '抗', '折', '抨', '披', '抬', '抱', '抵', '抹', '押', '抽', '抿', '拂', '拄', '拆', '拇', '拈', '拉', '拋', '拌', '拍', '拎', '拐', '拒', '拓', '拔', '拖', '拗', '拘', '拙', '拚', '招', '拜', '括', '拭', '拮', '拯', '拱', '拳', '拴', '拷', '拼', '拽', '拾', '拿', '持', '指', '挈', '按', '挑', '挖', '挨', '挪', '挫', '振', '挺', '挽', '挾', '捂', '捆', '捉', '捎', '捏', '捐', '捕', '捧', '捨', '捩', '捫', '捱', '捲', '捶', '捷', '捻', '掀', '掃', '掄', '授', '掉', '掌', '掏', '排', '掖', '掘', '掙', '掛', '掠', '採', '探', '掣', '接', '控', '推', '掩', '措', '掬', '揀', '揆', '揉', '揍', '描', '提', '插', '揖', '揚', '換', '握', '揣', '揩', '揪', '揭', '揮', '援', '損', '搏', '搓', '搔', '搖', '搗', '搜', '搞', '搪', '搬', '搭', '搶', '搽', '搾', '摑', '摒', '摔', '摘', '摟', '摧', '摩', '摯', '摸', '摹', '摺', '撇', '撈', '撐', '撒', '撓', '撕', '撚', '撞', '撤', '撥', '撩', '撫', '撬', '播', '撮', '撰', '撲', '撻', '撼', '撿', '擁', '擂', '擄', '擅', '擇', '擊', '擋', '操', '擎', '擒', '擔', '擘', '據', '擠', '擦', '擬', '擰', '擱', '擲', '擴', '擺', '擻', '擾', '攀', '攆', '攏', '攔', '攘', '攙', '攜', '攝', '攣', '攤', '攪', '攫', '攬', '支', '收', '改', '攻', '放', '政', '故', '效', '敏', '救', '敖', '敗', '敘', '教', '敝', '敞', '敢', '散', '敦', '敬', '敲', '整', '敵', '敷', '數', '斂', '斃', '文', '斐', '斑', '斗', '料', '斜', '斟', '斡', '斤', '斥', '斧', '斫', '斬', '斯', '新', '斷', '方', '於', '施', '旁', '旅', '旋', '旌', '旎', '族', '旖', '旗', '既', '日', '旦', '旨', '早', '旬', '旭', '旱', '旺', '昀', '昂', '昆', '昌', '明', '昏', '易', '昔', '星', '映', '春', '昧', '昨', '昭', '是', '時', '晃', '晉', '晌', '晏', '晒', '晚', '晝', '晤', '晦', '晨', '普', '景', '晰', '晴', '晶', '智', '暇', '暈', '暉', '暑', '暖', '暗', '暢', '暨', '暫', '暮', '暴', '暹', '曆', '曉', '曖', '曙', '曝', '曠', '曦', '曰', '曲', '曳', '更', '曷', '書', '曹', '曼', '曾', '替', '最', '會', '月', '有', '朋', '服', '朔', '朕', '朗', '望', '朝', '期', '朦', '朧', '木', '未', '末', '本', '札', '朮', '朱', '朴', '朵', '朽', '杉', '李', '杏', '材', '村', '杖', '杜', '杞', '束', '杭', '杯', '杰', '東', '杳', '杵', '杷', '松', '板', '枇', '枉', '枋', '析', '枕', '林', '枚', '果', '枝', '枯', '枴', '架', '枸', '柄', '柏', '某', '柑', '染', '柔', '柚', '柞', '查', '柩', '柬', '柯', '柱', '柳', '柴', '柵', '柿', '栓', '栗', '校', '栩', '株', '核', '根', '格', '栽', '桀', '桂', '桃', '桅', '框', '案', '桌', '桐', '桑', '桓', '桔', '桶', '桿', '梁', '梃', '梅', '梆', '梓', '梔', '梗', '條', '梟', '梢', '梧', '梨', '梭', '梯', '械', '梱', '梳', '梵', '棄', '棉', '棋', '棍', '棒', '棕', '棗', '棘', '棚', '棟', '棠', '棣', '棧', '森', '棲', '棵', '棹', '棺', '椅', '植', '椎', '椒', '椰', '楊', '楓', '楔', '楚', '楞', '楠', '楨', '楫', '業', '極', '楷', '楹', '概', '榆', '榔', '榕', '榛', '榜', '榨', '榫', '榭', '榮', '榴', '榷', '榻', '槁', '構', '槌', '槍', '槐', '槓', '槨', '槳', '槽', '樁', '樂', '樅', '樊', '樓', '標', '樞', '樟', '模', '樣', '樵', '樸', '樹', '樺', '樽', '橄', '橇', '橋', '橘', '橙', '機', '橡', '橢', '橫', '檀', '檄', '檔', '檜', '檢', '檬', '檳', '檸', '檻', '櫂', '櫃', '櫓', '櫚', '櫛', '櫝', '櫥', '櫻', '欄', '權', '欖', '欠', '次', '欣', '欲', '欺', '欽', '款', '歇', '歉', '歌', '歐', '歙', '歟', '歡', '止', '正', '此', '步', '武', '歧', '歪', '歲', '歷', '歸', '歹', '死', '歿', '殃', '殆', '殉', '殊', '殖', '殘', '殤', '殮', '殯', '殲', '段', '殷', '殺', '殼', '殿', '毀', '毅', '毆', '毋', '母', '每', '毒', '毓', '比', '毗', '毛', '毫', '毯', '毽', '氏', '氐', '民', '氓', '氖', '氛', '氟', '氣', '氤', '氦', '氧', '氨', '氫', '氮', '氯', '氳', '水', '永', '氾', '汀', '汁', '求', '汐', '汕', '汗', '汙', '汝', '汞', '江', '池', '汨', '汪', '汰', '汲', '決', '汽', '汾', '沁', '沃', '沅', '沈', '沉', '沌', '沐', '沒', '沖', '沙', '沛', '沫', '沮', '沱', '河', '沸', '油', '治', '沼', '沽', '沾', '沿', '況', '泄', '泅', '泉', '泊', '泌', '泓', '法', '泗', '泛', '泡', '波', '泣', '泥', '注', '泰', '泱', '泳', '洋', '洌', '洗', '洛', '洞', '津', '洪', '洱', '洲', '洶', '活', '洽', '派', '流', '浙', '浚', '浦', '浩', '浪', '浬', '浮', '浴', '海', '浸', '涇', '消', '涉', '涎', '涓', '涕', '涮', '涯', '液', '涵', '涸', '涼', '淄', '淅', '淆', '淇', '淋', '淌', '淑', '淒', '淘', '淙', '淚', '淞', '淡', '淤', '淨', '淪', '淫', '淮', '深', '淳', '淵', '混', '淹', '淺', '添', '清', '渙', '渚', '減', '渝', '渠', '渡', '渣', '渤', '渥', '渦', '測', '渭', '港', '渲', '渴', '游', '渺', '渾', '湃', '湊', '湍', '湔', '湖', '湘', '湛', '湧', '湮', '湯', '溉', '源', '準', '溘', '溜', '溝', '溢', '溥', '溪', '溫', '溯', '溶', '溺', '溼', '滂', '滄', '滅', '滇', '滋', '滌', '滑', '滓', '滔', '滬', '滯', '滲', '滴', '滾', '滿', '漁', '漂', '漆', '漏', '漓', '演', '漕', '漠', '漢', '漣', '漩', '漪', '漫', '漬', '漯', '漱', '漲', '漳', '漸', '漾', '漿', '潑', '潔', '潘', '潛', '潤', '潦', '潭', '潮', '潰', '潸', '潺', '潼', '澀', '澄', '澆', '澈', '澎', '澗', '澡', '澤', '澧', '澱', '澳', '澹', '激', '濁', '濂', '濃', '濘', '濛', '濟', '濠', '濡', '濤', '濫', '濬', '濯', '濱', '濺', '濾', '瀆', '瀉', '瀋', '瀏', '瀑', '瀕', '瀚', '瀛', '瀝', '瀟', '瀨', '瀰', '瀾', '灌', '灑', '灘', '灣', '灤', '火', '灰', '灶', '灸', '灼', '災', '炊', '炎', '炒', '炕', '炙', '炫', '炬', '炭', '炮', '炯', '炳', '炸', '為', '烈', '烊', '烏', '烘', '烙', '烤', '烹', '烽', '焉', '焊', '焙', '焚', '無', '焦', '焰', '然', '煉', '煌', '煎', '煙', '煜', '煞', '煤', '煥', '煦', '照', '煩', '煬', '煮', '煽', '熄', '熊', '熔', '熙', '熟', '熨', '熬', '熱', '熹', '熾', '燃', '燄', '燈', '燉', '燎', '燐', '燒', '燕', '燙', '燜', '營', '燥', '燦', '燧', '燬', '燭', '燮', '燴', '燻', '爆', '爍', '爐', '爛', '爨', '爪', '爬', '爭', '爰', '爵', '父', '爸', '爹', '爺', '爻', '爽', '爾', '牆', '片', '版', '牌', '牒', '牖', '牘', '牙', '牛', '牝', '牟', '牠', '牡', '牢', '牧', '物', '牯', '牲', '牴', '特', '牽', '犀', '犁', '犄', '犒', '犖', '犛', '犢', '犧', '犬', '犯', '狀', '狂', '狄', '狎', '狐', '狗', '狙', '狠', '狡', '狩', '狷', '狸', '狹', '狼', '狽', '猓', '猖', '猙', '猛', '猜', '猥', '猩', '猴', '猶', '猷', '猾', '猿', '獄', '獅', '獎', '獐', '獗', '獨', '獰', '獲', '獵', '獷', '獸', '獺', '獻', '玀', '玄', '率', '玉', '王', '玖', '玟', '玨', '玩', '玫', '玲', '玳', '玷', '玻', '珀', '珊', '珍', '珠', '班', '珮', '現', '球', '琅', '理', '琉', '琊', '琍', '琢', '琥', '琪', '琳', '琴', '琵', '琶', '琺', '琿', '瑁', '瑕', '瑙', '瑚', '瑛', '瑜', '瑞', '瑟', '瑣', '瑤', '瑩', '瑪', '瑯', '瑰', '璃', '璋', '璜', '璣', '璦', '璧', '璩', '環', '璽', '瓊', '瓏', '瓜', '瓠', '瓢', '瓣', '瓦', '瓶', '瓷', '甄', '甌', '甕', '甘', '甚', '甜', '生', '產', '甥', '甦', '用', '甩', '甫', '甬', '甭', '田', '由', '甲', '申', '男', '甸', '甽', '界', '畏', '畔', '留', '畚', '畜', '畝', '畢', '略', '畦', '番', '畫', '異', '當', '畸', '疆', '疇', '疊', '疋', '疏', '疑', '疙', '疚', '疝', '疤', '疥', '疫', '疲', '疳', '疵', '疹', '疼', '疽', '疾', '病', '症', '痊', '痔', '痕', '痘', '痙', '痛', '痞', '痢', '痣', '痰', '痱', '痲', '痴', '痺', '痿', '瘀', '瘁', '瘉', '瘋', '瘍', '瘓', '瘟', '瘠', '瘡', '瘤', '瘦', '瘧', '瘩', '瘴', '瘸', '療', '癆', '癌', '癒', '癖', '癘', '癢', '癥', '癩', '癬', '癮', '癱', '癲', '癸', '登', '發', '白', '百', '皂', '的', '皆', '皇', '皈', '皎', '皓', '皖', '皚', '皮', '皰', '皴', '皺', '皿', '盂', '盃', '盆', '盈', '益', '盍', '盎', '盒', '盔', '盛', '盜', '盞', '盟', '盡', '監', '盤', '盥', '盧', '盪', '目', '盯', '盲', '直', '相', '盹', '盼', '盾', '省', '眉', '看', '真', '眠', '眨', '眩', '眶', '眷', '眸', '眺', '眼', '眾', '睏', '睛', '睜', '睞', '睡', '督', '睥', '睦', '睨', '睪', '睫', '睬', '睹', '睽', '睿', '瞄', '瞇', '瞌', '瞎', '瞑', '瞞', '瞟', '瞠', '瞥', '瞧', '瞪', '瞬', '瞭', '瞰', '瞳', '瞻', '瞽', '瞿', '矇', '矓', '矗', '矚', '矛', '矜', '矢', '矣', '知', '矩', '短', '矮', '矯', '石', '矽', '砂', '砌', '砍', '研', '砝', '砥', '砧', '砭', '砰', '破', '砷', '砸', '硃', '硝', '硫', '硬', '硯', '硼', '碉', '碌', '碎', '碑', '碗', '碘', '碟', '碧', '碩', '碰', '碳', '確', '碼', '碾', '磁', '磅', '磊', '磋', '磐', '磕', '磚', '磨', '磬', '磯', '磴', '磷', '磺', '礁', '礎', '礙', '礦', '礪', '礫', '礬', '示', '社', '祀', '祁', '祆', '祇', '祈', '祉', '祐', '祕', '祖', '祗', '祚', '祝', '神', '祟', '祠', '祥', '票', '祭', '祺', '祿', '禁', '禍', '禎', '福', '禦', '禧', '禪', '禮', '禱', '禹', '禽', '禾', '禿', '秀', '私', '秉', '秋', '科', '秒', '租', '秣', '秤', '秦', '秧', '秩', '移', '稀', '稅', '稈', '程', '稍', '稔', '稚', '稜', '稟', '稠', '種', '稱', '稷', '稻', '稼', '稽', '稿', '穀', '穆', '穌', '積', '穎', '穗', '穡', '穢', '穩', '穫', '穴', '究', '穹', '空', '穿', '突', '窄', '窈', '窒', '窕', '窖', '窗', '窘', '窟', '窠', '窩', '窪', '窮', '窯', '窺', '竄', '竅', '竇', '竊', '立', '站', '竟', '章', '竣', '童', '竭', '端', '競', '竹', '竺', '竽', '竿', '笆', '笑', '笙', '笛', '笞', '笠', '符', '笨', '第', '筆', '等', '筋', '筍', '筏', '筐', '筒', '答', '策', '筠', '筵', '筷', '箋', '箏', '箔', '箕', '算', '箝', '管', '箭', '箱', '箴', '節', '篁', '範', '篆', '篇', '築', '篙', '篛', '篡', '篤', '篩', '篷', '篾', '簇', '簍', '簑', '簞', '簡', '簣', '簧', '簪', '簫', '簷', '簸', '簽', '簾', '簿', '籃', '籌', '籍', '籐', '籟', '籠', '籤', '籬', '籮', '籲', '米', '粉', '粒', '粗', '粟', '粥', '粱', '粳', '粵', '粹', '粽', '精', '糊', '糕', '糖', '糙', '糜', '糞', '糟', '糠', '糢', '糧', '糯', '糸', '系', '糾', '紀', '紂', '約', '紅', '紇', '紉', '紊', '紋', '納', '紐', '純', '紕', '紗', '紙', '級', '紛', '紜', '素', '紡', '索', '紫', '紮', '累', '細', '紳', '紹', '紼', '絀', '終', '絃', '組', '絆', '結', '絕', '絞', '絡', '絢', '給', '絨', '絮', '統', '絲', '絹', '綁', '綏', '綑', '經', '綜', '綞', '綠', '綢', '維', '綰', '綱', '網', '綴', '綵', '綸', '綺', '綻', '綽', '綾', '綿', '緇', '緊', '緒', '緘', '線', '緝', '緞', '締', '緣', '編', '緩', '緬', '緯', '練', '緻', '縈', '縊', '縑', '縛', '縣', '縫', '縮', '縱', '縲', '縷', '總', '績', '繁', '繃', '繅', '繆', '織', '繕', '繚', '繞', '繡', '繩', '繪', '繫', '繭', '繹', '繼', '繽', '纂', '續', '纏', '纓', '纖', '纜', '缶', '缸', '缺', '缽', '罄', '罈', '罐', '罔', '罕', '罟', '罩', '罪', '置', '罰', '署', '罵', '罷', '罹', '羅', '羈', '羊', '羋', '羌', '美', '羔', '羚', '羞', '群', '羨', '義', '羯', '羲', '羶', '羸', '羹', '羽', '羿', '翁', '翅', '翌', '翎', '習', '翔', '翕', '翟', '翠', '翡', '翩', '翰', '翱', '翳', '翹', '翻', '翼', '耀', '老', '考', '者', '耆', '而', '耍', '耐', '耒', '耕', '耗', '耘', '耙', '耜', '耳', '耶', '耽', '耿', '聆', '聊', '聖', '聘', '聚', '聞', '聯', '聰', '聱', '聲', '聳', '聶', '職', '聽', '聾', '聿', '肄', '肅', '肆', '肇', '肉', '肋', '肌', '肓', '肖', '肘', '肚', '肛', '肝', '股', '肢', '肥', '肩', '肪', '肫', '肯', '肱', '育', '肴', '肺', '胃', '胄', '背', '胎', '胖', '胚', '胛', '胞', '胡', '胤', '胥', '胭', '胰', '胱', '胳', '胴', '胸', '能', '脂', '脅', '脆', '脈', '脊', '脖', '脣', '脩', '脫', '脯', '脹', '脾', '腆', '腋', '腎', '腐', '腑', '腔', '腕', '腥', '腦', '腫', '腮', '腰', '腱', '腳', '腸', '腹', '腺', '腿', '膀', '膈', '膊', '膏', '膚', '膛', '膜', '膝', '膠', '膨', '膩', '膳', '膺', '膽', '膾', '膿', '臀', '臂', '臃', '臆', '臉', '臍', '臏', '臘', '臚', '臟', '臣', '臥', '臧', '臨', '自', '臬', '臭', '至', '致', '臺', '臻', '臼', '臾', '舀', '舂', '舅', '與', '興', '舉', '舊', '舌', '舍', '舐', '舒', '舔', '舛', '舜', '舞', '舟', '舢', '舨', '航', '舫', '般', '舵', '舶', '舷', '船', '艇', '艘', '艙', '艦', '艮', '良', '艱', '色', '艾', '芋', '芍', '芒', '芙', '芝', '芟', '芥', '芬', '芭', '花', '芳', '芹', '芻', '芽', '苑', '苒', '苓', '苔', '苗', '苛', '苜', '苞', '苟', '苣', '若', '苦', '苧', '英', '茁', '茂', '范', '茄', '茅', '茉', '茗', '茫', '茱', '茲', '茴', '茵', '茶', '茸', '茹', '荀', '草', '荊', '荏', '荐', '荒', '荔', '荷', '荸', '荻', '荼', '莉', '莊', '莎', '莒', '莓', '莖', '莘', '莞', '莠', '莢', '莫', '莽', '菁', '菅', '菊', '菌', '菜', '菠', '菩', '華', '菰', '菱', '菲', '菴', '菸', '菽', '萃', '萄', '萊', '萋', '萌', '萍', '萎', '萬', '萱', '萵', '萸', '萼', '落', '葉', '著', '葛', '葡', '董', '葦', '葩', '葫', '葬', '葵', '葷', '蒂', '蒐', '蒙', '蒜', '蒞', '蒲', '蒸', '蒼', '蒿', '蓀', '蓄', '蓆', '蓉', '蓋', '蓓', '蓬', '蓮', '蓿', '蔑', '蔓', '蔔', '蔗', '蔚', '蔡', '蔣', '蔥', '蔬', '蔭', '蔽', '蕃', '蕈', '蕉', '蕊', '蕙', '蕨', '蕩', '蕪', '蕭', '蕾', '薄', '薇', '薑', '薔', '薛', '薜', '薩', '薪', '薯', '薰', '藉', '藍', '藏', '藐', '藕', '藝', '藤', '藥', '藩', '藪', '藹', '藺', '藻', '蘆', '蘇', '蘊', '蘋', '蘑', '蘗', '蘚', '蘭', '蘸', '蘿', '虎', '虐', '虔', '處', '虛', '虜', '虞', '號', '虧', '虫', '虱', '虹', '蚊', '蚌', '蚓', '蚣', '蚤', '蚩', '蚪', '蚯', '蚱', '蚵', '蚶', '蛀', '蛄', '蛆', '蛇', '蛋', '蛔', '蛙', '蛛', '蛟', '蛤', '蛭', '蛹', '蛻', '蛾', '蜀', '蜂', '蜃', '蜇', '蜈', '蜓', '蜘', '蜜', '蜢', '蜥', '蜴', '蜻', '蜿', '蝌', '蝕', '蝗', '蝙', '蝠', '蝦', '蝨', '蝴', '蝶', '蝸', '螂', '螃', '融', '螞', '螟', '螢', '螫', '螳', '螺', '螻', '蟀', '蟆', '蟈', '蟋', '蟑', '蟒', '蟬', '蟯', '蟲', '蟹', '蟻', '蠅', '蠍', '蠔', '蠕', '蠟', '蠡', '蠢', '蠣', '蠱', '蠶', '蠹', '蠻', '血', '行', '衍', '術', '街', '衙', '衛', '衝', '衡', '衢', '衣', '表', '衫', '衰', '衷', '袁', '袂', '袈', '袋', '袍', '袒', '袖', '袞', '被', '袱', '裁', '裂', '裊', '裔', '裕', '裘', '裙', '補', '裝', '裟', '裡', '裨', '裳', '裴', '裸', '裹', '製', '褂', '複', '褐', '褒', '褓', '褚', '褥', '褪', '褫', '褲', '褶', '褸', '褻', '襄', '襖', '襟', '襠', '襤', '襪', '襯', '襲', '西', '要', '覃', '覆', '見', '規', '覓', '視', '覦', '親', '覬', '覲', '覺', '覽', '觀', '角', '解', '觴', '觸', '言', '訂', '訃', '計', '訊', '訌', '討', '訐', '訓', '訕', '訖', '託', '記', '訛', '訝', '訟', '訣', '訥', '訪', '設', '許', '訴', '診', '註', '証', '詁', '詆', '詐', '詔', '評', '詛', '詞', '詠', '詢', '詣', '試', '詩', '詫', '詬', '詭', '詮', '詰', '話', '該', '詳', '詹', '詼', '誅', '誇', '誌', '認', '誑', '誓', '誕', '誘', '語', '誠', '誡', '誣', '誤', '誥', '誦', '誨', '說', '誰', '課', '誼', '調', '諂', '諄', '談', '諉', '請', '諍', '諒', '論', '諜', '諦', '諧', '諫', '諭', '諮', '諱', '諷', '諸', '諺', '諾', '謀', '謁', '謂', '謄', '謊', '謎', '謗', '謙', '講', '謝', '謠', '謨', '謬', '謹', '譁', '證', '譎', '譏', '識', '譚', '譜', '警', '譬', '譯', '議', '譴', '護', '譽', '讀', '變', '讒', '讓', '讖', '讚', '谷', '谿', '豁', '豆', '豈', '豉', '豌', '豎', '豐', '豔', '豕', '豚', '象', '豢', '豪', '豫', '豬', '豹', '豺', '貂', '貉', '貊', '貌', '貍', '貓', '貝', '貞', '負', '財', '貢', '貧', '貨', '販', '貪', '貫', '責', '貯', '貲', '貳', '貴', '貶', '買', '貸', '費', '貼', '貽', '貿', '賀', '賁', '賂', '賃', '賄', '賅', '資', '賈', '賊', '賑', '賒', '賓', '賜', '賞', '賠', '賢', '賣', '賤', '賦', '質', '賬', '賭', '賴', '賺', '購', '賽', '贅', '贈', '贊', '贍', '贏', '贓', '贖', '贗', '贛', '赤', '赦', '赧', '赫', '赭', '走', '赳', '赴', '起', '趁', '超', '越', '趕', '趙', '趟', '趣', '趨', '足', '趴', '趾', '跆', '跋', '跌', '跎', '跑', '跚', '跛', '距', '跟', '跡', '跨', '跪', '路', '跳', '跺', '跼', '踏', '踐', '踝', '踟', '踢', '踩', '踫', '踱', '踴', '踵', '踹', '蹂', '蹄', '蹈', '蹉', '蹊', '蹋', '蹙', '蹣', '蹤', '蹦', '蹬', '蹲', '蹶', '蹺', '蹼', '躁', '躂', '躅', '躇', '躉', '躊', '躍', '躑', '躡', '躪', '身', '躬', '躲', '躺', '軀', '車', '軋', '軌', '軍', '軒', '軔', '軛', '軟', '軸', '軻', '軼', '軾', '較', '載', '輊', '輒', '輓', '輔', '輕', '輛', '輜', '輝', '輟', '輦', '輩', '輪', '輯', '輸', '輻', '輾', '輿', '轂', '轄', '轅', '轉', '轍', '轎', '轔', '轟', '轡', '辛', '辜', '辟', '辣', '辦', '辨', '辭', '辮', '辯', '辰', '辱', '農', '迂', '迄', '迅', '迆', '迎', '近', '返', '迢', '迥', '迦', '迪', '迫', '迭', '述', '迴', '迷', '迺', '追', '退', '送', '逃', '逅', '逆', '逍', '透', '逐', '途', '逕', '逖', '逗', '這', '通', '逛', '逝', '逞', '速', '造', '逢', '連', '逮', '週', '進', '逵', '逸', '逼', '逾', '遁', '遂', '遇', '遊', '運', '遍', '過', '遏', '遐', '遑', '道', '達', '違', '遘', '遙', '遜', '遞', '遠', '遣', '遨', '適', '遭', '遮', '遲', '遴', '遵', '遷', '選', '遺', '遼', '遽', '避', '邀', '邁', '邂', '還', '邇', '邊', '邏', '邐', '邑', '邕', '邢', '那', '邦', '邪', '邱', '邵', '邸', '郁', '郊', '郎', '郡', '部', '郭', '郵', '都', '鄂', '鄉', '鄒', '鄙', '鄧', '鄭', '鄰', '鄱', '鄹', '酉', '酊', '酋', '酌', '配', '酒', '酗', '酣', '酥', '酩', '酪', '酬', '酵', '酷', '酸', '醃', '醇', '醉', '醋', '醒', '醜', '醞', '醣', '醫', '醬', '醺', '釀', '釁', '采', '釉', '釋', '里', '重', '野', '量', '釐', '金', '釗', '釘', '釜', '針', '釣', '釦', '釧', '釵', '鈉', '鈍', '鈐', '鈔', '鈕', '鈞', '鈣', '鈴', '鈷', '鈸', '鈽', '鈾', '鉀', '鉋', '鉑', '鉗', '鉛', '鉤', '鉸', '鉻', '銀', '銅', '銓', '銖', '銘', '銜', '銬', '銳', '銷', '銻', '銼', '鋁', '鋅', '鋒', '鋤', '鋪', '鋸', '鋼', '錄', '錐', '錘', '錚', '錠', '錢', '錦', '錨', '錫', '錯', '錳', '錶', '鍊', '鍋', '鍍', '鍛', '鍥', '鍬', '鍰', '鍵', '鍾', '鎂', '鎊', '鎔', '鎖', '鎢', '鎮', '鎳', '鏃', '鏈', '鏍', '鏑', '鏖', '鏗', '鏘', '鏜', '鏝', '鏟', '鏡', '鏢', '鏤', '鏽', '鐃', '鐘', '鐮', '鐲', '鐳', '鐵', '鐸', '鐺', '鑄', '鑑', '鑒', '鑠', '鑣', '鑰', '鑲', '鑼', '鑽', '鑾', '鑿', '長', '門', '閂', '閃', '閉', '開', '閏', '閑', '閒', '間', '閔', '閘', '閡', '閣', '閤', '閥', '閨', '閩', '閭', '閱', '閻', '闆', '闈', '闊', '闋', '闌', '闐', '闔', '闖', '關', '闡', '闢', '阜', '阡', '阪', '阮', '阱', '防', '阻', '阿', '陀', '附', '陋', '陌', '降', '限', '陛', '陝', '陡', '院', '陣', '除', '陪', '陰', '陲', '陳', '陴', '陵', '陶', '陷', '陸', '陽', '隅', '隆', '隊', '隋', '隍', '階', '隔', '隕', '隘', '隙', '際', '障', '隧', '隨', '險', '隱', '隴', '隸', '隻', '雀', '雁', '雄', '雅', '集', '雇', '雉', '雋', '雌', '雍', '雕', '雖', '雙', '雛', '雜', '雞', '離', '難', '雨', '雪', '雯', '雲', '零', '雷', '雹', '電', '需', '霄', '霆', '震', '霉', '霍', '霎', '霏', '霑', '霓', '霖', '霜', '霞', '霧', '霪', '露', '霸', '霹', '霽', '霾', '靂', '靄', '靈', '青', '靖', '靛', '靜', '非', '靠', '靡', '面', '靦', '靨', '革', '靴', '靶', '靼', '鞅', '鞋', '鞍', '鞏', '鞘', '鞠', '鞣', '鞦', '鞭', '韁', '韃', '韆', '韋', '韌', '韓', '韜', '韭', '音', '韶', '韻', '響', '頁', '頂', '頃', '項', '順', '須', '頊', '頌', '預', '頑', '頒', '頓', '頗', '領', '頡', '頤', '頭', '頰', '頷', '頸', '頹', '頻', '顆', '題', '額', '顎', '顏', '顓', '願', '顛', '類', '顧', '顫', '顯', '顰', '顱', '風', '颯', '颱', '颳', '颶', '颺', '颼', '飄', '飛', '食', '飢', '飧', '飩', '飪', '飭', '飯', '飲', '飴', '飼', '飽', '飾', '餃', '餅', '餉', '養', '餌', '餐', '餒', '餓', '餘', '餛', '餞', '餡', '館', '餵', '餽', '餾', '餿', '饅', '饑', '饒', '饜', '饞', '首', '香', '馥', '馨', '馬', '馭', '馮', '馱', '馳', '馴', '駁', '駐', '駑', '駒', '駕', '駙', '駛', '駝', '駟', '駢', '駭', '駱', '駿', '騁', '騎', '騖', '騙', '騫', '騰', '騷', '騾', '驀', '驃', '驅', '驕', '驗', '驚', '驛', '驟', '驢', '驥', '驪', '骨', '骯', '骰', '骷', '骸', '骼', '髏', '髒', '髓', '體', '高', '髦', '髭', '髮', '髯', '髻', '鬃', '鬆', '鬍', '鬚', '鬢', '鬥', '鬧', '鬨', '鬱', '鬲', '鬼', '魁', '魂', '魄', '魅', '魏', '魔', '魘', '魚', '魯', '魷', '鮑', '鮪', '鮫', '鮮', '鯉', '鯊', '鯧', '鯨', '鯽', '鰍', '鰓', '鰥', '鰭', '鰱', '鰻', '鰾', '鱉', '鱔', '鱖', '鱗', '鱷', '鱸', '鳥', '鳩', '鳳', '鳴', '鳶', '鴆', '鴉', '鴒', '鴕', '鴛', '鴣', '鴦', '鴨', '鴻', '鴿', '鵑', '鵝', '鵠', '鵡', '鵪', '鵬', '鵲', '鶉', '鶯', '鶴', '鷂', '鷓', '鷗', '鷥', '鷹', '鷺', '鸚', '鸞', '鹹', '鹼', '鹽', '鹿', '麂', '麋', '麒', '麓', '麗', '麝', '麟', '麥', '麩', '麴', '麵', '麻', '麼', '麾', '黃', '黍', '黎', '黏', '黑', '黔', '默', '黛', '黜', '黝', '點', '黠', '黨', '黯', '黴', '黷', '鼇', '鼎', '鼓', '鼕', '鼙', '鼠', '鼬', '鼴', '鼻', '鼾', '齊', '齋', '齒', '齜', '齟', '齡', '齣', '齦', '齪', '齬', '齲', '齷', '龍', '龐', '龔']\n",
      "\n",
      "開始 Optuna 超參數優化...\n",
      "這將進行多次試驗來找到最佳參數組合\n",
      "將進行 5 次試驗...\n",
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-06-10 19:28:42,416] Trial 0 failed with parameters: {'lr': 0.0001329291894316216, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0029154431891537554, 'scheduler': 'ExponentialLR', 'gamma': 0.9364594334728974, 'dropout_rate': 0.4329770563201687} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yahui\\AppData\\Local\\Temp\\ipykernel_28072\\182183883.py\", line 215, in objective\n",
      "    best_acc = train_model_optuna(model, criterion, optimizer, scheduler,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yahui\\AppData\\Local\\Temp\\ipykernel_28072\\182183883.py\", line 136, in train_model_optuna\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-06-10 19:28:42,438] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 754\u001b[39m\n\u001b[32m    751\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   準確率提升: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary[\u001b[33m'\u001b[39m\u001b[33moptimization_summary\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mimprovement\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 594\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    591\u001b[39m n_trials = \u001b[32m5\u001b[39m  \u001b[38;5;66;03m# 可以根據需要調整試驗次數\u001b[39;00m\n\u001b[32m    592\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m將進行 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 次試驗...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[38;5;66;03m# 顯示最佳結果\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Optuna 優化完成！\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 215\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# 簡化的訓練（較少 epochs 用於優化）\u001b[39;00m\n\u001b[32m    214\u001b[39m num_epochs_optuna = \u001b[32m3\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m best_acc = \u001b[43mtrain_model_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mnum_epochs_optuna\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_acc\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mtrain_model_optuna\u001b[39m\u001b[34m(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs)\u001b[39m\n\u001b[32m    134\u001b[39m     _, preds = torch.max(outputs, \u001b[32m1\u001b[39m)\n\u001b[32m    135\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     optimizer.step()\n\u001b[32m    139\u001b[39m running_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Setup parameters and check CUDA\n",
    "def check_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        current_device = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(current_device)\n",
    "        print(f\"CUDA 可用，使用 GPU 訓練\")\n",
    "        print(f\"當前 GPU 名稱: {device_name}\")\n",
    "        print(f\"GPU 數量: {device_count}\")\n",
    "        print(f\"當前使用的 GPU: {device_name}\")\n",
    "        print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "        print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"CUDA 不可用，將使用 CPU 訓練\")\n",
    "        return False\n",
    "\n",
    "use_gpu = check_cuda()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 設定本地路徑\n",
    "data_dir = 'Handwritten_Data' # 你的資料路徑\n",
    "input_size = 224\n",
    "\n",
    "# 設定 CUDA 優化參數\n",
    "if use_gpu:\n",
    "    torch.backends.cudnn.benchmark = True  # 加速 convolution 運算\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Global variables for data loading\n",
    "train_loader_global = None\n",
    "val_loader_global = None\n",
    "train_size_global = None\n",
    "val_size_global = None\n",
    "class_num_global = None\n",
    "class_names_global = None\n",
    "\n",
    "# Step 2: Define data loading functions\n",
    "def loaddata(data_dir, batch_size, shuffle=True):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # 原始無轉換資料集\n",
    "    raw_dataset = datasets.ImageFolder(root=data_dir, transform=transforms.ToTensor())\n",
    "\n",
    "    train_size = int(0.8 * len(raw_dataset))\n",
    "    val_size = len(raw_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(raw_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    class DatasetWrapper(torch.utils.data.Dataset):\n",
    "        def __init__(self, subset, transform):\n",
    "            self.subset = subset\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.subset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image, label = self.subset.dataset[self.subset.indices[idx]]\n",
    "            image = transforms.ToPILImage()(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "    train_dataset_wrapped = DatasetWrapper(train_dataset, data_transforms['train'])\n",
    "    val_dataset_wrapped = DatasetWrapper(val_dataset, data_transforms['val'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset_wrapped, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=use_gpu)\n",
    "    val_loader = DataLoader(val_dataset_wrapped, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=use_gpu)\n",
    "\n",
    "    return train_loader, val_loader, train_size, val_size, len(raw_dataset.classes), raw_dataset.classes\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr_scheduler(optimizer, scheduler_type='StepLR', step_size=2, gamma=0.5):\n",
    "    \"\"\"建立學習率調度器\"\"\"\n",
    "    if scheduler_type == 'StepLR':\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    elif scheduler_type == 'ExponentialLR':\n",
    "        return optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "    elif scheduler_type == 'CosineAnnealingLR':\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    else:\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training function for Optuna optimization\n",
    "def train_model_optuna(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs=10):\n",
    "    \"\"\"優化版訓練函數，僅返回最佳驗證準確率\"\"\"\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 訓練階段\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        # 驗證階段\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_acc = val_running_corrects.double() / val_size\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "    return best_acc.item()\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna 目標函數\"\"\"\n",
    "    # 建議超參數\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128] if use_gpu else [8, 16, 32])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    scheduler_type = trial.suggest_categorical('scheduler', ['StepLR', 'ExponentialLR', 'CosineAnnealingLR'])\n",
    "    \n",
    "    if scheduler_type == 'StepLR':\n",
    "        step_size = trial.suggest_int('step_size', 2, 8)\n",
    "        gamma = trial.suggest_float('gamma', 0.1, 0.9)\n",
    "    else:\n",
    "        step_size = 2\n",
    "        gamma = trial.suggest_float('gamma', 0.5, 0.95)\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    \n",
    "    # 重新載入資料（使用新的 batch_size）\n",
    "    train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(data_dir, batch_size)\n",
    "    \n",
    "    # 建立模型\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    num_ftrs = model._fc.in_features\n",
    "    \n",
    "    # 加入 Dropout\n",
    "    model._fc = nn.Sequential(\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(num_ftrs, class_num)\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 設定優化器\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:  # SGD\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    \n",
    "    # 設定學習率調度器\n",
    "    scheduler = get_lr_scheduler(optimizer, scheduler_type, step_size, gamma)\n",
    "    \n",
    "    # 損失函數\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 簡化的訓練（較少 epochs 用於優化）\n",
    "    num_epochs_optuna = 3\n",
    "    best_acc = train_model_optuna(model, criterion, optimizer, scheduler, \n",
    "                                 train_loader, val_loader, train_size, val_size, \n",
    "                                 num_epochs_optuna)\n",
    "    \n",
    "    return best_acc\n",
    "\n",
    "# Training function with full metrics\n",
    "def train_model_full(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs=10):\n",
    "    \"\"\"完整的訓練函數，包含所有指標記錄\"\"\"\n",
    "    since = time.time()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # 記錄用\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    train_precisions, val_precisions = [], []\n",
    "    train_recalls, val_recalls = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "\n",
    "    print(f\"開始訓練，使用設備: {device}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 40)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'當前學習率: {current_lr:.6f}')\n",
    "\n",
    "        # 訓練階段\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            all_train_preds.append(preds.cpu())\n",
    "            all_train_labels.append(labels.cpu())\n",
    "\n",
    "            if i % 50 == 0 or i == len(train_loader) - 1:\n",
    "                print(f'  批次 [{i+1}/{len(train_loader)}] Loss: {loss.item():.4f}')\n",
    "\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        print(f'訓練 Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc.cpu().item())\n",
    "\n",
    "        # 統計訓練 Precision/Recall/F1\n",
    "        train_preds = torch.cat(all_train_preds)\n",
    "        train_labels = torch.cat(all_train_labels)\n",
    "        train_precisions.append(precision_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "        train_recalls.append(recall_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "        train_f1s.append(f1_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "\n",
    "        # 驗證階段\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                all_val_preds.append(preds.cpu())\n",
    "                all_val_labels.append(labels.cpu())\n",
    "\n",
    "        epoch_loss = running_loss / val_size\n",
    "        epoch_acc = running_corrects.double() / val_size\n",
    "        print(f'驗證 Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        val_losses.append(epoch_loss)\n",
    "        val_accs.append(epoch_acc.cpu().item())\n",
    "\n",
    "        # 統計驗證 Precision/Recall/F1\n",
    "        val_preds = torch.cat(all_val_preds)\n",
    "        val_labels = torch.cat(all_val_labels)\n",
    "        val_precisions.append(precision_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        val_recalls.append(recall_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        val_f1s.append(f1_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            \n",
    "            os.makedirs('./best', exist_ok=True)\n",
    "            checkpoint_path = os.path.join(data_dir, 'best_model_checkpoint.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': epoch_loss,\n",
    "                'acc': epoch_acc,\n",
    "            }, checkpoint_path)\n",
    "            print(f'新的最佳模型已儲存，準確率: {best_acc:.4f}')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'訓練完成，耗時 {time_elapsed // 60:.0f}分 {time_elapsed % 60:.0f}秒')\n",
    "    print(f'最佳驗證準確率: {best_acc:.4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # 儲存模型\n",
    "    save_dir = os.path.join(data_dir, 'model')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs('./best', exist_ok=True)\n",
    "    model_path = os.path.join('./best', 'best.pth')\n",
    "    torch.save(model, model_path)\n",
    "    print(f'最終模型已儲存至: {model_path}')\n",
    "\n",
    "    # 繪製訓練曲線\n",
    "    plot_training_curves(train_losses, val_losses, train_accs, val_accs,\n",
    "                        train_precisions, val_precisions, train_recalls, val_recalls,\n",
    "                        train_f1s, val_f1s, num_epochs, save_dir)\n",
    "\n",
    "    return model, best_acc\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, train_accs, val_accs,\n",
    "                        train_precisions, val_precisions, train_recalls, val_recalls,\n",
    "                        train_f1s, val_f1s, num_epochs, save_dir):\n",
    "    \"\"\"繪製訓練曲線\"\"\"\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='train Loss')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='validation Loss')\n",
    "    plt.title('Loss change')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, num_epochs+1), train_accs, 'b-', label='train Accuracy')\n",
    "    plt.plot(range(1, num_epochs+1), val_accs, 'r-', label='validation Accuracy')\n",
    "    plt.title('Accuracy change')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, num_epochs+1), train_precisions, 'b--', label='train Precision')\n",
    "    plt.plot(range(1, num_epochs+1), val_precisions, 'r--', label='train Precision')\n",
    "    plt.plot(range(1, num_epochs+1), train_recalls, 'b:', label='train Recall')\n",
    "    plt.plot(range(1, num_epochs+1), val_recalls, 'r:', label='train Recall')\n",
    "    plt.title('Precision / Recall change')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, num_epochs+1), train_f1s, 'b-', label='train F1')\n",
    "    plt.plot(range(1, num_epochs+1), val_f1s, 'r-', label='train F1')\n",
    "    plt.title('F1-score change')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    curve_path = os.path.join(save_dir, 'training_curves_all_metrics.png')\n",
    "    plt.savefig(curve_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'訓練曲線（包含 F1、Precision、Recall）已儲存至: {curve_path}')\n",
    "\n",
    "def plot_optuna_results(study):\n",
    "    \"\"\"繪製 Optuna 優化結果圖表\"\"\"\n",
    "    print(\"\\n繪製 Optuna 優化結果...\")\n",
    "    \n",
    "    # 創建子圖\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. 優化歷史\n",
    "    try:\n",
    "        optuna.visualization.matplotlib.plot_optimization_history(study, ax=axes[0,0])\n",
    "        axes[0,0].set_title('optimization_history')\n",
    "    except Exception as e:\n",
    "        print(f\"無法繪製優化歷史: {e}\")\n",
    "        axes[0,0].text(0.5, 0.5, '優化歷史圖表不可用', ha='center', va='center')\n",
    "    \n",
    "    # 2. 參數重要性\n",
    "    try:\n",
    "        optuna.visualization.matplotlib.plot_param_importances(study, ax=axes[0,1])\n",
    "        axes[0,1].set_title('param_importances')\n",
    "    except Exception as e:\n",
    "        print(f\"無法繪製參數重要性: {e}\")\n",
    "        axes[0,1].text(0.5, 0.5, '參數重要性圖表不可用', ha='center', va='center')\n",
    "    \n",
    "    # 3. 試驗值分布\n",
    "    values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "    if values:\n",
    "        axes[1,0].hist(values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[1,0].set_title('試驗值分布')\n",
    "        axes[1,0].set_xlabel('accuracy')\n",
    "        axes[1,0].set_ylabel('frequency')#頻次\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 最佳參數表格\n",
    "    axes[1,1].axis('off')\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "    \n",
    "    param_text = f\"最佳準確率: {best_value:.4f}\\n\\n最佳參數:\\n\"\n",
    "    for key, value in best_params.items():\n",
    "        param_text += f\"{key}: {value}\\n\"\n",
    "    \n",
    "    axes[1,1].text(0.1, 0.9, param_text, transform=axes[1,1].transAxes, \n",
    "                   fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    axes[1,1].set_title('最佳參數')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 儲存圖表\n",
    "    optuna_results_path = os.path.join(data_dir, 'optuna_optimization_results.png')\n",
    "    plt.savefig(optuna_results_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Optuna 優化結果已儲存至: {optuna_results_path}')\n",
    "    \n",
    "    # 創建詳細的參數對比圖\n",
    "    plot_parameter_comparison(study)\n",
    "\n",
    "def plot_parameter_comparison(study):\n",
    "    \"\"\"繪製參數對比圖\"\"\"\n",
    "    print(\"繪製參數對比圖...\")\n",
    "    \n",
    "    df = study.trials_dataframe()\n",
    "    if len(df) == 0:\n",
    "        print(\"沒有試驗數據可以繪製\"\n",
    "        return\n",
    "    \n",
    "    # 提取數值參數\n",
    "    numeric_params = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith('params_') and df[col].dtype in ['float64', 'int64']:\n",
    "            numeric_params.append(col)\n",
    "    \n",
    "    if len(numeric_params) >= 2:\n",
    "        n_params = len(numeric_params)\n",
    "        n_cols = min(3, n_params)\n",
    "        n_rows = (n_params + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        elif n_cols == 1:\n",
    "            axes = [[ax] for ax in axes]\n",
    "        \n",
    "        for i, param in enumerate(numeric_params):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            ax = axes[row][col] if n_rows > 1 else axes[col]\n",
    "            \n",
    "            # 散點圖：參數值 vs 準確率\n",
    "            x = df[param].values\n",
    "            y = df['value'].values\n",
    "            \n",
    "            # 移除 NaN 值\n",
    "            mask = ~(pd.isna(x) | pd.isna(y))\n",
    "            x = x[mask]\n",
    "            y = y[mask]\n",
    "            \n",
    "            if len(x) > 0:\n",
    "                ax.scatter(x, y, alpha=0.6)\n",
    "                ax.set_xlabel(param.replace('params_', ''))\n",
    "                ax.set_ylabel('accuracy')\n",
    "                ax.set_title(f'{param.replace(\"params_\", \"\")} vs accuracy')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 隱藏多餘的子圖\n",
    "        for i in range(n_params, n_rows * n_cols):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            if n_rows > 1:\n",
    "                axes[row][col].set_visible(False)\n",
    "            elif n_cols > 1:\n",
    "                axes[col].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        param_comp_path = os.path.join(data_dir, 'parameter_comparison.png')\n",
    "        plt.savefig(param_comp_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f'參數對比圖已儲存至: {param_comp_path}')\n",
    "\n",
    "def test_samples(model, val_loader, class_names, device, num_samples=10):\n",
    "    \"\"\"測試幾個樣本\"\"\"\n",
    "    model.eval()\n",
    "    samples_tested = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(min(len(preds), num_samples - samples_tested)):\n",
    "                pred_class = class_names[preds[i]]\n",
    "                true_class = class_names[labels[i]]\n",
    "                is_correct = preds[i] == labels[i]\n",
    "                \n",
    "                status = \"✓\" if is_correct else \"✗\"\n",
    "                print(f\"{status} 預測: {pred_class} | 實際: {true_class}\")\n",
    "                \n",
    "                if is_correct:\n",
    "                    correct_predictions += 1\n",
    "                samples_tested += 1\n",
    "                \n",
    "                if samples_tested >= num_samples:\n",
    "                    break\n",
    "            \n",
    "            if samples_tested >= num_samples:\n",
    "                break\n",
    "    \n",
    "    accuracy = correct_predictions / samples_tested\n",
    "    print(f\"\\n測試樣本準確率: {accuracy:.4f} ({correct_predictions}/{samples_tested})\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主程式\"\"\"\n",
    "    print(\"開始手寫字符辨識訓練 - 使用 Optuna 優化\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 檢查資料路徑\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"資料夾不存在: {data_dir}\")\n",
    "        print(\"請確認資料夾路徑是否正確\")\n",
    "        return\n",
    "    \n",
    "    print(\"載入資料...\")\n",
    "    try:\n",
    "        # 載入資料用於優化\n",
    "        train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(data_dir, 64)\n",
    "        print(f\"資料載入成功\")\n",
    "        print(f\"   訓練樣本: {train_size}\")\n",
    "        print(f\"   驗證樣本: {val_size}\")\n",
    "        print(f\"   字符類別: {class_num}\")\n",
    "        print(f\"   類別名稱: {class_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"資料載入失敗: {e}\")\n",
    "        return\n",
    "\n",
    "    # === Optuna 超參數優化 ===\n",
    "    print(f\"\\n開始 Optuna 超參數優化...\")\n",
    "    print(\"這將進行多次試驗來找到最佳參數組合\")\n",
    "    \n",
    "    # 創建研究\n",
    "    study = optuna.create_study(direction='maximize', \n",
    "                               study_name='handwriting_optimization',\n",
    "                               sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    \n",
    "    # 執行優化\n",
    "    n_trials = 5  # 可以根據需要調整試驗次數\n",
    "    print(f\"將進行 {n_trials} 次試驗...\")\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, timeout=None)\n",
    "    \n",
    "    # 顯示最佳結果\n",
    "    print(f\"\\n Optuna 優化完成！\")\n",
    "    print(f\"最佳準確率: {study.best_value:.4f}\")\n",
    "    print(f\"最佳參數: {study.best_params}\")\n",
    "    \n",
    "    # 繪製優化結果\n",
    "    plot_optuna_results(study)\n",
    "    \n",
    "    # === 使用最佳參數進行完整訓練 ===\n",
    "    print(f\"\\n使用最佳參數進行完整訓練...\")\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # 重建模型和優化器\n",
    "    train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(\n",
    "        data_dir, best_params['batch_size'])\n",
    "    \n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    num_ftrs = model._fc.in_features\n",
    "    model._fc = nn.Sequential(\n",
    "        nn.Dropout(best_params['dropout_rate']),\n",
    "        nn.Linear(num_ftrs, class_num)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 設定優化器\n",
    "    if best_params['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), \n",
    "                              lr=best_params['lr'], \n",
    "                              weight_decay=best_params['weight_decay'])\n",
    "    elif best_params['optimizer'] == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), \n",
    "                               lr=best_params['lr'], \n",
    "                               weight_decay=best_params['weight_decay'])\n",
    "    else:  # SGD\n",
    "        optimizer = optim.SGD(model.parameters(), \n",
    "                             lr=best_params['lr'], \n",
    "                             momentum=0.9, \n",
    "                             weight_decay=best_params['weight_decay'])\n",
    "    \n",
    "    # 設定學習率調度器\n",
    "    if best_params['scheduler'] == 'StepLR':\n",
    "        scheduler = get_lr_scheduler(optimizer, 'StepLR', \n",
    "                                   best_params.get('step_size', 2), \n",
    "                                   best_params['gamma'])\n",
    "    else:\n",
    "        scheduler = get_lr_scheduler(optimizer, best_params['scheduler'], \n",
    "                                   gamma=best_params['gamma'])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(f\"使用最佳參數:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # 完整訓練\n",
    "    num_epochs_final = 5  # 更多的訓練輪數用於最終訓練\n",
    "    try:\n",
    "        model, best_acc = train_model_full(\n",
    "            model, criterion, optimizer, scheduler,\n",
    "            train_loader, val_loader,\n",
    "            train_size, val_size,\n",
    "            num_epochs=num_epochs_final\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🎉 完整訓練完成！最終準確率: {best_acc:.4f}\")\n",
    "        \n",
    "        # 測試幾個樣本\n",
    "        print(\"\\n測試幾個樣本...\")\n",
    "        test_samples(model, val_loader, class_names, device)\n",
    "        \n",
    "        # 保存最佳參數到文件\n",
    "        import json\n",
    "        params_file = os.path.join(data_dir, 'best_hyperparameters.json')\n",
    "        with open(params_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"最佳超參數已保存到: {params_file}\")\n",
    "        \n",
    "        # 創建優化摘要報告\n",
    "        create_optimization_summary(study, best_acc, data_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"完整訓練過程中發生錯誤: {e}\")\n",
    "        return\n",
    "\n",
    "def create_optimization_summary(study, final_accuracy, save_dir):\n",
    "    \"\"\"創建優化摘要報告\"\"\"\n",
    "    print(\"\\n創建優化摘要報告...\")\n",
    "    \n",
    "    # 收集統計信息\n",
    "    trials_df = study.trials_dataframe()\n",
    "    completed_trials = trials_df[trials_df['state'] == 'COMPLETE']\n",
    "    \n",
    "    if len(completed_trials) == 0:\n",
    "        print(\"沒有完成的試驗可以分析\")\n",
    "        return\n",
    "    \n",
    "    summary = {\n",
    "        'optimization_summary': {\n",
    "            'total_trials': len(study.trials),\n",
    "            'completed_trials': len(completed_trials),\n",
    "            'best_trial_number': study.best_trial.number,\n",
    "            'best_validation_accuracy': study.best_value,\n",
    "            'final_training_accuracy': final_accuracy,\n",
    "            'improvement': final_accuracy - study.best_value if study.best_value else 0\n",
    "        },\n",
    "        'best_parameters': study.best_params,\n",
    "        'statistics': {\n",
    "            'mean_accuracy': float(completed_trials['value'].mean()),\n",
    "            'std_accuracy': float(completed_trials['value'].std()),\n",
    "            'min_accuracy': float(completed_trials['value'].min()),\n",
    "            'max_accuracy': float(completed_trials['value'].max())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 保存摘要到 JSON\n",
    "    import json\n",
    "    summary_file = os.path.join(save_dir, 'optimization_summary.json')\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # 創建文字報告\n",
    "    report_file = os.path.join(save_dir, 'optimization_report.txt')\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(\"手寫字符辨識 - Optuna 優化報告\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"優化摘要:\\n\")\n",
    "        f.write(f\"  總試驗次數: {summary['optimization_summary']['total_trials']}\\n\")\n",
    "        f.write(f\"  完成試驗次數: {summary['optimization_summary']['completed_trials']}\\n\")\n",
    "        f.write(f\"  最佳試驗編號: {summary['optimization_summary']['best_trial_number']}\\n\")\n",
    "        f.write(f\"  最佳驗證準確率: {summary['optimization_summary']['best_validation_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  最終訓練準確率: {summary['optimization_summary']['final_training_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  準確率提升: {summary['optimization_summary']['improvement']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"最佳參數:\\n\")\n",
    "        for key, value in summary['best_parameters'].items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"試驗統計:\\n\")\n",
    "        f.write(f\"  平均準確率: {summary['statistics']['mean_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  準確率標準差: {summary['statistics']['std_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  最低準確率: {summary['statistics']['min_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  最高準確率: {summary['statistics']['max_accuracy']:.4f}\\n\")\n",
    "    \n",
    "    print(f\"優化摘要已保存:\")\n",
    "    print(f\"  JSON格式: {summary_file}\")\n",
    "    print(f\"  文字報告: {report_file}\")\n",
    "    \n",
    "    # 顯示摘要\n",
    "    print(f\"\\n 優化摘要:\")\n",
    "    print(f\"   總試驗次數: {summary['optimization_summary']['total_trials']}\")\n",
    "    print(f\"   最佳驗證準確率: {summary['optimization_summary']['best_validation_accuracy']:.4f}\")\n",
    "    print(f\"   最終訓練準確率: {summary['optimization_summary']['final_training_accuracy']:.4f}\")\n",
    "    print(f\"   準確率提升: {summary['optimization_summary']['improvement']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9b1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available, using GPU training\n",
      "Current GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU count: 1\n",
      "Currently using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA version: 11.8\n",
      "PyTorch version: 2.7.0+cu118\n",
      "Using device: cuda\n",
      "Starting handwritten character recognition training - Using Optuna optimization\n",
      "======================================================================\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-10 19:29:00,489] A new study created in memory with name: handwriting_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "   Training samples: 200569\n",
      "   Validation samples: 50143\n",
      "   Character classes: 4803\n",
      "   Class names: ['丁', '七', '丈', '三', '上', '下', '不', '丐', '丑', '且', '丕', '世', '丘', '丙', '丞', '丟', '並', '丫', '中', '串', '丸', '丹', '主', '乃', '久', '么', '之', '乍', '乎', '乏', '乒', '乓', '乖', '乘', '乙', '九', '乞', '也', '乩', '乳', '乾', '亂', '了', '予', '事', '二', '于', '云', '互', '五', '井', '亙', '些', '亞', '亟', '亡', '交', '亥', '亦', '亨', '享', '京', '亭', '亮', '人', '什', '仁', '仃', '仄', '仆', '仇', '今', '介', '仍', '仔', '仕', '他', '仗', '付', '仙', '仞', '仟', '代', '令', '以', '仰', '仲', '仳', '件', '任', '份', '仿', '企', '伉', '伊', '伍', '伏', '伐', '休', '伕', '伙', '伯', '估', '伴', '伶', '伸', '伺', '似', '伽', '佃', '但', '佇', '位', '低', '住', '佐', '佑', '佔', '何', '佗', '余', '佛', '作', '佝', '佞', '你', '佣', '佩', '佬', '佯', '佰', '佳', '併', '佻', '佾', '使', '侃', '來', '侈', '例', '侍', '侏', '侖', '供', '依', '侮', '侯', '侵', '侶', '便', '係', '促', '俄', '俊', '俎', '俏', '俐', '俑', '俗', '俘', '俚', '保', '俞', '俟', '俠', '信', '修', '俯', '俱', '俳', '俸', '俺', '俾', '倀', '倆', '倉', '個', '倌', '倍', '倏', '們', '倒', '倔', '倖', '倘', '候', '倚', '借', '倡', '倣', '倥', '倦', '倨', '倩', '倪', '倫', '倭', '值', '偃', '假', '偉', '偌', '偎', '偏', '偕', '做', '停', '健', '側', '偵', '偶', '偷', '偺', '偽', '傀', '傅', '傍', '傑', '傖', '傘', '備', '傢', '催', '傭', '傯', '傲', '傳', '債', '傷', '傻', '傾', '僅', '像', '僑', '僕', '僖', '僚', '僥', '僧', '僭', '僮', '僱', '僵', '價', '僻', '儀', '儂', '億', '儈', '儉', '儐', '儒', '儔', '儘', '償', '儡', '優', '儲', '儷', '儼', '兀', '允', '元', '兄', '充', '兆', '兇', '先', '光', '克', '兌', '免', '兒', '兔', '兕', '兗', '兜', '兢', '入', '內', '全', '兩', '八', '公', '六', '兮', '共', '兵', '其', '具', '典', '兼', '冀', '冉', '冊', '再', '冑', '冒', '冕', '冗', '冠', '冢', '冤', '冥', '冬', '冰', '冶', '冷', '冽', '准', '凋', '凌', '凍', '凜', '凝', '几', '凡', '凰', '凱', '凳', '凶', '凸', '凹', '出', '函', '刀', '刁', '刃', '分', '切', '刈', '刊', '刎', '刑', '划', '列', '初', '判', '別', '刨', '利', '刪', '刮', '到', '制', '刷', '券', '刻', '剁', '剃', '則', '削', '剋', '剌', '前', '剎', '剔', '剖', '剛', '剜', '剝', '剩', '剪', '副', '割', '剴', '創', '剷', '剽', '剿', '劃', '劇', '劈', '劉', '劍', '劑', '力', '功', '加', '劣', '助', '努', '劫', '劬', '劾', '勁', '勃', '勇', '勉', '勒', '動', '勗', '勘', '務', '勛', '勝', '勞', '募', '勢', '勤', '勦', '勵', '勸', '勻', '勾', '勿', '包', '匆', '匈', '匍', '匏', '匐', '匕', '化', '北', '匙', '匝', '匠', '匡', '匣', '匪', '匯', '匱', '匹', '匾', '匿', '區', '十', '千', '卅', '升', '午', '卉', '半', '卑', '卒', '卓', '協', '南', '博', '卜', '卞', '占', '卡', '卦', '卮', '卯', '印', '危', '即', '卵', '卷', '卸', '卹', '卻', '卿', '厄', '厚', '厝', '原', '厥', '厭', '厲', '去', '參', '又', '叉', '及', '友', '反', '叔', '取', '受', '叛', '叟', '叢', '口', '古', '句', '另', '叨', '叩', '只', '叫', '召', '叭', '叮', '可', '台', '叱', '史', '右', '叵', '司', '叼', '吁', '吃', '各', '吆', '合', '吉', '吊', '吋', '同', '名', '后', '吏', '吐', '向', '吒', '君', '吝', '吞', '吟', '吠', '否', '吧', '吩', '含', '吭', '吮', '吱', '吳', '吵', '吶', '吸', '吹', '吻', '吼', '吾', '呀', '呂', '呃', '呆', '呈', '告', '呎', '呢', '周', '呱', '味', '呵', '呶', '呷', '呸', '呻', '呼', '命', '咀', '咄', '咆', '咋', '和', '咎', '咐', '咒', '咕', '咖', '咚', '咦', '咨', '咪', '咫', '咬', '咯', '咱', '咳', '咸', '咻', '咽', '哀', '品', '哂', '哄', '哇', '哈', '哉', '哎', '員', '哥', '哦', '哨', '哩', '哪', '哭', '哮', '哲', '哺', '哼', '唁', '唆', '唉', '唐', '唔', '唧', '唬', '售', '唯', '唱', '唳', '唷', '唸', '唾', '啃', '啄', '商', '啊', '問', '啕', '啖', '啜', '啞', '啟', '啡', '啣', '啤', '啦', '啪', '啻', '啼', '啾', '喀', '喂', '喃', '善', '喇', '喉', '喊', '喋', '喔', '喘', '喚', '喜', '喝', '喟', '喧', '喪', '喬', '單', '喱', '喲', '喳', '喻', '嗅', '嗆', '嗇', '嗎', '嗑', '嗓', '嗚', '嗜', '嗟', '嗡', '嗣', '嗤', '嗥', '嗦', '嗨', '嗯', '嗷', '嗽', '嗾', '嘀', '嘆', '嘈', '嘉', '嘍', '嘎', '嘔', '嘖', '嘗', '嘛', '嘟', '嘩', '嘮', '嘯', '嘰', '嘲', '嘴', '嘶', '嘹', '嘻', '嘿', '噎', '噓', '噗', '噙', '噢', '噤', '噥', '器', '噩', '噪', '噫', '噬', '噯', '噱', '噴', '噸', '噹', '嚀', '嚅', '嚇', '嚎', '嚏', '嚐', '嚕', '嚥', '嚨', '嚮', '嚴', '嚶', '嚷', '嚼', '囀', '囁', '囂', '囈', '囉', '囊', '囌', '囑', '囚', '四', '回', '因', '囤', '囪', '困', '固', '圃', '圈', '國', '圍', '園', '圓', '圖', '團', '土', '在', '圬', '圭', '圯', '地', '圳', '圾', '址', '均', '坊', '坍', '坎', '坏', '坐', '坑', '坡', '坤', '坦', '坩', '坪', '坷', '坼', '垂', '垃', '型', '垠', '垢', '垣', '垮', '埂', '埃', '埋', '城', '埔', '域', '埠', '埤', '執', '培', '基', '堂', '堅', '堆', '堊', '堡', '堤', '堪', '堯', '堰', '報', '場', '堵', '塊', '塌', '塑', '塔', '塗', '塘', '塚', '塞', '塢', '填', '塭', '塵', '塹', '塾', '墀', '境', '墅', '墊', '墓', '墜', '增', '墟', '墨', '墮', '墳', '墾', '壁', '壅', '壇', '壑', '壓', '壕', '壘', '壙', '壞', '壟', '壢', '壤', '壩', '士', '壬', '壯', '壹', '壺', '壽', '夏', '夔', '夕', '外', '夙', '多', '夜', '夠', '夢', '夤', '夥', '大', '天', '太', '夫', '夭', '央', '失', '夷', '夸', '夾', '奄', '奇', '奈', '奉', '奎', '奏', '奐', '契', '奔', '奕', '套', '奘', '奚', '奠', '奢', '奧', '奩', '奪', '奮', '女', '奴', '奶', '奸', '她', '好', '妁', '如', '妃', '妄', '妊', '妍', '妒', '妓', '妖', '妙', '妝', '妞', '妣', '妤', '妥', '妨', '妮', '妯', '妳', '妹', '妻', '妾', '姆', '姊', '始', '姍', '姐', '姑', '姒', '姓', '委', '姘', '姚', '姜', '姣', '姥', '姦', '姨', '姪', '姬', '姻', '姿', '威', '娃', '娌', '娑', '娓', '娘', '娛', '娜', '娟', '娠', '娣', '娥', '娩', '娶', '娼', '婀', '婁', '婆', '婉', '婊', '婚', '婢', '婦', '婪', '婷', '婿', '媒', '媚', '媛', '媲', '媳', '媼', '媽', '媾', '嫁', '嫂', '嫉', '嫌', '嫖', '嫗', '嫘', '嫡', '嫣', '嫦', '嫩', '嫵', '嫻', '嬉', '嬋', '嬌', '嬝', '嬤', '嬪', '嬰', '嬴', '嬸', '孀', '子', '孑', '孓', '孔', '孕', '字', '存', '孚', '孜', '孝', '孟', '季', '孤', '孩', '孫', '孰', '孱', '孳', '孵', '學', '孺', '孽', '孿', '它', '宅', '宇', '守', '安', '宋', '完', '宏', '宗', '官', '宙', '定', '宛', '宜', '客', '宣', '室', '宥', '宦', '宮', '宰', '害', '宴', '宵', '家', '宸', '容', '宿', '寂', '寄', '寅', '密', '寇', '富', '寐', '寒', '寓', '寞', '察', '寡', '寢', '寤', '寥', '實', '寧', '寨', '審', '寫', '寬', '寮', '寵', '寶', '寸', '寺', '封', '射', '將', '專', '尉', '尊', '尋', '對', '導', '小', '少', '尖', '尚', '尤', '尬', '就', '尷', '尸', '尹', '尺', '尼', '尾', '尿', '局', '屁', '居', '屆', '屈', '屋', '屍', '屎', '屏', '屐', '屑', '展', '屜', '屠', '屢', '層', '履', '屬', '屯', '山', '屹', '岌', '岐', '岑', '岔', '岡', '岩', '岫', '岱', '岳', '岷', '岸', '峙', '峨', '峪', '峭', '峰', '島', '峻', '峽', '崁', '崆', '崇', '崎', '崑', '崔', '崖', '崙', '崛', '崢', '崩', '嵌', '嵐', '嵩', '嶄', '嶇', '嶝', '嶺', '嶼', '嶽', '巍', '巒', '巔', '巖', '川', '州', '巡', '巢', '工', '左', '巧', '巨', '巫', '差', '己', '已', '巳', '巴', '巷', '巽', '巾', '市', '布', '帆', '希', '帑', '帕', '帖', '帘', '帚', '帛', '帝', '帥', '師', '席', '帳', '帶', '帷', '常', '帽', '幀', '幅', '幌', '幔', '幗', '幛', '幟', '幢', '幣', '幫', '干', '平', '年', '并', '幸', '幹', '幻', '幼', '幽', '幾', '庇', '床', '序', '底', '庖', '店', '庚', '府', '庠', '度', '座', '庫', '庭', '庵', '庶', '康', '庸', '庾', '廁', '廂', '廈', '廉', '廊', '廓', '廖', '廚', '廝', '廟', '廠', '廢', '廣', '廬', '廳', '延', '廷', '建', '廿', '弁', '弄', '弈', '弊', '式', '弒', '弓', '弔', '引', '弗', '弘', '弛', '弟', '弦', '弧', '弩', '弭', '弱', '張', '強', '弼', '彆', '彈', '彌', '彎', '彗', '彙', '形', '彤', '彥', '彩', '彪', '彫', '彬', '彭', '彰', '影', '彷', '役', '彼', '彿', '往', '征', '待', '徇', '很', '徊', '律', '後', '徐', '徑', '徒', '得', '徘', '徙', '從', '御', '徨', '復', '循', '徬', '微', '徵', '德', '徹', '徽', '心', '必', '忌', '忍', '忖', '志', '忘', '忙', '忝', '忠', '快', '忱', '念', '忽', '忿', '怎', '怏', '怒', '怔', '怕', '怖', '思', '怠', '怡', '急', '性', '怨', '怪', '怯', '怵', '恃', '恆', '恍', '恐', '恕', '恙', '恢', '恣', '恤', '恥', '恨', '恩', '恪', '恫', '恬', '恭', '息', '恰', '恿', '悄', '悅', '悉', '悌', '悍', '悔', '悖', '悚', '悟', '悠', '患', '您', '悲', '悴', '悵', '悶', '悸', '悻', '悼', '悽', '情', '惆', '惋', '惑', '惕', '惘', '惚', '惜', '惟', '惠', '惡', '惦', '惰', '惱', '想', '惴', '惶', '惹', '惺', '惻', '愀', '愁', '愈', '愉', '愎', '意', '愕', '愚', '愛', '愜', '感', '愣', '愧', '愴', '愾', '愿', '慄', '慇', '慈', '態', '慌', '慍', '慎', '慕', '慘', '慚', '慝', '慟', '慢', '慣', '慧', '慨', '慫', '慮', '慰', '慶', '慷', '慼', '慾', '憂', '憊', '憎', '憐', '憑', '憔', '憚', '憤', '憧', '憩', '憫', '憬', '憲', '憶', '憾', '懂', '懇', '懈', '應', '懊', '懍', '懣', '懦', '懲', '懵', '懶', '懷', '懸', '懺', '懼', '懾', '懿', '戀', '戈', '戊', '戌', '戍', '戎', '成', '我', '戒', '戕', '或', '戚', '戛', '戟', '戡', '戢', '截', '戮', '戰', '戲', '戳', '戴', '戶', '戾', '房', '所', '扁', '扇', '扈', '扉', '手', '才', '扎', '扒', '打', '扔', '托', '扛', '扣', '扭', '扮', '扯', '扳', '扶', '批', '扼', '找', '承', '技', '抄', '抉', '把', '抑', '抒', '抓', '投', '抖', '抗', '折', '抨', '披', '抬', '抱', '抵', '抹', '押', '抽', '抿', '拂', '拄', '拆', '拇', '拈', '拉', '拋', '拌', '拍', '拎', '拐', '拒', '拓', '拔', '拖', '拗', '拘', '拙', '拚', '招', '拜', '括', '拭', '拮', '拯', '拱', '拳', '拴', '拷', '拼', '拽', '拾', '拿', '持', '指', '挈', '按', '挑', '挖', '挨', '挪', '挫', '振', '挺', '挽', '挾', '捂', '捆', '捉', '捎', '捏', '捐', '捕', '捧', '捨', '捩', '捫', '捱', '捲', '捶', '捷', '捻', '掀', '掃', '掄', '授', '掉', '掌', '掏', '排', '掖', '掘', '掙', '掛', '掠', '採', '探', '掣', '接', '控', '推', '掩', '措', '掬', '揀', '揆', '揉', '揍', '描', '提', '插', '揖', '揚', '換', '握', '揣', '揩', '揪', '揭', '揮', '援', '損', '搏', '搓', '搔', '搖', '搗', '搜', '搞', '搪', '搬', '搭', '搶', '搽', '搾', '摑', '摒', '摔', '摘', '摟', '摧', '摩', '摯', '摸', '摹', '摺', '撇', '撈', '撐', '撒', '撓', '撕', '撚', '撞', '撤', '撥', '撩', '撫', '撬', '播', '撮', '撰', '撲', '撻', '撼', '撿', '擁', '擂', '擄', '擅', '擇', '擊', '擋', '操', '擎', '擒', '擔', '擘', '據', '擠', '擦', '擬', '擰', '擱', '擲', '擴', '擺', '擻', '擾', '攀', '攆', '攏', '攔', '攘', '攙', '攜', '攝', '攣', '攤', '攪', '攫', '攬', '支', '收', '改', '攻', '放', '政', '故', '效', '敏', '救', '敖', '敗', '敘', '教', '敝', '敞', '敢', '散', '敦', '敬', '敲', '整', '敵', '敷', '數', '斂', '斃', '文', '斐', '斑', '斗', '料', '斜', '斟', '斡', '斤', '斥', '斧', '斫', '斬', '斯', '新', '斷', '方', '於', '施', '旁', '旅', '旋', '旌', '旎', '族', '旖', '旗', '既', '日', '旦', '旨', '早', '旬', '旭', '旱', '旺', '昀', '昂', '昆', '昌', '明', '昏', '易', '昔', '星', '映', '春', '昧', '昨', '昭', '是', '時', '晃', '晉', '晌', '晏', '晒', '晚', '晝', '晤', '晦', '晨', '普', '景', '晰', '晴', '晶', '智', '暇', '暈', '暉', '暑', '暖', '暗', '暢', '暨', '暫', '暮', '暴', '暹', '曆', '曉', '曖', '曙', '曝', '曠', '曦', '曰', '曲', '曳', '更', '曷', '書', '曹', '曼', '曾', '替', '最', '會', '月', '有', '朋', '服', '朔', '朕', '朗', '望', '朝', '期', '朦', '朧', '木', '未', '末', '本', '札', '朮', '朱', '朴', '朵', '朽', '杉', '李', '杏', '材', '村', '杖', '杜', '杞', '束', '杭', '杯', '杰', '東', '杳', '杵', '杷', '松', '板', '枇', '枉', '枋', '析', '枕', '林', '枚', '果', '枝', '枯', '枴', '架', '枸', '柄', '柏', '某', '柑', '染', '柔', '柚', '柞', '查', '柩', '柬', '柯', '柱', '柳', '柴', '柵', '柿', '栓', '栗', '校', '栩', '株', '核', '根', '格', '栽', '桀', '桂', '桃', '桅', '框', '案', '桌', '桐', '桑', '桓', '桔', '桶', '桿', '梁', '梃', '梅', '梆', '梓', '梔', '梗', '條', '梟', '梢', '梧', '梨', '梭', '梯', '械', '梱', '梳', '梵', '棄', '棉', '棋', '棍', '棒', '棕', '棗', '棘', '棚', '棟', '棠', '棣', '棧', '森', '棲', '棵', '棹', '棺', '椅', '植', '椎', '椒', '椰', '楊', '楓', '楔', '楚', '楞', '楠', '楨', '楫', '業', '極', '楷', '楹', '概', '榆', '榔', '榕', '榛', '榜', '榨', '榫', '榭', '榮', '榴', '榷', '榻', '槁', '構', '槌', '槍', '槐', '槓', '槨', '槳', '槽', '樁', '樂', '樅', '樊', '樓', '標', '樞', '樟', '模', '樣', '樵', '樸', '樹', '樺', '樽', '橄', '橇', '橋', '橘', '橙', '機', '橡', '橢', '橫', '檀', '檄', '檔', '檜', '檢', '檬', '檳', '檸', '檻', '櫂', '櫃', '櫓', '櫚', '櫛', '櫝', '櫥', '櫻', '欄', '權', '欖', '欠', '次', '欣', '欲', '欺', '欽', '款', '歇', '歉', '歌', '歐', '歙', '歟', '歡', '止', '正', '此', '步', '武', '歧', '歪', '歲', '歷', '歸', '歹', '死', '歿', '殃', '殆', '殉', '殊', '殖', '殘', '殤', '殮', '殯', '殲', '段', '殷', '殺', '殼', '殿', '毀', '毅', '毆', '毋', '母', '每', '毒', '毓', '比', '毗', '毛', '毫', '毯', '毽', '氏', '氐', '民', '氓', '氖', '氛', '氟', '氣', '氤', '氦', '氧', '氨', '氫', '氮', '氯', '氳', '水', '永', '氾', '汀', '汁', '求', '汐', '汕', '汗', '汙', '汝', '汞', '江', '池', '汨', '汪', '汰', '汲', '決', '汽', '汾', '沁', '沃', '沅', '沈', '沉', '沌', '沐', '沒', '沖', '沙', '沛', '沫', '沮', '沱', '河', '沸', '油', '治', '沼', '沽', '沾', '沿', '況', '泄', '泅', '泉', '泊', '泌', '泓', '法', '泗', '泛', '泡', '波', '泣', '泥', '注', '泰', '泱', '泳', '洋', '洌', '洗', '洛', '洞', '津', '洪', '洱', '洲', '洶', '活', '洽', '派', '流', '浙', '浚', '浦', '浩', '浪', '浬', '浮', '浴', '海', '浸', '涇', '消', '涉', '涎', '涓', '涕', '涮', '涯', '液', '涵', '涸', '涼', '淄', '淅', '淆', '淇', '淋', '淌', '淑', '淒', '淘', '淙', '淚', '淞', '淡', '淤', '淨', '淪', '淫', '淮', '深', '淳', '淵', '混', '淹', '淺', '添', '清', '渙', '渚', '減', '渝', '渠', '渡', '渣', '渤', '渥', '渦', '測', '渭', '港', '渲', '渴', '游', '渺', '渾', '湃', '湊', '湍', '湔', '湖', '湘', '湛', '湧', '湮', '湯', '溉', '源', '準', '溘', '溜', '溝', '溢', '溥', '溪', '溫', '溯', '溶', '溺', '溼', '滂', '滄', '滅', '滇', '滋', '滌', '滑', '滓', '滔', '滬', '滯', '滲', '滴', '滾', '滿', '漁', '漂', '漆', '漏', '漓', '演', '漕', '漠', '漢', '漣', '漩', '漪', '漫', '漬', '漯', '漱', '漲', '漳', '漸', '漾', '漿', '潑', '潔', '潘', '潛', '潤', '潦', '潭', '潮', '潰', '潸', '潺', '潼', '澀', '澄', '澆', '澈', '澎', '澗', '澡', '澤', '澧', '澱', '澳', '澹', '激', '濁', '濂', '濃', '濘', '濛', '濟', '濠', '濡', '濤', '濫', '濬', '濯', '濱', '濺', '濾', '瀆', '瀉', '瀋', '瀏', '瀑', '瀕', '瀚', '瀛', '瀝', '瀟', '瀨', '瀰', '瀾', '灌', '灑', '灘', '灣', '灤', '火', '灰', '灶', '灸', '灼', '災', '炊', '炎', '炒', '炕', '炙', '炫', '炬', '炭', '炮', '炯', '炳', '炸', '為', '烈', '烊', '烏', '烘', '烙', '烤', '烹', '烽', '焉', '焊', '焙', '焚', '無', '焦', '焰', '然', '煉', '煌', '煎', '煙', '煜', '煞', '煤', '煥', '煦', '照', '煩', '煬', '煮', '煽', '熄', '熊', '熔', '熙', '熟', '熨', '熬', '熱', '熹', '熾', '燃', '燄', '燈', '燉', '燎', '燐', '燒', '燕', '燙', '燜', '營', '燥', '燦', '燧', '燬', '燭', '燮', '燴', '燻', '爆', '爍', '爐', '爛', '爨', '爪', '爬', '爭', '爰', '爵', '父', '爸', '爹', '爺', '爻', '爽', '爾', '牆', '片', '版', '牌', '牒', '牖', '牘', '牙', '牛', '牝', '牟', '牠', '牡', '牢', '牧', '物', '牯', '牲', '牴', '特', '牽', '犀', '犁', '犄', '犒', '犖', '犛', '犢', '犧', '犬', '犯', '狀', '狂', '狄', '狎', '狐', '狗', '狙', '狠', '狡', '狩', '狷', '狸', '狹', '狼', '狽', '猓', '猖', '猙', '猛', '猜', '猥', '猩', '猴', '猶', '猷', '猾', '猿', '獄', '獅', '獎', '獐', '獗', '獨', '獰', '獲', '獵', '獷', '獸', '獺', '獻', '玀', '玄', '率', '玉', '王', '玖', '玟', '玨', '玩', '玫', '玲', '玳', '玷', '玻', '珀', '珊', '珍', '珠', '班', '珮', '現', '球', '琅', '理', '琉', '琊', '琍', '琢', '琥', '琪', '琳', '琴', '琵', '琶', '琺', '琿', '瑁', '瑕', '瑙', '瑚', '瑛', '瑜', '瑞', '瑟', '瑣', '瑤', '瑩', '瑪', '瑯', '瑰', '璃', '璋', '璜', '璣', '璦', '璧', '璩', '環', '璽', '瓊', '瓏', '瓜', '瓠', '瓢', '瓣', '瓦', '瓶', '瓷', '甄', '甌', '甕', '甘', '甚', '甜', '生', '產', '甥', '甦', '用', '甩', '甫', '甬', '甭', '田', '由', '甲', '申', '男', '甸', '甽', '界', '畏', '畔', '留', '畚', '畜', '畝', '畢', '略', '畦', '番', '畫', '異', '當', '畸', '疆', '疇', '疊', '疋', '疏', '疑', '疙', '疚', '疝', '疤', '疥', '疫', '疲', '疳', '疵', '疹', '疼', '疽', '疾', '病', '症', '痊', '痔', '痕', '痘', '痙', '痛', '痞', '痢', '痣', '痰', '痱', '痲', '痴', '痺', '痿', '瘀', '瘁', '瘉', '瘋', '瘍', '瘓', '瘟', '瘠', '瘡', '瘤', '瘦', '瘧', '瘩', '瘴', '瘸', '療', '癆', '癌', '癒', '癖', '癘', '癢', '癥', '癩', '癬', '癮', '癱', '癲', '癸', '登', '發', '白', '百', '皂', '的', '皆', '皇', '皈', '皎', '皓', '皖', '皚', '皮', '皰', '皴', '皺', '皿', '盂', '盃', '盆', '盈', '益', '盍', '盎', '盒', '盔', '盛', '盜', '盞', '盟', '盡', '監', '盤', '盥', '盧', '盪', '目', '盯', '盲', '直', '相', '盹', '盼', '盾', '省', '眉', '看', '真', '眠', '眨', '眩', '眶', '眷', '眸', '眺', '眼', '眾', '睏', '睛', '睜', '睞', '睡', '督', '睥', '睦', '睨', '睪', '睫', '睬', '睹', '睽', '睿', '瞄', '瞇', '瞌', '瞎', '瞑', '瞞', '瞟', '瞠', '瞥', '瞧', '瞪', '瞬', '瞭', '瞰', '瞳', '瞻', '瞽', '瞿', '矇', '矓', '矗', '矚', '矛', '矜', '矢', '矣', '知', '矩', '短', '矮', '矯', '石', '矽', '砂', '砌', '砍', '研', '砝', '砥', '砧', '砭', '砰', '破', '砷', '砸', '硃', '硝', '硫', '硬', '硯', '硼', '碉', '碌', '碎', '碑', '碗', '碘', '碟', '碧', '碩', '碰', '碳', '確', '碼', '碾', '磁', '磅', '磊', '磋', '磐', '磕', '磚', '磨', '磬', '磯', '磴', '磷', '磺', '礁', '礎', '礙', '礦', '礪', '礫', '礬', '示', '社', '祀', '祁', '祆', '祇', '祈', '祉', '祐', '祕', '祖', '祗', '祚', '祝', '神', '祟', '祠', '祥', '票', '祭', '祺', '祿', '禁', '禍', '禎', '福', '禦', '禧', '禪', '禮', '禱', '禹', '禽', '禾', '禿', '秀', '私', '秉', '秋', '科', '秒', '租', '秣', '秤', '秦', '秧', '秩', '移', '稀', '稅', '稈', '程', '稍', '稔', '稚', '稜', '稟', '稠', '種', '稱', '稷', '稻', '稼', '稽', '稿', '穀', '穆', '穌', '積', '穎', '穗', '穡', '穢', '穩', '穫', '穴', '究', '穹', '空', '穿', '突', '窄', '窈', '窒', '窕', '窖', '窗', '窘', '窟', '窠', '窩', '窪', '窮', '窯', '窺', '竄', '竅', '竇', '竊', '立', '站', '竟', '章', '竣', '童', '竭', '端', '競', '竹', '竺', '竽', '竿', '笆', '笑', '笙', '笛', '笞', '笠', '符', '笨', '第', '筆', '等', '筋', '筍', '筏', '筐', '筒', '答', '策', '筠', '筵', '筷', '箋', '箏', '箔', '箕', '算', '箝', '管', '箭', '箱', '箴', '節', '篁', '範', '篆', '篇', '築', '篙', '篛', '篡', '篤', '篩', '篷', '篾', '簇', '簍', '簑', '簞', '簡', '簣', '簧', '簪', '簫', '簷', '簸', '簽', '簾', '簿', '籃', '籌', '籍', '籐', '籟', '籠', '籤', '籬', '籮', '籲', '米', '粉', '粒', '粗', '粟', '粥', '粱', '粳', '粵', '粹', '粽', '精', '糊', '糕', '糖', '糙', '糜', '糞', '糟', '糠', '糢', '糧', '糯', '糸', '系', '糾', '紀', '紂', '約', '紅', '紇', '紉', '紊', '紋', '納', '紐', '純', '紕', '紗', '紙', '級', '紛', '紜', '素', '紡', '索', '紫', '紮', '累', '細', '紳', '紹', '紼', '絀', '終', '絃', '組', '絆', '結', '絕', '絞', '絡', '絢', '給', '絨', '絮', '統', '絲', '絹', '綁', '綏', '綑', '經', '綜', '綞', '綠', '綢', '維', '綰', '綱', '網', '綴', '綵', '綸', '綺', '綻', '綽', '綾', '綿', '緇', '緊', '緒', '緘', '線', '緝', '緞', '締', '緣', '編', '緩', '緬', '緯', '練', '緻', '縈', '縊', '縑', '縛', '縣', '縫', '縮', '縱', '縲', '縷', '總', '績', '繁', '繃', '繅', '繆', '織', '繕', '繚', '繞', '繡', '繩', '繪', '繫', '繭', '繹', '繼', '繽', '纂', '續', '纏', '纓', '纖', '纜', '缶', '缸', '缺', '缽', '罄', '罈', '罐', '罔', '罕', '罟', '罩', '罪', '置', '罰', '署', '罵', '罷', '罹', '羅', '羈', '羊', '羋', '羌', '美', '羔', '羚', '羞', '群', '羨', '義', '羯', '羲', '羶', '羸', '羹', '羽', '羿', '翁', '翅', '翌', '翎', '習', '翔', '翕', '翟', '翠', '翡', '翩', '翰', '翱', '翳', '翹', '翻', '翼', '耀', '老', '考', '者', '耆', '而', '耍', '耐', '耒', '耕', '耗', '耘', '耙', '耜', '耳', '耶', '耽', '耿', '聆', '聊', '聖', '聘', '聚', '聞', '聯', '聰', '聱', '聲', '聳', '聶', '職', '聽', '聾', '聿', '肄', '肅', '肆', '肇', '肉', '肋', '肌', '肓', '肖', '肘', '肚', '肛', '肝', '股', '肢', '肥', '肩', '肪', '肫', '肯', '肱', '育', '肴', '肺', '胃', '胄', '背', '胎', '胖', '胚', '胛', '胞', '胡', '胤', '胥', '胭', '胰', '胱', '胳', '胴', '胸', '能', '脂', '脅', '脆', '脈', '脊', '脖', '脣', '脩', '脫', '脯', '脹', '脾', '腆', '腋', '腎', '腐', '腑', '腔', '腕', '腥', '腦', '腫', '腮', '腰', '腱', '腳', '腸', '腹', '腺', '腿', '膀', '膈', '膊', '膏', '膚', '膛', '膜', '膝', '膠', '膨', '膩', '膳', '膺', '膽', '膾', '膿', '臀', '臂', '臃', '臆', '臉', '臍', '臏', '臘', '臚', '臟', '臣', '臥', '臧', '臨', '自', '臬', '臭', '至', '致', '臺', '臻', '臼', '臾', '舀', '舂', '舅', '與', '興', '舉', '舊', '舌', '舍', '舐', '舒', '舔', '舛', '舜', '舞', '舟', '舢', '舨', '航', '舫', '般', '舵', '舶', '舷', '船', '艇', '艘', '艙', '艦', '艮', '良', '艱', '色', '艾', '芋', '芍', '芒', '芙', '芝', '芟', '芥', '芬', '芭', '花', '芳', '芹', '芻', '芽', '苑', '苒', '苓', '苔', '苗', '苛', '苜', '苞', '苟', '苣', '若', '苦', '苧', '英', '茁', '茂', '范', '茄', '茅', '茉', '茗', '茫', '茱', '茲', '茴', '茵', '茶', '茸', '茹', '荀', '草', '荊', '荏', '荐', '荒', '荔', '荷', '荸', '荻', '荼', '莉', '莊', '莎', '莒', '莓', '莖', '莘', '莞', '莠', '莢', '莫', '莽', '菁', '菅', '菊', '菌', '菜', '菠', '菩', '華', '菰', '菱', '菲', '菴', '菸', '菽', '萃', '萄', '萊', '萋', '萌', '萍', '萎', '萬', '萱', '萵', '萸', '萼', '落', '葉', '著', '葛', '葡', '董', '葦', '葩', '葫', '葬', '葵', '葷', '蒂', '蒐', '蒙', '蒜', '蒞', '蒲', '蒸', '蒼', '蒿', '蓀', '蓄', '蓆', '蓉', '蓋', '蓓', '蓬', '蓮', '蓿', '蔑', '蔓', '蔔', '蔗', '蔚', '蔡', '蔣', '蔥', '蔬', '蔭', '蔽', '蕃', '蕈', '蕉', '蕊', '蕙', '蕨', '蕩', '蕪', '蕭', '蕾', '薄', '薇', '薑', '薔', '薛', '薜', '薩', '薪', '薯', '薰', '藉', '藍', '藏', '藐', '藕', '藝', '藤', '藥', '藩', '藪', '藹', '藺', '藻', '蘆', '蘇', '蘊', '蘋', '蘑', '蘗', '蘚', '蘭', '蘸', '蘿', '虎', '虐', '虔', '處', '虛', '虜', '虞', '號', '虧', '虫', '虱', '虹', '蚊', '蚌', '蚓', '蚣', '蚤', '蚩', '蚪', '蚯', '蚱', '蚵', '蚶', '蛀', '蛄', '蛆', '蛇', '蛋', '蛔', '蛙', '蛛', '蛟', '蛤', '蛭', '蛹', '蛻', '蛾', '蜀', '蜂', '蜃', '蜇', '蜈', '蜓', '蜘', '蜜', '蜢', '蜥', '蜴', '蜻', '蜿', '蝌', '蝕', '蝗', '蝙', '蝠', '蝦', '蝨', '蝴', '蝶', '蝸', '螂', '螃', '融', '螞', '螟', '螢', '螫', '螳', '螺', '螻', '蟀', '蟆', '蟈', '蟋', '蟑', '蟒', '蟬', '蟯', '蟲', '蟹', '蟻', '蠅', '蠍', '蠔', '蠕', '蠟', '蠡', '蠢', '蠣', '蠱', '蠶', '蠹', '蠻', '血', '行', '衍', '術', '街', '衙', '衛', '衝', '衡', '衢', '衣', '表', '衫', '衰', '衷', '袁', '袂', '袈', '袋', '袍', '袒', '袖', '袞', '被', '袱', '裁', '裂', '裊', '裔', '裕', '裘', '裙', '補', '裝', '裟', '裡', '裨', '裳', '裴', '裸', '裹', '製', '褂', '複', '褐', '褒', '褓', '褚', '褥', '褪', '褫', '褲', '褶', '褸', '褻', '襄', '襖', '襟', '襠', '襤', '襪', '襯', '襲', '西', '要', '覃', '覆', '見', '規', '覓', '視', '覦', '親', '覬', '覲', '覺', '覽', '觀', '角', '解', '觴', '觸', '言', '訂', '訃', '計', '訊', '訌', '討', '訐', '訓', '訕', '訖', '託', '記', '訛', '訝', '訟', '訣', '訥', '訪', '設', '許', '訴', '診', '註', '証', '詁', '詆', '詐', '詔', '評', '詛', '詞', '詠', '詢', '詣', '試', '詩', '詫', '詬', '詭', '詮', '詰', '話', '該', '詳', '詹', '詼', '誅', '誇', '誌', '認', '誑', '誓', '誕', '誘', '語', '誠', '誡', '誣', '誤', '誥', '誦', '誨', '說', '誰', '課', '誼', '調', '諂', '諄', '談', '諉', '請', '諍', '諒', '論', '諜', '諦', '諧', '諫', '諭', '諮', '諱', '諷', '諸', '諺', '諾', '謀', '謁', '謂', '謄', '謊', '謎', '謗', '謙', '講', '謝', '謠', '謨', '謬', '謹', '譁', '證', '譎', '譏', '識', '譚', '譜', '警', '譬', '譯', '議', '譴', '護', '譽', '讀', '變', '讒', '讓', '讖', '讚', '谷', '谿', '豁', '豆', '豈', '豉', '豌', '豎', '豐', '豔', '豕', '豚', '象', '豢', '豪', '豫', '豬', '豹', '豺', '貂', '貉', '貊', '貌', '貍', '貓', '貝', '貞', '負', '財', '貢', '貧', '貨', '販', '貪', '貫', '責', '貯', '貲', '貳', '貴', '貶', '買', '貸', '費', '貼', '貽', '貿', '賀', '賁', '賂', '賃', '賄', '賅', '資', '賈', '賊', '賑', '賒', '賓', '賜', '賞', '賠', '賢', '賣', '賤', '賦', '質', '賬', '賭', '賴', '賺', '購', '賽', '贅', '贈', '贊', '贍', '贏', '贓', '贖', '贗', '贛', '赤', '赦', '赧', '赫', '赭', '走', '赳', '赴', '起', '趁', '超', '越', '趕', '趙', '趟', '趣', '趨', '足', '趴', '趾', '跆', '跋', '跌', '跎', '跑', '跚', '跛', '距', '跟', '跡', '跨', '跪', '路', '跳', '跺', '跼', '踏', '踐', '踝', '踟', '踢', '踩', '踫', '踱', '踴', '踵', '踹', '蹂', '蹄', '蹈', '蹉', '蹊', '蹋', '蹙', '蹣', '蹤', '蹦', '蹬', '蹲', '蹶', '蹺', '蹼', '躁', '躂', '躅', '躇', '躉', '躊', '躍', '躑', '躡', '躪', '身', '躬', '躲', '躺', '軀', '車', '軋', '軌', '軍', '軒', '軔', '軛', '軟', '軸', '軻', '軼', '軾', '較', '載', '輊', '輒', '輓', '輔', '輕', '輛', '輜', '輝', '輟', '輦', '輩', '輪', '輯', '輸', '輻', '輾', '輿', '轂', '轄', '轅', '轉', '轍', '轎', '轔', '轟', '轡', '辛', '辜', '辟', '辣', '辦', '辨', '辭', '辮', '辯', '辰', '辱', '農', '迂', '迄', '迅', '迆', '迎', '近', '返', '迢', '迥', '迦', '迪', '迫', '迭', '述', '迴', '迷', '迺', '追', '退', '送', '逃', '逅', '逆', '逍', '透', '逐', '途', '逕', '逖', '逗', '這', '通', '逛', '逝', '逞', '速', '造', '逢', '連', '逮', '週', '進', '逵', '逸', '逼', '逾', '遁', '遂', '遇', '遊', '運', '遍', '過', '遏', '遐', '遑', '道', '達', '違', '遘', '遙', '遜', '遞', '遠', '遣', '遨', '適', '遭', '遮', '遲', '遴', '遵', '遷', '選', '遺', '遼', '遽', '避', '邀', '邁', '邂', '還', '邇', '邊', '邏', '邐', '邑', '邕', '邢', '那', '邦', '邪', '邱', '邵', '邸', '郁', '郊', '郎', '郡', '部', '郭', '郵', '都', '鄂', '鄉', '鄒', '鄙', '鄧', '鄭', '鄰', '鄱', '鄹', '酉', '酊', '酋', '酌', '配', '酒', '酗', '酣', '酥', '酩', '酪', '酬', '酵', '酷', '酸', '醃', '醇', '醉', '醋', '醒', '醜', '醞', '醣', '醫', '醬', '醺', '釀', '釁', '采', '釉', '釋', '里', '重', '野', '量', '釐', '金', '釗', '釘', '釜', '針', '釣', '釦', '釧', '釵', '鈉', '鈍', '鈐', '鈔', '鈕', '鈞', '鈣', '鈴', '鈷', '鈸', '鈽', '鈾', '鉀', '鉋', '鉑', '鉗', '鉛', '鉤', '鉸', '鉻', '銀', '銅', '銓', '銖', '銘', '銜', '銬', '銳', '銷', '銻', '銼', '鋁', '鋅', '鋒', '鋤', '鋪', '鋸', '鋼', '錄', '錐', '錘', '錚', '錠', '錢', '錦', '錨', '錫', '錯', '錳', '錶', '鍊', '鍋', '鍍', '鍛', '鍥', '鍬', '鍰', '鍵', '鍾', '鎂', '鎊', '鎔', '鎖', '鎢', '鎮', '鎳', '鏃', '鏈', '鏍', '鏑', '鏖', '鏗', '鏘', '鏜', '鏝', '鏟', '鏡', '鏢', '鏤', '鏽', '鐃', '鐘', '鐮', '鐲', '鐳', '鐵', '鐸', '鐺', '鑄', '鑑', '鑒', '鑠', '鑣', '鑰', '鑲', '鑼', '鑽', '鑾', '鑿', '長', '門', '閂', '閃', '閉', '開', '閏', '閑', '閒', '間', '閔', '閘', '閡', '閣', '閤', '閥', '閨', '閩', '閭', '閱', '閻', '闆', '闈', '闊', '闋', '闌', '闐', '闔', '闖', '關', '闡', '闢', '阜', '阡', '阪', '阮', '阱', '防', '阻', '阿', '陀', '附', '陋', '陌', '降', '限', '陛', '陝', '陡', '院', '陣', '除', '陪', '陰', '陲', '陳', '陴', '陵', '陶', '陷', '陸', '陽', '隅', '隆', '隊', '隋', '隍', '階', '隔', '隕', '隘', '隙', '際', '障', '隧', '隨', '險', '隱', '隴', '隸', '隻', '雀', '雁', '雄', '雅', '集', '雇', '雉', '雋', '雌', '雍', '雕', '雖', '雙', '雛', '雜', '雞', '離', '難', '雨', '雪', '雯', '雲', '零', '雷', '雹', '電', '需', '霄', '霆', '震', '霉', '霍', '霎', '霏', '霑', '霓', '霖', '霜', '霞', '霧', '霪', '露', '霸', '霹', '霽', '霾', '靂', '靄', '靈', '青', '靖', '靛', '靜', '非', '靠', '靡', '面', '靦', '靨', '革', '靴', '靶', '靼', '鞅', '鞋', '鞍', '鞏', '鞘', '鞠', '鞣', '鞦', '鞭', '韁', '韃', '韆', '韋', '韌', '韓', '韜', '韭', '音', '韶', '韻', '響', '頁', '頂', '頃', '項', '順', '須', '頊', '頌', '預', '頑', '頒', '頓', '頗', '領', '頡', '頤', '頭', '頰', '頷', '頸', '頹', '頻', '顆', '題', '額', '顎', '顏', '顓', '願', '顛', '類', '顧', '顫', '顯', '顰', '顱', '風', '颯', '颱', '颳', '颶', '颺', '颼', '飄', '飛', '食', '飢', '飧', '飩', '飪', '飭', '飯', '飲', '飴', '飼', '飽', '飾', '餃', '餅', '餉', '養', '餌', '餐', '餒', '餓', '餘', '餛', '餞', '餡', '館', '餵', '餽', '餾', '餿', '饅', '饑', '饒', '饜', '饞', '首', '香', '馥', '馨', '馬', '馭', '馮', '馱', '馳', '馴', '駁', '駐', '駑', '駒', '駕', '駙', '駛', '駝', '駟', '駢', '駭', '駱', '駿', '騁', '騎', '騖', '騙', '騫', '騰', '騷', '騾', '驀', '驃', '驅', '驕', '驗', '驚', '驛', '驟', '驢', '驥', '驪', '骨', '骯', '骰', '骷', '骸', '骼', '髏', '髒', '髓', '體', '高', '髦', '髭', '髮', '髯', '髻', '鬃', '鬆', '鬍', '鬚', '鬢', '鬥', '鬧', '鬨', '鬱', '鬲', '鬼', '魁', '魂', '魄', '魅', '魏', '魔', '魘', '魚', '魯', '魷', '鮑', '鮪', '鮫', '鮮', '鯉', '鯊', '鯧', '鯨', '鯽', '鰍', '鰓', '鰥', '鰭', '鰱', '鰻', '鰾', '鱉', '鱔', '鱖', '鱗', '鱷', '鱸', '鳥', '鳩', '鳳', '鳴', '鳶', '鴆', '鴉', '鴒', '鴕', '鴛', '鴣', '鴦', '鴨', '鴻', '鴿', '鵑', '鵝', '鵠', '鵡', '鵪', '鵬', '鵲', '鶉', '鶯', '鶴', '鷂', '鷓', '鷗', '鷥', '鷹', '鷺', '鸚', '鸞', '鹹', '鹼', '鹽', '鹿', '麂', '麋', '麒', '麓', '麗', '麝', '麟', '麥', '麩', '麴', '麵', '麻', '麼', '麾', '黃', '黍', '黎', '黏', '黑', '黔', '默', '黛', '黜', '黝', '點', '黠', '黨', '黯', '黴', '黷', '鼇', '鼎', '鼓', '鼕', '鼙', '鼠', '鼬', '鼴', '鼻', '鼾', '齊', '齋', '齒', '齜', '齟', '齡', '齣', '齦', '齪', '齬', '齲', '齷', '龍', '龐', '龔']\n",
      "\n",
      "Starting Optuna hyperparameter optimization...\n",
      "This will perform multiple trials to find the best parameter combination\n",
      "Will perform 5 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optuna Trials:   0%|          | 0/5 [00:00<?, ?trial/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Optuna Training:   0%|          | 0/3 [03:43<?, ?epoch/s]\n",
      "[W 2025-06-10 19:32:45,047] Trial 0 failed with parameters: {'lr': 0.0001329291894316216, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0029154431891537554, 'scheduler': 'ExponentialLR', 'gamma': 0.9364594334728974, 'dropout_rate': 0.4329770563201687} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yahui\\AppData\\Local\\Temp\\ipykernel_28072\\2902343861.py\", line 237, in objective\n",
      "    best_acc = train_model_optuna(model, criterion, optimizer, scheduler,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yahui\\AppData\\Local\\Temp\\ipykernel_28072\\2902343861.py\", line 136, in train_model_optuna\n",
      "    for inputs, labels in train_pbar:\n",
      "                          ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\tqdm\\std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "               ^^^^^^^^\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 733, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 789, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"C:\\Users\\yahui\\AppData\\Local\\Temp\\ipykernel_28072\\2902343861.py\", line 95, in __getitem__\n",
      "    image = self.transform(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 95, in __call__\n",
      "    img = t(img)\n",
      "          ^^^^^^\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 1279, in forward\n",
      "    elif fn_id == 3 and hue_factor is not None:\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-06-10 19:32:45,068] Trial 0 failed with value None.\n",
      "Optuna Trials:   0%|          | 0/5 [03:44<?, ?trial/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 808\u001b[39m\n\u001b[32m    805\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Accuracy improvement: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary[\u001b[33m'\u001b[39m\u001b[33moptimization_summary\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mimprovement\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 648\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    642\u001b[39m         pbar.update(\u001b[32m1\u001b[39m)\n\u001b[32m    643\u001b[39m         pbar.set_postfix({\n\u001b[32m    644\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mbest_acc\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m study.best_value \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    645\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mtrial\u001b[39m\u001b[33m'\u001b[39m: trial.number + \u001b[32m1\u001b[39m\n\u001b[32m    646\u001b[39m         })\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[38;5;66;03m# Show best results\u001b[39;00m\n\u001b[32m    651\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOptuna optimization complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 237\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Simplified training (reduced epochs to 3)\u001b[39;00m\n\u001b[32m    236\u001b[39m num_epochs_optuna = \u001b[32m3\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m best_acc = \u001b[43mtrain_model_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mnum_epochs_optuna\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_acc\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mtrain_model_optuna\u001b[39m\u001b[34m(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Add batch progress bar\u001b[39;00m\n\u001b[32m    133\u001b[39m train_pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Train\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m    134\u001b[39m                  leave=\u001b[38;5;28;01mFalse\u001b[39;00m, unit=\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_pbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mloaddata.<locals>.DatasetWrapper.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     93\u001b[39m image = transforms.ToPILImage()(image)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yahui\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\loan-risk-predictor-hBt6Gynv-py3.12\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:1279\u001b[39m, in \u001b[36mColorJitter.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m saturation_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m         img = F.adjust_saturation(img, saturation_factor)\n\u001b[32m-> \u001b[39m\u001b[32m1279\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1280\u001b[39m         img = F.adjust_hue(img, hue_factor)\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # 添加進度條庫\n",
    "\n",
    "# Step 1: Setup parameters and check CUDA\n",
    "def check_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        current_device = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(current_device)\n",
    "        print(f\"CUDA available, using GPU training\")\n",
    "        print(f\"Current GPU name: {device_name}\")\n",
    "        print(f\"GPU count: {device_count}\")\n",
    "        print(f\"Currently using GPU: {device_name}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU training\")\n",
    "        return False\n",
    "\n",
    "use_gpu = check_cuda()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set local path\n",
    "data_dir = 'Handwritten_Data' # Your data path\n",
    "input_size = 224\n",
    "\n",
    "# Set CUDA optimization parameters\n",
    "if use_gpu:\n",
    "    torch.backends.cudnn.benchmark = True  # Accelerate convolution operations\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Global variables for data loading\n",
    "train_loader_global = None\n",
    "val_loader_global = None\n",
    "train_size_global = None\n",
    "val_size_global = None\n",
    "class_num_global = None\n",
    "class_names_global = None\n",
    "\n",
    "# Step 2: Define data loading functions\n",
    "def loaddata(data_dir, batch_size, shuffle=True):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # Original dataset without transformation\n",
    "    raw_dataset = datasets.ImageFolder(root=data_dir, transform=transforms.ToTensor())\n",
    "\n",
    "    train_size = int(0.8 * len(raw_dataset))\n",
    "    val_size = len(raw_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(raw_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    class DatasetWrapper(torch.utils.data.Dataset):\n",
    "        def __init__(self, subset, transform):\n",
    "            self.subset = subset\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.subset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image, label = self.subset.dataset[self.subset.indices[idx]]\n",
    "            image = transforms.ToPILImage()(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "    train_dataset_wrapped = DatasetWrapper(train_dataset, data_transforms['train'])\n",
    "    val_dataset_wrapped = DatasetWrapper(val_dataset, data_transforms['val'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset_wrapped, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=use_gpu)\n",
    "    val_loader = DataLoader(val_dataset_wrapped, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=use_gpu)\n",
    "\n",
    "    return train_loader, val_loader, train_size, val_size, len(raw_dataset.classes), raw_dataset.classes\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr_scheduler(optimizer, scheduler_type='StepLR', step_size=2, gamma=0.5):\n",
    "    \"\"\"Create learning rate scheduler\"\"\"\n",
    "    if scheduler_type == 'StepLR':\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    elif scheduler_type == 'ExponentialLR':\n",
    "        return optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "    elif scheduler_type == 'CosineAnnealingLR':\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    else:\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training function for Optuna optimization\n",
    "def train_model_optuna(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs=3):\n",
    "    \"\"\"Optimized training function, returns best validation accuracy (reduced epochs to 3)\"\"\"\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # Add epoch progress bar\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Optuna Training\", unit=\"epoch\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Add batch progress bar\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\", \n",
    "                         leave=False, unit=\"batch\")\n",
    "        \n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\", \n",
    "                       leave=False, unit=\"batch\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        epoch_acc = val_running_corrects.double() / val_size\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'best_acc': f'{best_acc:.4f}',\n",
    "            'current_acc': f'{epoch_acc:.4f}'\n",
    "        })\n",
    "\n",
    "    return best_acc.item()\n",
    "\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function\"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128] if use_gpu else [8, 16, 32])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    scheduler_type = trial.suggest_categorical('scheduler', ['StepLR', 'ExponentialLR', 'CosineAnnealingLR'])\n",
    "    \n",
    "    if scheduler_type == 'StepLR':\n",
    "        step_size = trial.suggest_int('step_size', 2, 8)\n",
    "        gamma = trial.suggest_float('gamma', 0.1, 0.9)\n",
    "    else:\n",
    "        step_size = 2\n",
    "        gamma = trial.suggest_float('gamma', 0.5, 0.95)\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    \n",
    "    # Reload data (using new batch_size)\n",
    "    train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(data_dir, batch_size)\n",
    "    \n",
    "    # Build model\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    num_ftrs = model._fc.in_features\n",
    "    \n",
    "    # Add Dropout\n",
    "    model._fc = nn.Sequential(\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(num_ftrs, class_num)\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:  # SGD\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    \n",
    "    # Set learning rate scheduler\n",
    "    scheduler = get_lr_scheduler(optimizer, scheduler_type, step_size, gamma)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Simplified training (reduced epochs to 3)\n",
    "    num_epochs_optuna = 3\n",
    "    best_acc = train_model_optuna(model, criterion, optimizer, scheduler, \n",
    "                                 train_loader, val_loader, train_size, val_size, \n",
    "                                 num_epochs_optuna)\n",
    "    \n",
    "    return best_acc\n",
    "\n",
    "# Training function with full metrics\n",
    "def train_model_full(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs=8):\n",
    "    \"\"\"Complete training function with all metrics recording (reduced epochs to 8)\"\"\"\n",
    "    since = time.time()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Recording lists\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    train_precisions, val_precisions = [], []\n",
    "    train_recalls, val_recalls = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "\n",
    "    print(f\"Starting training, using device: {device}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Main epoch progress bar\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Full Training\", unit=\"epoch\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 40)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current learning rate: {current_lr:.6f}')\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "\n",
    "        # Training batch progress bar\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False, unit=\"batch\")\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            all_train_preds.append(preds.cpu())\n",
    "            all_train_labels.append(labels.cpu())\n",
    "\n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{(running_corrects.double() / ((i+1) * inputs.size(0))):.4f}'\n",
    "            })\n",
    "\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc.cpu().item())\n",
    "\n",
    "        # Calculate training Precision/Recall/F1\n",
    "        train_preds = torch.cat(all_train_preds)\n",
    "        train_labels = torch.cat(all_train_labels)\n",
    "        train_precisions.append(precision_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "        train_recalls.append(recall_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "        train_f1s.append(f1_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "\n",
    "        # Validation batch progress bar\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\", leave=False, unit=\"batch\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                all_val_preds.append(preds.cpu())\n",
    "                all_val_labels.append(labels.cpu())\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        epoch_loss = running_loss / val_size\n",
    "        epoch_acc = running_corrects.double() / val_size\n",
    "        print(f'Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        val_losses.append(epoch_loss)\n",
    "        val_accs.append(epoch_acc.cpu().item())\n",
    "\n",
    "        # Calculate validation Precision/Recall/F1\n",
    "        val_preds = torch.cat(all_val_preds)\n",
    "        val_labels = torch.cat(all_val_labels)\n",
    "        val_precisions.append(precision_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        val_recalls.append(recall_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        val_f1s.append(f1_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            \n",
    "            os.makedirs('./best', exist_ok=True)\n",
    "            checkpoint_path = os.path.join(data_dir, 'best_model_checkpoint.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': epoch_loss,\n",
    "                'acc': epoch_acc,\n",
    "            }, checkpoint_path)\n",
    "            print(f'New best model saved, accuracy: {best_acc:.4f}')\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'best_acc': f'{best_acc:.4f}',\n",
    "            'current_acc': f'{epoch_acc:.4f}',\n",
    "            'loss': f'{epoch_loss:.4f}'\n",
    "        })\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best validation accuracy: {best_acc:.4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Save model\n",
    "    save_dir = os.path.join(data_dir, 'model')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs('./best', exist_ok=True)\n",
    "    model_path = os.path.join('./best', 'best.pth')\n",
    "    torch.save(model, model_path)\n",
    "    print(f'Final model saved to: {model_path}')\n",
    "\n",
    "    # Plot training curves\n",
    "    plot_training_curves(train_losses, val_losses, train_accs, val_accs,\n",
    "                        train_precisions, val_precisions, train_recalls, val_recalls,\n",
    "                        train_f1s, val_f1s, num_epochs, save_dir)\n",
    "\n",
    "    return model, best_acc\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, train_accs, val_accs,\n",
    "                        train_precisions, val_precisions, train_recalls, val_recalls,\n",
    "                        train_f1s, val_f1s, num_epochs, save_dir):\n",
    "    \"\"\"Plot training curves (all in English)\"\"\"\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, num_epochs+1), train_accs, 'b-', label='Train Accuracy')\n",
    "    plt.plot(range(1, num_epochs+1), val_accs, 'r-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, num_epochs+1), train_precisions, 'b--', label='Train Precision')\n",
    "    plt.plot(range(1, num_epochs+1), val_precisions, 'r--', label='Val Precision')\n",
    "    plt.plot(range(1, num_epochs+1), train_recalls, 'b:', label='Train Recall')\n",
    "    plt.plot(range(1, num_epochs+1), val_recalls, 'r:', label='Val Recall')\n",
    "    plt.title('Precision and Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, num_epochs+1), train_f1s, 'b-', label='Train F1-Score')\n",
    "    plt.plot(range(1, num_epochs+1), val_f1s, 'r-', label='Val F1-Score')\n",
    "    plt.title('F1-Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    curve_path = os.path.join(save_dir, 'training_curves_all_metrics.png')\n",
    "    plt.savefig(curve_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Training curves (including F1, Precision, Recall) saved to: {curve_path}')\n",
    "\n",
    "def plot_optuna_results(study):\n",
    "    \"\"\"Plot Optuna optimization results (all in English)\"\"\"\n",
    "    print(\"\\nPlotting Optuna optimization results...\")\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Optimization history\n",
    "    try:\n",
    "        optuna.visualization.matplotlib.plot_optimization_history(study, ax=axes[0,0])\n",
    "        axes[0,0].set_title('Optimization History')\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot plot optimization history: {e}\")\n",
    "        axes[0,0].text(0.5, 0.5, 'Optimization History\\nNot Available', ha='center', va='center')\n",
    "    \n",
    "    # 2. Parameter importance\n",
    "    try:\n",
    "        optuna.visualization.matplotlib.plot_param_importances(study, ax=axes[0,1])\n",
    "        axes[0,1].set_title('Parameter Importance')\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot plot parameter importance: {e}\")\n",
    "        axes[0,1].text(0.5, 0.5, 'Parameter Importance\\nNot Available', ha='center', va='center')\n",
    "    \n",
    "    # 3. Trial value distribution\n",
    "    values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "    if values:\n",
    "        axes[1,0].hist(values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[1,0].set_title('Trial Value Distribution')\n",
    "        axes[1,0].set_xlabel('Accuracy')\n",
    "        axes[1,0].set_ylabel('Frequency')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Best parameters table\n",
    "    axes[1,1].axis('off')\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "    \n",
    "    param_text = f\"Best Accuracy: {best_value:.4f}\\n\\nBest Parameters:\\n\"\n",
    "    for key, value in best_params.items():\n",
    "        param_text += f\"{key}: {value}\\n\"\n",
    "    \n",
    "    axes[1,1].text(0.1, 0.9, param_text, transform=axes[1,1].transAxes, \n",
    "                   fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    axes[1,1].set_title('Best Parameters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save chart\n",
    "    optuna_results_path = os.path.join(data_dir, 'optuna_optimization_results.png')\n",
    "    plt.savefig(optuna_results_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Optuna optimization results saved to: {optuna_results_path}')\n",
    "    \n",
    "    # Create detailed parameter comparison chart\n",
    "    plot_parameter_comparison(study)\n",
    "\n",
    "def plot_parameter_comparison(study):\n",
    "    \"\"\"Plot parameter comparison chart (all in English)\"\"\"\n",
    "    print(\"Plotting parameter comparison chart...\")\n",
    "    \n",
    "    df = study.trials_dataframe()\n",
    "    if len(df) == 0:\n",
    "        print(\"No trial data available for plotting\")\n",
    "        return\n",
    "    \n",
    "    # Extract numeric parameters\n",
    "    numeric_params = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith('params_') and df[col].dtype in ['float64', 'int64']:\n",
    "            numeric_params.append(col)\n",
    "    \n",
    "    if len(numeric_params) >= 2:\n",
    "        n_params = len(numeric_params)\n",
    "        n_cols = min(3, n_params)\n",
    "        n_rows = (n_params + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        elif n_cols == 1:\n",
    "            axes = [[ax] for ax in axes]\n",
    "        \n",
    "        for i, param in enumerate(numeric_params):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            ax = axes[row][col] if n_rows > 1 else axes[col]\n",
    "            \n",
    "            # Scatter plot: parameter value vs accuracy\n",
    "            x = df[param].values\n",
    "            y = df['value'].values\n",
    "            \n",
    "            # Remove NaN values\n",
    "            mask = ~(pd.isna(x) | pd.isna(y))\n",
    "            x = x[mask]\n",
    "            y = y[mask]\n",
    "            \n",
    "            if len(x) > 0:\n",
    "                ax.scatter(x, y, alpha=0.6)\n",
    "                ax.set_xlabel(param.replace('params_', ''))\n",
    "                ax.set_ylabel('Accuracy')\n",
    "                ax.set_title(f'{param.replace(\"params_\", \"\")} vs Accuracy')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide extra subplots\n",
    "        for i in range(n_params, n_rows * n_cols):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            if n_rows > 1:\n",
    "                axes[row][col].set_visible(False)\n",
    "            elif n_cols > 1:\n",
    "                axes[col].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        param_comp_path = os.path.join(data_dir, 'parameter_comparison.png')\n",
    "        plt.savefig(param_comp_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f'Parameter comparison chart saved to: {param_comp_path}')\n",
    "\n",
    "def test_samples(model, val_loader, class_names, device, num_samples=10):\n",
    "    \"\"\"Test several samples\"\"\"\n",
    "    model.eval()\n",
    "    samples_tested = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    print(f\"\\nTesting {num_samples} samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(min(len(preds), num_samples - samples_tested)):\n",
    "                pred_class = class_names[preds[i]]\n",
    "                true_class = class_names[labels[i]]\n",
    "                is_correct = preds[i] == labels[i]\n",
    "                \n",
    "                status = \"✓\" if is_correct else \"✗\"\n",
    "                print(f\"{status} Predicted: {pred_class} | Actual: {true_class}\")\n",
    "                \n",
    "                if is_correct:\n",
    "                    correct_predictions += 1\n",
    "                samples_tested += 1\n",
    "                \n",
    "                if samples_tested >= num_samples:\n",
    "                    break\n",
    "            \n",
    "            if samples_tested >= num_samples:\n",
    "                break\n",
    "    \n",
    "    accuracy = correct_predictions / samples_tested\n",
    "    print(f\"\\nTest sample accuracy: {accuracy:.4f} ({correct_predictions}/{samples_tested})\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main program\"\"\"\n",
    "\n",
    "    subsample_rate = 0.05\n",
    "    print(\"Starting handwritten character recognition training - Using Optuna optimization\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Check data path\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Data folder does not exist: {data_dir}\")\n",
    "        print(\"Please confirm if the data folder path is correct\")\n",
    "        return\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        # Load data for optimization\n",
    "        train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(data_dir, 64)\n",
    "        print(f\"Data loaded successfully\")\n",
    "        print(f\"   Training samples: {train_size}\")\n",
    "        print(f\"   Validation samples: {val_size}\")\n",
    "        print(f\"   Character classes: {class_num}\")\n",
    "        print(f\"   Class names: {class_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Data loading failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # === Optuna hyperparameter optimization ===\n",
    "    print(f\"\\nStarting Optuna hyperparameter optimization...\")\n",
    "    print(\"This will perform multiple trials to find the best parameter combination\")\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize', \n",
    "                               study_name='handwriting_optimization',\n",
    "                               sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    \n",
    "    # Execute optimization\n",
    "    n_trials = 5  # Reduced number of trials for faster execution\n",
    "    print(f\"Will perform {n_trials} trials...\")\n",
    "    \n",
    "    # Add progress bar for Optuna trials\n",
    "    with tqdm(total=n_trials, desc=\"Optuna Trials\", unit=\"trial\") as pbar:\n",
    "        def callback(study, trial):\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                'best_acc': f'{study.best_value:.4f}' if study.best_value else 'N/A',\n",
    "                'trial': trial.number + 1\n",
    "            })\n",
    "        \n",
    "        study.optimize(objective, n_trials=n_trials, callbacks=[callback])\n",
    "    \n",
    "    # Show best results\n",
    "    print(f\"\\nOptuna optimization complete!\")\n",
    "    print(f\"Best accuracy: {study.best_value:.4f}\")\n",
    "    print(f\"Best parameters: {study.best_params}\")\n",
    "    \n",
    "    # Plot optimization results\n",
    "    plot_optuna_results(study)\n",
    "    \n",
    "    # === Use best parameters for complete training ===\n",
    "    print(f\"\\nUsing best parameters for complete training...\")\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Rebuild model and optimizer\n",
    "    train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(\n",
    "        data_dir, best_params['batch_size'])\n",
    "    \n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    num_ftrs = model._fc.in_features\n",
    "    model._fc = nn.Sequential(\n",
    "        nn.Dropout(best_params['dropout_rate']),\n",
    "        nn.Linear(num_ftrs, class_num)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set optimizer\n",
    "    if best_params['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), \n",
    "                              lr=best_params['lr'], \n",
    "                              weight_decay=best_params['weight_decay'])\n",
    "    elif best_params['optimizer'] == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), \n",
    "                               lr=best_params['lr'], \n",
    "                               weight_decay=best_params['weight_decay'])\n",
    "    else:  # SGD\n",
    "        optimizer = optim.SGD(model.parameters(), \n",
    "                             lr=best_params['lr'], \n",
    "                             momentum=0.9, \n",
    "                             weight_decay=best_params['weight_decay'])\n",
    "    \n",
    "    # Set learning rate scheduler\n",
    "    if best_params['scheduler'] == 'StepLR':\n",
    "        scheduler = get_lr_scheduler(optimizer, 'StepLR', \n",
    "                                   best_params.get('step_size', 2), \n",
    "                                   best_params['gamma'])\n",
    "    else:\n",
    "        scheduler = get_lr_scheduler(optimizer, best_params['scheduler'], \n",
    "                                   gamma=best_params['gamma'])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(f\"Using best parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Complete training\n",
    "    num_epochs_final = 5  # Reduced from 15 to 10 epochs for final training\n",
    "    try:\n",
    "        model, best_acc = train_model_full(\n",
    "            model, criterion, optimizer, scheduler,\n",
    "            train_loader, val_loader,\n",
    "            train_size, val_size,\n",
    "            num_epochs=num_epochs_final\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🎉 Complete training finished! Final accuracy: {best_acc:.4f}\")\n",
    "        \n",
    "        # Test several samples\n",
    "        print(\"\\nTesting several samples...\")\n",
    "        test_samples(model, val_loader, class_names, device)\n",
    "        \n",
    "        # Save best parameters to file\n",
    "        import json\n",
    "        params_file = os.path.join(data_dir, 'best_hyperparameters.json')\n",
    "        with open(params_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Best hyperparameters saved to: {params_file}\")\n",
    "        \n",
    "        # Create optimization summary report\n",
    "        create_optimization_summary(study, best_acc, data_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during complete training: {e}\")\n",
    "        return\n",
    "\n",
    "def create_optimization_summary(study, final_accuracy, save_dir):\n",
    "    \"\"\"Create optimization summary report (all in English)\"\"\"\n",
    "    print(\"\\nCreating optimization summary report...\")\n",
    "    \n",
    "    # Collect statistical information\n",
    "    trials_df = study.trials_dataframe()\n",
    "    completed_trials = trials_df[trials_df['state'] == 'COMPLETE']\n",
    "    \n",
    "    if len(completed_trials) == 0:\n",
    "        print(\"No completed trials to analyze\")\n",
    "        return\n",
    "    \n",
    "    summary = {\n",
    "        'optimization_summary': {\n",
    "            'total_trials': len(study.trials),\n",
    "            'completed_trials': len(completed_trials),\n",
    "            'best_trial_number': study.best_trial.number,\n",
    "            'best_validation_accuracy': study.best_value,\n",
    "            'final_training_accuracy': final_accuracy,\n",
    "            'improvement': final_accuracy - study.best_value if study.best_value else 0\n",
    "        },\n",
    "        'best_parameters': study.best_params,\n",
    "        'statistics': {\n",
    "            'mean_accuracy': float(completed_trials['value'].mean()),\n",
    "            'std_accuracy': float(completed_trials['value'].std()),\n",
    "            'min_accuracy': float(completed_trials['value'].min()),\n",
    "            'max_accuracy': float(completed_trials['value'].max())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save summary to JSON\n",
    "    import json\n",
    "    summary_file = os.path.join(save_dir, 'optimization_summary.json')\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Create text report\n",
    "    report_file = os.path.join(save_dir, 'optimization_report.txt')\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(\"Handwritten Character Recognition - Optuna Optimization Report\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Optimization Summary:\\n\")\n",
    "        f.write(f\"  Total trials: {summary['optimization_summary']['total_trials']}\\n\")\n",
    "        f.write(f\"  Completed trials: {summary['optimization_summary']['completed_trials']}\\n\")\n",
    "        f.write(f\"  Best trial number: {summary['optimization_summary']['best_trial_number']}\\n\")\n",
    "        f.write(f\"  Best validation accuracy: {summary['optimization_summary']['best_validation_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Final training accuracy: {summary['optimization_summary']['final_training_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Accuracy improvement: {summary['optimization_summary']['improvement']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Best Parameters:\\n\")\n",
    "        for key, value in summary['best_parameters'].items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"Trial Statistics:\\n\")\n",
    "        f.write(f\"  Mean accuracy: {summary['statistics']['mean_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Accuracy standard deviation: {summary['statistics']['std_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Minimum accuracy: {summary['statistics']['min_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Maximum accuracy: {summary['statistics']['max_accuracy']:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Optimization summary saved:\")\n",
    "    print(f\"  JSON format: {summary_file}\")\n",
    "    print(f\"  Text report: {report_file}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nOptimization Summary:\")\n",
    "    print(f\"   Total trials: {summary['optimization_summary']['total_trials']}\")\n",
    "    print(f\"   Best validation accuracy: {summary['optimization_summary']['best_validation_accuracy']:.4f}\")\n",
    "    print(f\"   Final training accuracy: {summary['optimization_summary']['final_training_accuracy']:.4f}\")\n",
    "    print(f\"   Accuracy improvement: {summary['optimization_summary']['improvement']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb5279b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8790abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available, using GPU training\n",
      "Current GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU count: 1\n",
      "Currently using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA version: 11.8\n",
      "PyTorch version: 2.7.1+cu118\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Setup parameters and check CUDA\n",
    "def check_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        current_device = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(current_device)\n",
    "        print(f\"CUDA available, using GPU training\")\n",
    "        print(f\"Current GPU name: {device_name}\")\n",
    "        print(f\"GPU count: {device_count}\")\n",
    "        print(f\"Currently using GPU: {device_name}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU training\")\n",
    "        return False\n",
    "\n",
    "use_gpu = check_cuda()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30cbfc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available, using GPU training\n",
      "Current GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU count: 1\n",
      "Currently using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA version: 11.8\n",
      "PyTorch version: 2.7.1+cu118\n",
      "Using device: cuda\n",
      "Starting handwritten character recognition training - Using Optuna optimization\n",
      "======================================================================\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 00:34:08,389] A new study created in memory with name: handwriting_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056 5014 4803\n",
      "Data loaded successfully\n",
      "   Training samples: 20056\n",
      "   Validation samples: 5014\n",
      "   Character classes: 4803\n",
      "   Class names: ['丁', '七', '丈', '三', '上', '下', '不', '丐', '丑', '且', '丕', '世', '丘', '丙', '丞', '丟', '並', '丫', '中', '串', '丸', '丹', '主', '乃', '久', '么', '之', '乍', '乎', '乏', '乒', '乓', '乖', '乘', '乙', '九', '乞', '也', '乩', '乳', '乾', '亂', '了', '予', '事', '二', '于', '云', '互', '五', '井', '亙', '些', '亞', '亟', '亡', '交', '亥', '亦', '亨', '享', '京', '亭', '亮', '人', '什', '仁', '仃', '仄', '仆', '仇', '今', '介', '仍', '仔', '仕', '他', '仗', '付', '仙', '仞', '仟', '代', '令', '以', '仰', '仲', '仳', '件', '任', '份', '仿', '企', '伉', '伊', '伍', '伏', '伐', '休', '伕', '伙', '伯', '估', '伴', '伶', '伸', '伺', '似', '伽', '佃', '但', '佇', '位', '低', '住', '佐', '佑', '佔', '何', '佗', '余', '佛', '作', '佝', '佞', '你', '佣', '佩', '佬', '佯', '佰', '佳', '併', '佻', '佾', '使', '侃', '來', '侈', '例', '侍', '侏', '侖', '供', '依', '侮', '侯', '侵', '侶', '便', '係', '促', '俄', '俊', '俎', '俏', '俐', '俑', '俗', '俘', '俚', '保', '俞', '俟', '俠', '信', '修', '俯', '俱', '俳', '俸', '俺', '俾', '倀', '倆', '倉', '個', '倌', '倍', '倏', '們', '倒', '倔', '倖', '倘', '候', '倚', '借', '倡', '倣', '倥', '倦', '倨', '倩', '倪', '倫', '倭', '值', '偃', '假', '偉', '偌', '偎', '偏', '偕', '做', '停', '健', '側', '偵', '偶', '偷', '偺', '偽', '傀', '傅', '傍', '傑', '傖', '傘', '備', '傢', '催', '傭', '傯', '傲', '傳', '債', '傷', '傻', '傾', '僅', '像', '僑', '僕', '僖', '僚', '僥', '僧', '僭', '僮', '僱', '僵', '價', '僻', '儀', '儂', '億', '儈', '儉', '儐', '儒', '儔', '儘', '償', '儡', '優', '儲', '儷', '儼', '兀', '允', '元', '兄', '充', '兆', '兇', '先', '光', '克', '兌', '免', '兒', '兔', '兕', '兗', '兜', '兢', '入', '內', '全', '兩', '八', '公', '六', '兮', '共', '兵', '其', '具', '典', '兼', '冀', '冉', '冊', '再', '冑', '冒', '冕', '冗', '冠', '冢', '冤', '冥', '冬', '冰', '冶', '冷', '冽', '准', '凋', '凌', '凍', '凜', '凝', '几', '凡', '凰', '凱', '凳', '凶', '凸', '凹', '出', '函', '刀', '刁', '刃', '分', '切', '刈', '刊', '刎', '刑', '划', '列', '初', '判', '別', '刨', '利', '刪', '刮', '到', '制', '刷', '券', '刻', '剁', '剃', '則', '削', '剋', '剌', '前', '剎', '剔', '剖', '剛', '剜', '剝', '剩', '剪', '副', '割', '剴', '創', '剷', '剽', '剿', '劃', '劇', '劈', '劉', '劍', '劑', '力', '功', '加', '劣', '助', '努', '劫', '劬', '劾', '勁', '勃', '勇', '勉', '勒', '動', '勗', '勘', '務', '勛', '勝', '勞', '募', '勢', '勤', '勦', '勵', '勸', '勻', '勾', '勿', '包', '匆', '匈', '匍', '匏', '匐', '匕', '化', '北', '匙', '匝', '匠', '匡', '匣', '匪', '匯', '匱', '匹', '匾', '匿', '區', '十', '千', '卅', '升', '午', '卉', '半', '卑', '卒', '卓', '協', '南', '博', '卜', '卞', '占', '卡', '卦', '卮', '卯', '印', '危', '即', '卵', '卷', '卸', '卹', '卻', '卿', '厄', '厚', '厝', '原', '厥', '厭', '厲', '去', '參', '又', '叉', '及', '友', '反', '叔', '取', '受', '叛', '叟', '叢', '口', '古', '句', '另', '叨', '叩', '只', '叫', '召', '叭', '叮', '可', '台', '叱', '史', '右', '叵', '司', '叼', '吁', '吃', '各', '吆', '合', '吉', '吊', '吋', '同', '名', '后', '吏', '吐', '向', '吒', '君', '吝', '吞', '吟', '吠', '否', '吧', '吩', '含', '吭', '吮', '吱', '吳', '吵', '吶', '吸', '吹', '吻', '吼', '吾', '呀', '呂', '呃', '呆', '呈', '告', '呎', '呢', '周', '呱', '味', '呵', '呶', '呷', '呸', '呻', '呼', '命', '咀', '咄', '咆', '咋', '和', '咎', '咐', '咒', '咕', '咖', '咚', '咦', '咨', '咪', '咫', '咬', '咯', '咱', '咳', '咸', '咻', '咽', '哀', '品', '哂', '哄', '哇', '哈', '哉', '哎', '員', '哥', '哦', '哨', '哩', '哪', '哭', '哮', '哲', '哺', '哼', '唁', '唆', '唉', '唐', '唔', '唧', '唬', '售', '唯', '唱', '唳', '唷', '唸', '唾', '啃', '啄', '商', '啊', '問', '啕', '啖', '啜', '啞', '啟', '啡', '啣', '啤', '啦', '啪', '啻', '啼', '啾', '喀', '喂', '喃', '善', '喇', '喉', '喊', '喋', '喔', '喘', '喚', '喜', '喝', '喟', '喧', '喪', '喬', '單', '喱', '喲', '喳', '喻', '嗅', '嗆', '嗇', '嗎', '嗑', '嗓', '嗚', '嗜', '嗟', '嗡', '嗣', '嗤', '嗥', '嗦', '嗨', '嗯', '嗷', '嗽', '嗾', '嘀', '嘆', '嘈', '嘉', '嘍', '嘎', '嘔', '嘖', '嘗', '嘛', '嘟', '嘩', '嘮', '嘯', '嘰', '嘲', '嘴', '嘶', '嘹', '嘻', '嘿', '噎', '噓', '噗', '噙', '噢', '噤', '噥', '器', '噩', '噪', '噫', '噬', '噯', '噱', '噴', '噸', '噹', '嚀', '嚅', '嚇', '嚎', '嚏', '嚐', '嚕', '嚥', '嚨', '嚮', '嚴', '嚶', '嚷', '嚼', '囀', '囁', '囂', '囈', '囉', '囊', '囌', '囑', '囚', '四', '回', '因', '囤', '囪', '困', '固', '圃', '圈', '國', '圍', '園', '圓', '圖', '團', '土', '在', '圬', '圭', '圯', '地', '圳', '圾', '址', '均', '坊', '坍', '坎', '坏', '坐', '坑', '坡', '坤', '坦', '坩', '坪', '坷', '坼', '垂', '垃', '型', '垠', '垢', '垣', '垮', '埂', '埃', '埋', '城', '埔', '域', '埠', '埤', '執', '培', '基', '堂', '堅', '堆', '堊', '堡', '堤', '堪', '堯', '堰', '報', '場', '堵', '塊', '塌', '塑', '塔', '塗', '塘', '塚', '塞', '塢', '填', '塭', '塵', '塹', '塾', '墀', '境', '墅', '墊', '墓', '墜', '增', '墟', '墨', '墮', '墳', '墾', '壁', '壅', '壇', '壑', '壓', '壕', '壘', '壙', '壞', '壟', '壢', '壤', '壩', '士', '壬', '壯', '壹', '壺', '壽', '夏', '夔', '夕', '外', '夙', '多', '夜', '夠', '夢', '夤', '夥', '大', '天', '太', '夫', '夭', '央', '失', '夷', '夸', '夾', '奄', '奇', '奈', '奉', '奎', '奏', '奐', '契', '奔', '奕', '套', '奘', '奚', '奠', '奢', '奧', '奩', '奪', '奮', '女', '奴', '奶', '奸', '她', '好', '妁', '如', '妃', '妄', '妊', '妍', '妒', '妓', '妖', '妙', '妝', '妞', '妣', '妤', '妥', '妨', '妮', '妯', '妳', '妹', '妻', '妾', '姆', '姊', '始', '姍', '姐', '姑', '姒', '姓', '委', '姘', '姚', '姜', '姣', '姥', '姦', '姨', '姪', '姬', '姻', '姿', '威', '娃', '娌', '娑', '娓', '娘', '娛', '娜', '娟', '娠', '娣', '娥', '娩', '娶', '娼', '婀', '婁', '婆', '婉', '婊', '婚', '婢', '婦', '婪', '婷', '婿', '媒', '媚', '媛', '媲', '媳', '媼', '媽', '媾', '嫁', '嫂', '嫉', '嫌', '嫖', '嫗', '嫘', '嫡', '嫣', '嫦', '嫩', '嫵', '嫻', '嬉', '嬋', '嬌', '嬝', '嬤', '嬪', '嬰', '嬴', '嬸', '孀', '子', '孑', '孓', '孔', '孕', '字', '存', '孚', '孜', '孝', '孟', '季', '孤', '孩', '孫', '孰', '孱', '孳', '孵', '學', '孺', '孽', '孿', '它', '宅', '宇', '守', '安', '宋', '完', '宏', '宗', '官', '宙', '定', '宛', '宜', '客', '宣', '室', '宥', '宦', '宮', '宰', '害', '宴', '宵', '家', '宸', '容', '宿', '寂', '寄', '寅', '密', '寇', '富', '寐', '寒', '寓', '寞', '察', '寡', '寢', '寤', '寥', '實', '寧', '寨', '審', '寫', '寬', '寮', '寵', '寶', '寸', '寺', '封', '射', '將', '專', '尉', '尊', '尋', '對', '導', '小', '少', '尖', '尚', '尤', '尬', '就', '尷', '尸', '尹', '尺', '尼', '尾', '尿', '局', '屁', '居', '屆', '屈', '屋', '屍', '屎', '屏', '屐', '屑', '展', '屜', '屠', '屢', '層', '履', '屬', '屯', '山', '屹', '岌', '岐', '岑', '岔', '岡', '岩', '岫', '岱', '岳', '岷', '岸', '峙', '峨', '峪', '峭', '峰', '島', '峻', '峽', '崁', '崆', '崇', '崎', '崑', '崔', '崖', '崙', '崛', '崢', '崩', '嵌', '嵐', '嵩', '嶄', '嶇', '嶝', '嶺', '嶼', '嶽', '巍', '巒', '巔', '巖', '川', '州', '巡', '巢', '工', '左', '巧', '巨', '巫', '差', '己', '已', '巳', '巴', '巷', '巽', '巾', '市', '布', '帆', '希', '帑', '帕', '帖', '帘', '帚', '帛', '帝', '帥', '師', '席', '帳', '帶', '帷', '常', '帽', '幀', '幅', '幌', '幔', '幗', '幛', '幟', '幢', '幣', '幫', '干', '平', '年', '并', '幸', '幹', '幻', '幼', '幽', '幾', '庇', '床', '序', '底', '庖', '店', '庚', '府', '庠', '度', '座', '庫', '庭', '庵', '庶', '康', '庸', '庾', '廁', '廂', '廈', '廉', '廊', '廓', '廖', '廚', '廝', '廟', '廠', '廢', '廣', '廬', '廳', '延', '廷', '建', '廿', '弁', '弄', '弈', '弊', '式', '弒', '弓', '弔', '引', '弗', '弘', '弛', '弟', '弦', '弧', '弩', '弭', '弱', '張', '強', '弼', '彆', '彈', '彌', '彎', '彗', '彙', '形', '彤', '彥', '彩', '彪', '彫', '彬', '彭', '彰', '影', '彷', '役', '彼', '彿', '往', '征', '待', '徇', '很', '徊', '律', '後', '徐', '徑', '徒', '得', '徘', '徙', '從', '御', '徨', '復', '循', '徬', '微', '徵', '德', '徹', '徽', '心', '必', '忌', '忍', '忖', '志', '忘', '忙', '忝', '忠', '快', '忱', '念', '忽', '忿', '怎', '怏', '怒', '怔', '怕', '怖', '思', '怠', '怡', '急', '性', '怨', '怪', '怯', '怵', '恃', '恆', '恍', '恐', '恕', '恙', '恢', '恣', '恤', '恥', '恨', '恩', '恪', '恫', '恬', '恭', '息', '恰', '恿', '悄', '悅', '悉', '悌', '悍', '悔', '悖', '悚', '悟', '悠', '患', '您', '悲', '悴', '悵', '悶', '悸', '悻', '悼', '悽', '情', '惆', '惋', '惑', '惕', '惘', '惚', '惜', '惟', '惠', '惡', '惦', '惰', '惱', '想', '惴', '惶', '惹', '惺', '惻', '愀', '愁', '愈', '愉', '愎', '意', '愕', '愚', '愛', '愜', '感', '愣', '愧', '愴', '愾', '愿', '慄', '慇', '慈', '態', '慌', '慍', '慎', '慕', '慘', '慚', '慝', '慟', '慢', '慣', '慧', '慨', '慫', '慮', '慰', '慶', '慷', '慼', '慾', '憂', '憊', '憎', '憐', '憑', '憔', '憚', '憤', '憧', '憩', '憫', '憬', '憲', '憶', '憾', '懂', '懇', '懈', '應', '懊', '懍', '懣', '懦', '懲', '懵', '懶', '懷', '懸', '懺', '懼', '懾', '懿', '戀', '戈', '戊', '戌', '戍', '戎', '成', '我', '戒', '戕', '或', '戚', '戛', '戟', '戡', '戢', '截', '戮', '戰', '戲', '戳', '戴', '戶', '戾', '房', '所', '扁', '扇', '扈', '扉', '手', '才', '扎', '扒', '打', '扔', '托', '扛', '扣', '扭', '扮', '扯', '扳', '扶', '批', '扼', '找', '承', '技', '抄', '抉', '把', '抑', '抒', '抓', '投', '抖', '抗', '折', '抨', '披', '抬', '抱', '抵', '抹', '押', '抽', '抿', '拂', '拄', '拆', '拇', '拈', '拉', '拋', '拌', '拍', '拎', '拐', '拒', '拓', '拔', '拖', '拗', '拘', '拙', '拚', '招', '拜', '括', '拭', '拮', '拯', '拱', '拳', '拴', '拷', '拼', '拽', '拾', '拿', '持', '指', '挈', '按', '挑', '挖', '挨', '挪', '挫', '振', '挺', '挽', '挾', '捂', '捆', '捉', '捎', '捏', '捐', '捕', '捧', '捨', '捩', '捫', '捱', '捲', '捶', '捷', '捻', '掀', '掃', '掄', '授', '掉', '掌', '掏', '排', '掖', '掘', '掙', '掛', '掠', '採', '探', '掣', '接', '控', '推', '掩', '措', '掬', '揀', '揆', '揉', '揍', '描', '提', '插', '揖', '揚', '換', '握', '揣', '揩', '揪', '揭', '揮', '援', '損', '搏', '搓', '搔', '搖', '搗', '搜', '搞', '搪', '搬', '搭', '搶', '搽', '搾', '摑', '摒', '摔', '摘', '摟', '摧', '摩', '摯', '摸', '摹', '摺', '撇', '撈', '撐', '撒', '撓', '撕', '撚', '撞', '撤', '撥', '撩', '撫', '撬', '播', '撮', '撰', '撲', '撻', '撼', '撿', '擁', '擂', '擄', '擅', '擇', '擊', '擋', '操', '擎', '擒', '擔', '擘', '據', '擠', '擦', '擬', '擰', '擱', '擲', '擴', '擺', '擻', '擾', '攀', '攆', '攏', '攔', '攘', '攙', '攜', '攝', '攣', '攤', '攪', '攫', '攬', '支', '收', '改', '攻', '放', '政', '故', '效', '敏', '救', '敖', '敗', '敘', '教', '敝', '敞', '敢', '散', '敦', '敬', '敲', '整', '敵', '敷', '數', '斂', '斃', '文', '斐', '斑', '斗', '料', '斜', '斟', '斡', '斤', '斥', '斧', '斫', '斬', '斯', '新', '斷', '方', '於', '施', '旁', '旅', '旋', '旌', '旎', '族', '旖', '旗', '既', '日', '旦', '旨', '早', '旬', '旭', '旱', '旺', '昀', '昂', '昆', '昌', '明', '昏', '易', '昔', '星', '映', '春', '昧', '昨', '昭', '是', '時', '晃', '晉', '晌', '晏', '晒', '晚', '晝', '晤', '晦', '晨', '普', '景', '晰', '晴', '晶', '智', '暇', '暈', '暉', '暑', '暖', '暗', '暢', '暨', '暫', '暮', '暴', '暹', '曆', '曉', '曖', '曙', '曝', '曠', '曦', '曰', '曲', '曳', '更', '曷', '書', '曹', '曼', '曾', '替', '最', '會', '月', '有', '朋', '服', '朔', '朕', '朗', '望', '朝', '期', '朦', '朧', '木', '未', '末', '本', '札', '朮', '朱', '朴', '朵', '朽', '杉', '李', '杏', '材', '村', '杖', '杜', '杞', '束', '杭', '杯', '杰', '東', '杳', '杵', '杷', '松', '板', '枇', '枉', '枋', '析', '枕', '林', '枚', '果', '枝', '枯', '枴', '架', '枸', '柄', '柏', '某', '柑', '染', '柔', '柚', '柞', '查', '柩', '柬', '柯', '柱', '柳', '柴', '柵', '柿', '栓', '栗', '校', '栩', '株', '核', '根', '格', '栽', '桀', '桂', '桃', '桅', '框', '案', '桌', '桐', '桑', '桓', '桔', '桶', '桿', '梁', '梃', '梅', '梆', '梓', '梔', '梗', '條', '梟', '梢', '梧', '梨', '梭', '梯', '械', '梱', '梳', '梵', '棄', '棉', '棋', '棍', '棒', '棕', '棗', '棘', '棚', '棟', '棠', '棣', '棧', '森', '棲', '棵', '棹', '棺', '椅', '植', '椎', '椒', '椰', '楊', '楓', '楔', '楚', '楞', '楠', '楨', '楫', '業', '極', '楷', '楹', '概', '榆', '榔', '榕', '榛', '榜', '榨', '榫', '榭', '榮', '榴', '榷', '榻', '槁', '構', '槌', '槍', '槐', '槓', '槨', '槳', '槽', '樁', '樂', '樅', '樊', '樓', '標', '樞', '樟', '模', '樣', '樵', '樸', '樹', '樺', '樽', '橄', '橇', '橋', '橘', '橙', '機', '橡', '橢', '橫', '檀', '檄', '檔', '檜', '檢', '檬', '檳', '檸', '檻', '櫂', '櫃', '櫓', '櫚', '櫛', '櫝', '櫥', '櫻', '欄', '權', '欖', '欠', '次', '欣', '欲', '欺', '欽', '款', '歇', '歉', '歌', '歐', '歙', '歟', '歡', '止', '正', '此', '步', '武', '歧', '歪', '歲', '歷', '歸', '歹', '死', '歿', '殃', '殆', '殉', '殊', '殖', '殘', '殤', '殮', '殯', '殲', '段', '殷', '殺', '殼', '殿', '毀', '毅', '毆', '毋', '母', '每', '毒', '毓', '比', '毗', '毛', '毫', '毯', '毽', '氏', '氐', '民', '氓', '氖', '氛', '氟', '氣', '氤', '氦', '氧', '氨', '氫', '氮', '氯', '氳', '水', '永', '氾', '汀', '汁', '求', '汐', '汕', '汗', '汙', '汝', '汞', '江', '池', '汨', '汪', '汰', '汲', '決', '汽', '汾', '沁', '沃', '沅', '沈', '沉', '沌', '沐', '沒', '沖', '沙', '沛', '沫', '沮', '沱', '河', '沸', '油', '治', '沼', '沽', '沾', '沿', '況', '泄', '泅', '泉', '泊', '泌', '泓', '法', '泗', '泛', '泡', '波', '泣', '泥', '注', '泰', '泱', '泳', '洋', '洌', '洗', '洛', '洞', '津', '洪', '洱', '洲', '洶', '活', '洽', '派', '流', '浙', '浚', '浦', '浩', '浪', '浬', '浮', '浴', '海', '浸', '涇', '消', '涉', '涎', '涓', '涕', '涮', '涯', '液', '涵', '涸', '涼', '淄', '淅', '淆', '淇', '淋', '淌', '淑', '淒', '淘', '淙', '淚', '淞', '淡', '淤', '淨', '淪', '淫', '淮', '深', '淳', '淵', '混', '淹', '淺', '添', '清', '渙', '渚', '減', '渝', '渠', '渡', '渣', '渤', '渥', '渦', '測', '渭', '港', '渲', '渴', '游', '渺', '渾', '湃', '湊', '湍', '湔', '湖', '湘', '湛', '湧', '湮', '湯', '溉', '源', '準', '溘', '溜', '溝', '溢', '溥', '溪', '溫', '溯', '溶', '溺', '溼', '滂', '滄', '滅', '滇', '滋', '滌', '滑', '滓', '滔', '滬', '滯', '滲', '滴', '滾', '滿', '漁', '漂', '漆', '漏', '漓', '演', '漕', '漠', '漢', '漣', '漩', '漪', '漫', '漬', '漯', '漱', '漲', '漳', '漸', '漾', '漿', '潑', '潔', '潘', '潛', '潤', '潦', '潭', '潮', '潰', '潸', '潺', '潼', '澀', '澄', '澆', '澈', '澎', '澗', '澡', '澤', '澧', '澱', '澳', '澹', '激', '濁', '濂', '濃', '濘', '濛', '濟', '濠', '濡', '濤', '濫', '濬', '濯', '濱', '濺', '濾', '瀆', '瀉', '瀋', '瀏', '瀑', '瀕', '瀚', '瀛', '瀝', '瀟', '瀨', '瀰', '瀾', '灌', '灑', '灘', '灣', '灤', '火', '灰', '灶', '灸', '灼', '災', '炊', '炎', '炒', '炕', '炙', '炫', '炬', '炭', '炮', '炯', '炳', '炸', '為', '烈', '烊', '烏', '烘', '烙', '烤', '烹', '烽', '焉', '焊', '焙', '焚', '無', '焦', '焰', '然', '煉', '煌', '煎', '煙', '煜', '煞', '煤', '煥', '煦', '照', '煩', '煬', '煮', '煽', '熄', '熊', '熔', '熙', '熟', '熨', '熬', '熱', '熹', '熾', '燃', '燄', '燈', '燉', '燎', '燐', '燒', '燕', '燙', '燜', '營', '燥', '燦', '燧', '燬', '燭', '燮', '燴', '燻', '爆', '爍', '爐', '爛', '爨', '爪', '爬', '爭', '爰', '爵', '父', '爸', '爹', '爺', '爻', '爽', '爾', '牆', '片', '版', '牌', '牒', '牖', '牘', '牙', '牛', '牝', '牟', '牠', '牡', '牢', '牧', '物', '牯', '牲', '牴', '特', '牽', '犀', '犁', '犄', '犒', '犖', '犛', '犢', '犧', '犬', '犯', '狀', '狂', '狄', '狎', '狐', '狗', '狙', '狠', '狡', '狩', '狷', '狸', '狹', '狼', '狽', '猓', '猖', '猙', '猛', '猜', '猥', '猩', '猴', '猶', '猷', '猾', '猿', '獄', '獅', '獎', '獐', '獗', '獨', '獰', '獲', '獵', '獷', '獸', '獺', '獻', '玀', '玄', '率', '玉', '王', '玖', '玟', '玨', '玩', '玫', '玲', '玳', '玷', '玻', '珀', '珊', '珍', '珠', '班', '珮', '現', '球', '琅', '理', '琉', '琊', '琍', '琢', '琥', '琪', '琳', '琴', '琵', '琶', '琺', '琿', '瑁', '瑕', '瑙', '瑚', '瑛', '瑜', '瑞', '瑟', '瑣', '瑤', '瑩', '瑪', '瑯', '瑰', '璃', '璋', '璜', '璣', '璦', '璧', '璩', '環', '璽', '瓊', '瓏', '瓜', '瓠', '瓢', '瓣', '瓦', '瓶', '瓷', '甄', '甌', '甕', '甘', '甚', '甜', '生', '產', '甥', '甦', '用', '甩', '甫', '甬', '甭', '田', '由', '甲', '申', '男', '甸', '甽', '界', '畏', '畔', '留', '畚', '畜', '畝', '畢', '略', '畦', '番', '畫', '異', '當', '畸', '疆', '疇', '疊', '疋', '疏', '疑', '疙', '疚', '疝', '疤', '疥', '疫', '疲', '疳', '疵', '疹', '疼', '疽', '疾', '病', '症', '痊', '痔', '痕', '痘', '痙', '痛', '痞', '痢', '痣', '痰', '痱', '痲', '痴', '痺', '痿', '瘀', '瘁', '瘉', '瘋', '瘍', '瘓', '瘟', '瘠', '瘡', '瘤', '瘦', '瘧', '瘩', '瘴', '瘸', '療', '癆', '癌', '癒', '癖', '癘', '癢', '癥', '癩', '癬', '癮', '癱', '癲', '癸', '登', '發', '白', '百', '皂', '的', '皆', '皇', '皈', '皎', '皓', '皖', '皚', '皮', '皰', '皴', '皺', '皿', '盂', '盃', '盆', '盈', '益', '盍', '盎', '盒', '盔', '盛', '盜', '盞', '盟', '盡', '監', '盤', '盥', '盧', '盪', '目', '盯', '盲', '直', '相', '盹', '盼', '盾', '省', '眉', '看', '真', '眠', '眨', '眩', '眶', '眷', '眸', '眺', '眼', '眾', '睏', '睛', '睜', '睞', '睡', '督', '睥', '睦', '睨', '睪', '睫', '睬', '睹', '睽', '睿', '瞄', '瞇', '瞌', '瞎', '瞑', '瞞', '瞟', '瞠', '瞥', '瞧', '瞪', '瞬', '瞭', '瞰', '瞳', '瞻', '瞽', '瞿', '矇', '矓', '矗', '矚', '矛', '矜', '矢', '矣', '知', '矩', '短', '矮', '矯', '石', '矽', '砂', '砌', '砍', '研', '砝', '砥', '砧', '砭', '砰', '破', '砷', '砸', '硃', '硝', '硫', '硬', '硯', '硼', '碉', '碌', '碎', '碑', '碗', '碘', '碟', '碧', '碩', '碰', '碳', '確', '碼', '碾', '磁', '磅', '磊', '磋', '磐', '磕', '磚', '磨', '磬', '磯', '磴', '磷', '磺', '礁', '礎', '礙', '礦', '礪', '礫', '礬', '示', '社', '祀', '祁', '祆', '祇', '祈', '祉', '祐', '祕', '祖', '祗', '祚', '祝', '神', '祟', '祠', '祥', '票', '祭', '祺', '祿', '禁', '禍', '禎', '福', '禦', '禧', '禪', '禮', '禱', '禹', '禽', '禾', '禿', '秀', '私', '秉', '秋', '科', '秒', '租', '秣', '秤', '秦', '秧', '秩', '移', '稀', '稅', '稈', '程', '稍', '稔', '稚', '稜', '稟', '稠', '種', '稱', '稷', '稻', '稼', '稽', '稿', '穀', '穆', '穌', '積', '穎', '穗', '穡', '穢', '穩', '穫', '穴', '究', '穹', '空', '穿', '突', '窄', '窈', '窒', '窕', '窖', '窗', '窘', '窟', '窠', '窩', '窪', '窮', '窯', '窺', '竄', '竅', '竇', '竊', '立', '站', '竟', '章', '竣', '童', '竭', '端', '競', '竹', '竺', '竽', '竿', '笆', '笑', '笙', '笛', '笞', '笠', '符', '笨', '第', '筆', '等', '筋', '筍', '筏', '筐', '筒', '答', '策', '筠', '筵', '筷', '箋', '箏', '箔', '箕', '算', '箝', '管', '箭', '箱', '箴', '節', '篁', '範', '篆', '篇', '築', '篙', '篛', '篡', '篤', '篩', '篷', '篾', '簇', '簍', '簑', '簞', '簡', '簣', '簧', '簪', '簫', '簷', '簸', '簽', '簾', '簿', '籃', '籌', '籍', '籐', '籟', '籠', '籤', '籬', '籮', '籲', '米', '粉', '粒', '粗', '粟', '粥', '粱', '粳', '粵', '粹', '粽', '精', '糊', '糕', '糖', '糙', '糜', '糞', '糟', '糠', '糢', '糧', '糯', '糸', '系', '糾', '紀', '紂', '約', '紅', '紇', '紉', '紊', '紋', '納', '紐', '純', '紕', '紗', '紙', '級', '紛', '紜', '素', '紡', '索', '紫', '紮', '累', '細', '紳', '紹', '紼', '絀', '終', '絃', '組', '絆', '結', '絕', '絞', '絡', '絢', '給', '絨', '絮', '統', '絲', '絹', '綁', '綏', '綑', '經', '綜', '綞', '綠', '綢', '維', '綰', '綱', '網', '綴', '綵', '綸', '綺', '綻', '綽', '綾', '綿', '緇', '緊', '緒', '緘', '線', '緝', '緞', '締', '緣', '編', '緩', '緬', '緯', '練', '緻', '縈', '縊', '縑', '縛', '縣', '縫', '縮', '縱', '縲', '縷', '總', '績', '繁', '繃', '繅', '繆', '織', '繕', '繚', '繞', '繡', '繩', '繪', '繫', '繭', '繹', '繼', '繽', '纂', '續', '纏', '纓', '纖', '纜', '缶', '缸', '缺', '缽', '罄', '罈', '罐', '罔', '罕', '罟', '罩', '罪', '置', '罰', '署', '罵', '罷', '罹', '羅', '羈', '羊', '羋', '羌', '美', '羔', '羚', '羞', '群', '羨', '義', '羯', '羲', '羶', '羸', '羹', '羽', '羿', '翁', '翅', '翌', '翎', '習', '翔', '翕', '翟', '翠', '翡', '翩', '翰', '翱', '翳', '翹', '翻', '翼', '耀', '老', '考', '者', '耆', '而', '耍', '耐', '耒', '耕', '耗', '耘', '耙', '耜', '耳', '耶', '耽', '耿', '聆', '聊', '聖', '聘', '聚', '聞', '聯', '聰', '聱', '聲', '聳', '聶', '職', '聽', '聾', '聿', '肄', '肅', '肆', '肇', '肉', '肋', '肌', '肓', '肖', '肘', '肚', '肛', '肝', '股', '肢', '肥', '肩', '肪', '肫', '肯', '肱', '育', '肴', '肺', '胃', '胄', '背', '胎', '胖', '胚', '胛', '胞', '胡', '胤', '胥', '胭', '胰', '胱', '胳', '胴', '胸', '能', '脂', '脅', '脆', '脈', '脊', '脖', '脣', '脩', '脫', '脯', '脹', '脾', '腆', '腋', '腎', '腐', '腑', '腔', '腕', '腥', '腦', '腫', '腮', '腰', '腱', '腳', '腸', '腹', '腺', '腿', '膀', '膈', '膊', '膏', '膚', '膛', '膜', '膝', '膠', '膨', '膩', '膳', '膺', '膽', '膾', '膿', '臀', '臂', '臃', '臆', '臉', '臍', '臏', '臘', '臚', '臟', '臣', '臥', '臧', '臨', '自', '臬', '臭', '至', '致', '臺', '臻', '臼', '臾', '舀', '舂', '舅', '與', '興', '舉', '舊', '舌', '舍', '舐', '舒', '舔', '舛', '舜', '舞', '舟', '舢', '舨', '航', '舫', '般', '舵', '舶', '舷', '船', '艇', '艘', '艙', '艦', '艮', '良', '艱', '色', '艾', '芋', '芍', '芒', '芙', '芝', '芟', '芥', '芬', '芭', '花', '芳', '芹', '芻', '芽', '苑', '苒', '苓', '苔', '苗', '苛', '苜', '苞', '苟', '苣', '若', '苦', '苧', '英', '茁', '茂', '范', '茄', '茅', '茉', '茗', '茫', '茱', '茲', '茴', '茵', '茶', '茸', '茹', '荀', '草', '荊', '荏', '荐', '荒', '荔', '荷', '荸', '荻', '荼', '莉', '莊', '莎', '莒', '莓', '莖', '莘', '莞', '莠', '莢', '莫', '莽', '菁', '菅', '菊', '菌', '菜', '菠', '菩', '華', '菰', '菱', '菲', '菴', '菸', '菽', '萃', '萄', '萊', '萋', '萌', '萍', '萎', '萬', '萱', '萵', '萸', '萼', '落', '葉', '著', '葛', '葡', '董', '葦', '葩', '葫', '葬', '葵', '葷', '蒂', '蒐', '蒙', '蒜', '蒞', '蒲', '蒸', '蒼', '蒿', '蓀', '蓄', '蓆', '蓉', '蓋', '蓓', '蓬', '蓮', '蓿', '蔑', '蔓', '蔔', '蔗', '蔚', '蔡', '蔣', '蔥', '蔬', '蔭', '蔽', '蕃', '蕈', '蕉', '蕊', '蕙', '蕨', '蕩', '蕪', '蕭', '蕾', '薄', '薇', '薑', '薔', '薛', '薜', '薩', '薪', '薯', '薰', '藉', '藍', '藏', '藐', '藕', '藝', '藤', '藥', '藩', '藪', '藹', '藺', '藻', '蘆', '蘇', '蘊', '蘋', '蘑', '蘗', '蘚', '蘭', '蘸', '蘿', '虎', '虐', '虔', '處', '虛', '虜', '虞', '號', '虧', '虫', '虱', '虹', '蚊', '蚌', '蚓', '蚣', '蚤', '蚩', '蚪', '蚯', '蚱', '蚵', '蚶', '蛀', '蛄', '蛆', '蛇', '蛋', '蛔', '蛙', '蛛', '蛟', '蛤', '蛭', '蛹', '蛻', '蛾', '蜀', '蜂', '蜃', '蜇', '蜈', '蜓', '蜘', '蜜', '蜢', '蜥', '蜴', '蜻', '蜿', '蝌', '蝕', '蝗', '蝙', '蝠', '蝦', '蝨', '蝴', '蝶', '蝸', '螂', '螃', '融', '螞', '螟', '螢', '螫', '螳', '螺', '螻', '蟀', '蟆', '蟈', '蟋', '蟑', '蟒', '蟬', '蟯', '蟲', '蟹', '蟻', '蠅', '蠍', '蠔', '蠕', '蠟', '蠡', '蠢', '蠣', '蠱', '蠶', '蠹', '蠻', '血', '行', '衍', '術', '街', '衙', '衛', '衝', '衡', '衢', '衣', '表', '衫', '衰', '衷', '袁', '袂', '袈', '袋', '袍', '袒', '袖', '袞', '被', '袱', '裁', '裂', '裊', '裔', '裕', '裘', '裙', '補', '裝', '裟', '裡', '裨', '裳', '裴', '裸', '裹', '製', '褂', '複', '褐', '褒', '褓', '褚', '褥', '褪', '褫', '褲', '褶', '褸', '褻', '襄', '襖', '襟', '襠', '襤', '襪', '襯', '襲', '西', '要', '覃', '覆', '見', '規', '覓', '視', '覦', '親', '覬', '覲', '覺', '覽', '觀', '角', '解', '觴', '觸', '言', '訂', '訃', '計', '訊', '訌', '討', '訐', '訓', '訕', '訖', '託', '記', '訛', '訝', '訟', '訣', '訥', '訪', '設', '許', '訴', '診', '註', '証', '詁', '詆', '詐', '詔', '評', '詛', '詞', '詠', '詢', '詣', '試', '詩', '詫', '詬', '詭', '詮', '詰', '話', '該', '詳', '詹', '詼', '誅', '誇', '誌', '認', '誑', '誓', '誕', '誘', '語', '誠', '誡', '誣', '誤', '誥', '誦', '誨', '說', '誰', '課', '誼', '調', '諂', '諄', '談', '諉', '請', '諍', '諒', '論', '諜', '諦', '諧', '諫', '諭', '諮', '諱', '諷', '諸', '諺', '諾', '謀', '謁', '謂', '謄', '謊', '謎', '謗', '謙', '講', '謝', '謠', '謨', '謬', '謹', '譁', '證', '譎', '譏', '識', '譚', '譜', '警', '譬', '譯', '議', '譴', '護', '譽', '讀', '變', '讒', '讓', '讖', '讚', '谷', '谿', '豁', '豆', '豈', '豉', '豌', '豎', '豐', '豔', '豕', '豚', '象', '豢', '豪', '豫', '豬', '豹', '豺', '貂', '貉', '貊', '貌', '貍', '貓', '貝', '貞', '負', '財', '貢', '貧', '貨', '販', '貪', '貫', '責', '貯', '貲', '貳', '貴', '貶', '買', '貸', '費', '貼', '貽', '貿', '賀', '賁', '賂', '賃', '賄', '賅', '資', '賈', '賊', '賑', '賒', '賓', '賜', '賞', '賠', '賢', '賣', '賤', '賦', '質', '賬', '賭', '賴', '賺', '購', '賽', '贅', '贈', '贊', '贍', '贏', '贓', '贖', '贗', '贛', '赤', '赦', '赧', '赫', '赭', '走', '赳', '赴', '起', '趁', '超', '越', '趕', '趙', '趟', '趣', '趨', '足', '趴', '趾', '跆', '跋', '跌', '跎', '跑', '跚', '跛', '距', '跟', '跡', '跨', '跪', '路', '跳', '跺', '跼', '踏', '踐', '踝', '踟', '踢', '踩', '踫', '踱', '踴', '踵', '踹', '蹂', '蹄', '蹈', '蹉', '蹊', '蹋', '蹙', '蹣', '蹤', '蹦', '蹬', '蹲', '蹶', '蹺', '蹼', '躁', '躂', '躅', '躇', '躉', '躊', '躍', '躑', '躡', '躪', '身', '躬', '躲', '躺', '軀', '車', '軋', '軌', '軍', '軒', '軔', '軛', '軟', '軸', '軻', '軼', '軾', '較', '載', '輊', '輒', '輓', '輔', '輕', '輛', '輜', '輝', '輟', '輦', '輩', '輪', '輯', '輸', '輻', '輾', '輿', '轂', '轄', '轅', '轉', '轍', '轎', '轔', '轟', '轡', '辛', '辜', '辟', '辣', '辦', '辨', '辭', '辮', '辯', '辰', '辱', '農', '迂', '迄', '迅', '迆', '迎', '近', '返', '迢', '迥', '迦', '迪', '迫', '迭', '述', '迴', '迷', '迺', '追', '退', '送', '逃', '逅', '逆', '逍', '透', '逐', '途', '逕', '逖', '逗', '這', '通', '逛', '逝', '逞', '速', '造', '逢', '連', '逮', '週', '進', '逵', '逸', '逼', '逾', '遁', '遂', '遇', '遊', '運', '遍', '過', '遏', '遐', '遑', '道', '達', '違', '遘', '遙', '遜', '遞', '遠', '遣', '遨', '適', '遭', '遮', '遲', '遴', '遵', '遷', '選', '遺', '遼', '遽', '避', '邀', '邁', '邂', '還', '邇', '邊', '邏', '邐', '邑', '邕', '邢', '那', '邦', '邪', '邱', '邵', '邸', '郁', '郊', '郎', '郡', '部', '郭', '郵', '都', '鄂', '鄉', '鄒', '鄙', '鄧', '鄭', '鄰', '鄱', '鄹', '酉', '酊', '酋', '酌', '配', '酒', '酗', '酣', '酥', '酩', '酪', '酬', '酵', '酷', '酸', '醃', '醇', '醉', '醋', '醒', '醜', '醞', '醣', '醫', '醬', '醺', '釀', '釁', '采', '釉', '釋', '里', '重', '野', '量', '釐', '金', '釗', '釘', '釜', '針', '釣', '釦', '釧', '釵', '鈉', '鈍', '鈐', '鈔', '鈕', '鈞', '鈣', '鈴', '鈷', '鈸', '鈽', '鈾', '鉀', '鉋', '鉑', '鉗', '鉛', '鉤', '鉸', '鉻', '銀', '銅', '銓', '銖', '銘', '銜', '銬', '銳', '銷', '銻', '銼', '鋁', '鋅', '鋒', '鋤', '鋪', '鋸', '鋼', '錄', '錐', '錘', '錚', '錠', '錢', '錦', '錨', '錫', '錯', '錳', '錶', '鍊', '鍋', '鍍', '鍛', '鍥', '鍬', '鍰', '鍵', '鍾', '鎂', '鎊', '鎔', '鎖', '鎢', '鎮', '鎳', '鏃', '鏈', '鏍', '鏑', '鏖', '鏗', '鏘', '鏜', '鏝', '鏟', '鏡', '鏢', '鏤', '鏽', '鐃', '鐘', '鐮', '鐲', '鐳', '鐵', '鐸', '鐺', '鑄', '鑑', '鑒', '鑠', '鑣', '鑰', '鑲', '鑼', '鑽', '鑾', '鑿', '長', '門', '閂', '閃', '閉', '開', '閏', '閑', '閒', '間', '閔', '閘', '閡', '閣', '閤', '閥', '閨', '閩', '閭', '閱', '閻', '闆', '闈', '闊', '闋', '闌', '闐', '闔', '闖', '關', '闡', '闢', '阜', '阡', '阪', '阮', '阱', '防', '阻', '阿', '陀', '附', '陋', '陌', '降', '限', '陛', '陝', '陡', '院', '陣', '除', '陪', '陰', '陲', '陳', '陴', '陵', '陶', '陷', '陸', '陽', '隅', '隆', '隊', '隋', '隍', '階', '隔', '隕', '隘', '隙', '際', '障', '隧', '隨', '險', '隱', '隴', '隸', '隻', '雀', '雁', '雄', '雅', '集', '雇', '雉', '雋', '雌', '雍', '雕', '雖', '雙', '雛', '雜', '雞', '離', '難', '雨', '雪', '雯', '雲', '零', '雷', '雹', '電', '需', '霄', '霆', '震', '霉', '霍', '霎', '霏', '霑', '霓', '霖', '霜', '霞', '霧', '霪', '露', '霸', '霹', '霽', '霾', '靂', '靄', '靈', '青', '靖', '靛', '靜', '非', '靠', '靡', '面', '靦', '靨', '革', '靴', '靶', '靼', '鞅', '鞋', '鞍', '鞏', '鞘', '鞠', '鞣', '鞦', '鞭', '韁', '韃', '韆', '韋', '韌', '韓', '韜', '韭', '音', '韶', '韻', '響', '頁', '頂', '頃', '項', '順', '須', '頊', '頌', '預', '頑', '頒', '頓', '頗', '領', '頡', '頤', '頭', '頰', '頷', '頸', '頹', '頻', '顆', '題', '額', '顎', '顏', '顓', '願', '顛', '類', '顧', '顫', '顯', '顰', '顱', '風', '颯', '颱', '颳', '颶', '颺', '颼', '飄', '飛', '食', '飢', '飧', '飩', '飪', '飭', '飯', '飲', '飴', '飼', '飽', '飾', '餃', '餅', '餉', '養', '餌', '餐', '餒', '餓', '餘', '餛', '餞', '餡', '館', '餵', '餽', '餾', '餿', '饅', '饑', '饒', '饜', '饞', '首', '香', '馥', '馨', '馬', '馭', '馮', '馱', '馳', '馴', '駁', '駐', '駑', '駒', '駕', '駙', '駛', '駝', '駟', '駢', '駭', '駱', '駿', '騁', '騎', '騖', '騙', '騫', '騰', '騷', '騾', '驀', '驃', '驅', '驕', '驗', '驚', '驛', '驟', '驢', '驥', '驪', '骨', '骯', '骰', '骷', '骸', '骼', '髏', '髒', '髓', '體', '高', '髦', '髭', '髮', '髯', '髻', '鬃', '鬆', '鬍', '鬚', '鬢', '鬥', '鬧', '鬨', '鬱', '鬲', '鬼', '魁', '魂', '魄', '魅', '魏', '魔', '魘', '魚', '魯', '魷', '鮑', '鮪', '鮫', '鮮', '鯉', '鯊', '鯧', '鯨', '鯽', '鰍', '鰓', '鰥', '鰭', '鰱', '鰻', '鰾', '鱉', '鱔', '鱖', '鱗', '鱷', '鱸', '鳥', '鳩', '鳳', '鳴', '鳶', '鴆', '鴉', '鴒', '鴕', '鴛', '鴣', '鴦', '鴨', '鴻', '鴿', '鵑', '鵝', '鵠', '鵡', '鵪', '鵬', '鵲', '鶉', '鶯', '鶴', '鷂', '鷓', '鷗', '鷥', '鷹', '鷺', '鸚', '鸞', '鹹', '鹼', '鹽', '鹿', '麂', '麋', '麒', '麓', '麗', '麝', '麟', '麥', '麩', '麴', '麵', '麻', '麼', '麾', '黃', '黍', '黎', '黏', '黑', '黔', '默', '黛', '黜', '黝', '點', '黠', '黨', '黯', '黴', '黷', '鼇', '鼎', '鼓', '鼕', '鼙', '鼠', '鼬', '鼴', '鼻', '鼾', '齊', '齋', '齒', '齜', '齟', '齡', '齣', '齦', '齪', '齬', '齲', '齷', '龍', '龐', '龔']\n",
      "\n",
      "Starting Optuna hyperparameter optimization...\n",
      "This will perform multiple trials to find the best parameter combination\n",
      "Will perform 5 trials...\n",
      "\n",
      "============================================================\n",
      "🔍 OPTUNA TRIAL 1/5\n",
      "============================================================\n",
      "20056 5014 4803\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\n",
      "  Optuna Trial - Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 627/627 [02:56<00:00,  3.56it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████| 157/157 [00:17<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 - Train Acc: 0.0001, Val Acc: 0.0012, Val Loss: 8.4426\n",
      "\n",
      "  Optuna Trial - Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 627/627 [03:08<00:00,  3.32it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████| 157/157 [00:14<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 - Train Acc: 0.0018, Val Acc: 0.0179, Val Loss: 7.7078\n",
      "\n",
      "  Optuna Trial - Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 627/627 [02:38<00:00,  3.95it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████| 157/157 [00:14<00:00, 10.86it/s]\n",
      "[I 2025-06-11 00:43:40,498] Trial 0 finished with value: 0.06162744315915437 and parameters: {'lr': 0.0001329291894316216, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0003967605077052988, 'scheduler': 'ExponentialLR', 'gamma': 0.9364594334728974, 'dropout_rate': 0.4329770563201687}. Best is trial 0 with value: 0.06162744315915437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 - Train Acc: 0.0164, Val Acc: 0.0616, Val Loss: 6.5970\n",
      "  Trial Best Accuracy: 0.0616\n",
      "\n",
      "✅ Trial 1 completed!\n",
      "📊 Current Best Accuracy: 0.0616\n",
      "🎯 Best Parameters so far: {'lr': 0.0001329291894316216, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0003967605077052988, 'scheduler': 'ExponentialLR', 'gamma': 0.9364594334728974, 'dropout_rate': 0.4329770563201687}\n",
      "\n",
      "============================================================\n",
      "🔍 OPTUNA TRIAL 2/5\n",
      "============================================================\n",
      "20056 5014 4803\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\n",
      "  Optuna Trial - Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 157/157 [19:37<00:00,  7.50s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 40/40 [00:18<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 - Train Acc: 0.0000, Val Acc: 0.0002, Val Loss: 8.4841\n",
      "\n",
      "  Optuna Trial - Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 157/157 [17:44<00:00,  6.78s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 40/40 [01:44<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 - Train Acc: 0.0002, Val Acc: 0.0002, Val Loss: 8.4796\n",
      "\n",
      "  Optuna Trial - Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 157/157 [17:37<00:00,  6.74s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 40/40 [01:46<00:00,  2.66s/it]\n",
      "[I 2025-06-11 01:42:32,582] Trial 1 finished with value: 0.00039888312724371757 and parameters: {'lr': 4.335281794951564e-05, 'batch_size': 128, 'optimizer': 'Adam', 'weight_decay': 6.847920095574779e-05, 'scheduler': 'CosineAnnealingLR', 'gamma': 0.7052314928976662, 'dropout_rate': 0.41407038455720546}. Best is trial 0 with value: 0.06162744315915437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 - Train Acc: 0.0002, Val Acc: 0.0004, Val Loss: 8.4670\n",
      "  Trial Best Accuracy: 0.0004\n",
      "\n",
      "✅ Trial 2 completed!\n",
      "📊 Current Best Accuracy: 0.0616\n",
      "🎯 Best Parameters so far: {'lr': 0.0001329291894316216, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0003967605077052988, 'scheduler': 'ExponentialLR', 'gamma': 0.9364594334728974, 'dropout_rate': 0.4329770563201687}\n",
      "\n",
      "============================================================\n",
      "🔍 OPTUNA TRIAL 3/5\n",
      "============================================================\n",
      "20056 5014 4803\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\n",
      "  Optuna Trial - Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 314/314 [03:30<00:00,  1.49it/s]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 79/79 [00:18<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 - Train Acc: 0.0001, Val Acc: 0.0004, Val Loss: 8.4783\n",
      "\n",
      "  Optuna Trial - Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 314/314 [03:16<00:00,  1.60it/s]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 79/79 [00:18<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 - Train Acc: 0.0001, Val Acc: 0.0004, Val Loss: 8.4541\n",
      "\n",
      "  Optuna Trial - Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 314/314 [03:16<00:00,  1.60it/s]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 79/79 [00:18<00:00,  4.36it/s]\n",
      "[I 2025-06-11 01:53:34,587] Trial 2 finished with value: 0.0007977662544874351 and parameters: {'lr': 3.972110727381908e-05, 'batch_size': 64, 'optimizer': 'Adam', 'weight_decay': 0.0007025166339242157, 'scheduler': 'StepLR', 'step_size': 2, 'gamma': 0.6473864212097256, 'dropout_rate': 0.27606099749584057}. Best is trial 0 with value: 0.06162744315915437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 - Train Acc: 0.0006, Val Acc: 0.0008, Val Loss: 8.4264\n",
      "  Trial Best Accuracy: 0.0008\n",
      "\n",
      "✅ Trial 3 completed!\n",
      "📊 Current Best Accuracy: 0.0616\n",
      "🎯 Best Parameters so far: {'lr': 0.0001329291894316216, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0003967605077052988, 'scheduler': 'ExponentialLR', 'gamma': 0.9364594334728974, 'dropout_rate': 0.4329770563201687}\n",
      "\n",
      "============================================================\n",
      "🔍 OPTUNA TRIAL 4/5\n",
      "============================================================\n",
      "20056 5014 4803\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\n",
      "  Optuna Trial - Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 157/157 [15:03<00:00,  5.75s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 40/40 [01:30<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 - Train Acc: 0.0001, Val Acc: 0.0004, Val Loss: 8.4872\n",
      "\n",
      "  Optuna Trial - Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 157/157 [15:36<00:00,  5.97s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 40/40 [01:25<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 - Train Acc: 0.0001, Val Acc: 0.0006, Val Loss: 8.4855\n",
      "\n",
      "  Optuna Trial - Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 157/157 [15:47<00:00,  6.04s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 40/40 [01:31<00:00,  2.28s/it]\n",
      "[I 2025-06-11 02:44:32,103] Trial 3 finished with value: 0.0005983246908655763 and parameters: {'lr': 2.32335035153901e-05, 'batch_size': 128, 'optimizer': 'AdamW', 'weight_decay': 3.632486956676606e-05, 'scheduler': 'CosineAnnealingLR', 'gamma': 0.8488097705125015, 'dropout_rate': 0.4757995766256756}. Best is trial 0 with value: 0.06162744315915437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 - Train Acc: 0.0002, Val Acc: 0.0000, Val Loss: 8.4841\n",
      "  Trial Best Accuracy: 0.0006\n",
      "\n",
      "✅ Trial 4 completed!\n",
      "📊 Current Best Accuracy: 0.0616\n",
      "🎯 Best Parameters so far: {'lr': 0.0001329291894316216, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0003967605077052988, 'scheduler': 'ExponentialLR', 'gamma': 0.9364594334728974, 'dropout_rate': 0.4329770563201687}\n",
      "\n",
      "============================================================\n",
      "🔍 OPTUNA TRIAL 5/5\n",
      "============================================================\n",
      "20056 5014 4803\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\n",
      "  Optuna Trial - Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 314/314 [06:22<00:00,  1.22s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 79/79 [00:29<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 - Train Acc: 0.0001, Val Acc: 0.0000, Val Loss: 8.4813\n",
      "\n",
      "  Optuna Trial - Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 314/314 [05:46<00:00,  1.10s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 79/79 [00:23<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 - Train Acc: 0.0002, Val Acc: 0.0004, Val Loss: 8.4724\n",
      "\n",
      "  Optuna Trial - Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████| 314/314 [05:34<00:00,  1.07s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████| 79/79 [00:22<00:00,  3.54it/s]\n",
      "[I 2025-06-11 03:03:36,650] Trial 4 finished with value: 0.00039888312724371757 and parameters: {'lr': 0.004835952776465951, 'batch_size': 64, 'optimizer': 'SGD', 'weight_decay': 1.4656553886225336e-05, 'scheduler': 'ExponentialLR', 'gamma': 0.6264205293593214, 'dropout_rate': 0.31707843326329943}. Best is trial 0 with value: 0.06162744315915437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 - Train Acc: 0.0003, Val Acc: 0.0002, Val Loss: 8.4665\n",
      "  Trial Best Accuracy: 0.0004\n",
      "\n",
      "✅ Trial 5 completed!\n",
      "📊 Current Best Accuracy: 0.0616\n",
      "🎯 Best Parameters so far: {'lr': 0.0001329291894316216, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0003967605077052988, 'scheduler': 'ExponentialLR', 'gamma': 0.9364594334728974, 'dropout_rate': 0.4329770563201687}\n",
      "\n",
      "============================================================\n",
      "🏁 OPTUNA OPTIMIZATION COMPLETED\n",
      "============================================================\n",
      "\n",
      "Optuna optimization complete!\n",
      "Best accuracy: 0.0616\n",
      "Best parameters: {'lr': 0.0001329291894316216, 'batch_size': 32, 'optimizer': 'Adam', 'weight_decay': 0.0003967605077052988, 'scheduler': 'ExponentialLR', 'gamma': 0.9364594334728974, 'dropout_rate': 0.4329770563201687}\n",
      "\n",
      "Plotting Optuna optimization results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahui\\AppData\\Local\\Temp\\ipykernel_4180\\1379479490.py:522: ExperimentalWarning: plot_optimization_history is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  optuna.visualization.matplotlib.plot_optimization_history(study, ax=axes[0,0])\n",
      "C:\\Users\\yahui\\AppData\\Local\\Temp\\ipykernel_4180\\1379479490.py:530: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  optuna.visualization.matplotlib.plot_param_importances(study, ax=axes[0,1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot plot optimization history: plot_optimization_history() got an unexpected keyword argument 'ax'\n",
      "Cannot plot parameter importance: plot_param_importances() got an unexpected keyword argument 'ax'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna optimization results saved to: Handwritten_Data\\optuna_optimization_results.png\n",
      "Plotting parameter comparison chart...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter comparison chart saved to: Handwritten_Data\\parameter_comparison.png\n",
      "\n",
      "Using best parameters for complete training...\n",
      "200569 50143 4803\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Using best parameters:\n",
      "  lr: 0.0001329291894316216\n",
      "  batch_size: 32\n",
      "  optimizer: Adam\n",
      "  weight_decay: 0.0003967605077052988\n",
      "  scheduler: ExponentialLR\n",
      "  gamma: 0.9364594334728974\n",
      "  dropout_rate: 0.4329770563201687\n",
      "Starting training, using device: cuda\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full Training:   0%|          | 0/5 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "----------------------------------------\n",
      "Current learning rate: 0.000133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full Training:   0%|          | 0/5 [47:34<?, ?epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred during complete training: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # 添加進度條庫\n",
    "\n",
    "# Step 1: Setup parameters and check CUDA\n",
    "def check_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        current_device = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(current_device)\n",
    "        print(f\"CUDA available, using GPU training\")\n",
    "        print(f\"Current GPU name: {device_name}\")\n",
    "        print(f\"GPU count: {device_count}\")\n",
    "        print(f\"Currently using GPU: {device_name}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU training\")\n",
    "        return False\n",
    "\n",
    "use_gpu = check_cuda()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set local path\n",
    "data_dir = 'Handwritten_Data' # Your data path\n",
    "input_size = 224\n",
    "\n",
    "# Set CUDA optimization parameters\n",
    "if use_gpu:\n",
    "    torch.backends.cudnn.benchmark = True  # Accelerate convolution operations\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Global variables for data loading\n",
    "train_loader_global = None\n",
    "val_loader_global = None\n",
    "train_size_global = None\n",
    "val_size_global = None\n",
    "class_num_global = None\n",
    "class_names_global = None\n",
    "subsample_rate = 0.1\n",
    "\n",
    "# Step 2: Define data loading functions\n",
    "# Step 2: Define data loading functions\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def loaddata(data_dir, batch_size, shuffle=True, subsample_rate=1.0, subsample_val=True):\n",
    "    \"\"\"\n",
    "    Using stratified sampling for better class balance control\n",
    "    \"\"\"\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    raw_dataset = datasets.ImageFolder(root=data_dir, transform=transforms.ToTensor())\n",
    "    \n",
    "    # Get all indices and labels more efficiently\n",
    "    all_indices = torch.arange(len(raw_dataset))\n",
    "    all_labels = torch.tensor([raw_dataset.targets[i] for i in range(len(raw_dataset))])\n",
    "\n",
    "    # Stratified train-val split\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        all_indices.numpy(),  # Convert to numpy for sklearn compatibility\n",
    "        test_size=0.2,\n",
    "        stratify=all_labels.numpy(),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Apply subsampling to training set\n",
    "    if subsample_rate < 1.0:\n",
    "        train_labels = all_labels[train_indices]\n",
    "        \n",
    "        # Stratified subsampling of training set\n",
    "        subsampled_train_indices, _ = train_test_split(\n",
    "            train_indices,\n",
    "            train_size=subsample_rate,\n",
    "            stratify=train_labels.numpy(),\n",
    "            random_state=42\n",
    "        )\n",
    "        train_indices = subsampled_train_indices\n",
    "    \n",
    "    # Apply subsampling to validation set if specified\n",
    "    if subsample_val and subsample_rate < 1.0:\n",
    "        val_labels = all_labels[val_indices]\n",
    "        \n",
    "        # Stratified subsampling of validation set\n",
    "        subsampled_val_indices, _ = train_test_split(\n",
    "            val_indices,\n",
    "            train_size=subsample_rate,\n",
    "            stratify=val_labels.numpy(),\n",
    "            random_state=42\n",
    "        )\n",
    "        val_indices = subsampled_val_indices\n",
    "    \n",
    "    # Create subsets\n",
    "    train_subset = torch.utils.data.Subset(raw_dataset, train_indices)\n",
    "    val_subset = torch.utils.data.Subset(raw_dataset, val_indices)\n",
    "    \n",
    "    class DatasetWrapper(torch.utils.data.Dataset):\n",
    "        def __init__(self, subset, transform):\n",
    "            self.subset = subset\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.subset)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            image, label = self.subset.dataset[self.subset.indices[idx]]\n",
    "            image = transforms.ToPILImage()(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "    \n",
    "    train_dataset_wrapped = DatasetWrapper(train_subset, data_transforms['train'])\n",
    "    val_dataset_wrapped = DatasetWrapper(val_subset, data_transforms['val'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset_wrapped,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_gpu\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset_wrapped,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_gpu\n",
    "    )\n",
    "    print(len(train_indices), len(val_indices), len(raw_dataset.classes))\n",
    "    \n",
    "    return train_loader, val_loader, len(train_indices), len(val_indices), len(raw_dataset.classes), raw_dataset.classes\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr_scheduler(optimizer, scheduler_type='StepLR', step_size=2, gamma=0.5):\n",
    "    \"\"\"Create learning rate scheduler\"\"\"\n",
    "    if scheduler_type == 'StepLR':\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    elif scheduler_type == 'ExponentialLR':\n",
    "        return optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "    elif scheduler_type == 'CosineAnnealingLR':\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    else:\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training function for Optuna optimization\n",
    "def train_model_optuna(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs=3):\n",
    "    \"\"\"Optimized training function, returns best validation accuracy (reduced epochs to 3)\"\"\"\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n  Optuna Trial - Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Simple progress bar for training batches\n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\", ncols=100, \n",
    "                         bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')\n",
    "        \n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # Update every 100 iterations\n",
    "            if train_pbar.n % 100 == 0:\n",
    "                current_acc = running_corrects.double() / (train_pbar.n * inputs.size(0))\n",
    "                train_pbar.set_postfix(loss=f'{loss.item():.3f}', acc=f'{current_acc:.3f}')\n",
    "\n",
    "        train_pbar.close()  # Properly close the progress bar\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validation\", ncols=100,\n",
    "                       bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                if val_pbar.n % 50 == 0:\n",
    "                    val_pbar.set_postfix(loss=f'{loss.item():.3f}')\n",
    "\n",
    "        val_pbar.close()  # Properly close the progress bar\n",
    "\n",
    "        epoch_acc = val_running_corrects.double() / val_size\n",
    "        epoch_loss = val_running_loss / val_size\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1} - Train Acc: {running_corrects.double() / train_size:.4f}, Val Acc: {epoch_acc:.4f}, Val Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"  Trial Best Accuracy: {best_acc:.4f}\\n\")\n",
    "    return best_acc.item()\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function\"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128] if use_gpu else [8, 16, 32])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    scheduler_type = trial.suggest_categorical('scheduler', ['StepLR', 'ExponentialLR', 'CosineAnnealingLR'])\n",
    "    \n",
    "    if scheduler_type == 'StepLR':\n",
    "        step_size = trial.suggest_int('step_size', 2, 8)\n",
    "        gamma = trial.suggest_float('gamma', 0.1, 0.9)\n",
    "    else:\n",
    "        step_size = 2\n",
    "        gamma = trial.suggest_float('gamma', 0.5, 0.95)\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    \n",
    "    # Reload data (using new batch_size)\n",
    "    train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(data_dir, batch_size, subsample_rate=subsample_rate)\n",
    "    \n",
    "    # Build model\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    num_ftrs = model._fc.in_features\n",
    "    \n",
    "    # Add Dropout\n",
    "    model._fc = nn.Sequential(\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(num_ftrs, class_num)\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:  # SGD\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    \n",
    "    # Set learning rate scheduler\n",
    "    scheduler = get_lr_scheduler(optimizer, scheduler_type, step_size, gamma)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Simplified training (reduced epochs to 3)\n",
    "    num_epochs_optuna = 3\n",
    "    best_acc = train_model_optuna(model, criterion, optimizer, scheduler, \n",
    "                                 train_loader, val_loader, train_size, val_size, \n",
    "                                 num_epochs_optuna)\n",
    "    \n",
    "    return best_acc\n",
    "\n",
    "# Training function with full metrics\n",
    "def train_model_full(model, criterion, optimizer, scheduler, train_loader, val_loader, train_size, val_size, num_epochs=5):\n",
    "    \"\"\"Complete training function with all metrics recording (reduced epochs to 8)\"\"\"\n",
    "    since = time.time()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Recording lists\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    train_precisions, val_precisions = [], []\n",
    "    train_recalls, val_recalls = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "\n",
    "    print(f\"Starting training, using device: {device}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Main epoch progress bar\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Full Training\", unit=\"epoch\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 40)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current learning rate: {current_lr:.6f}')\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "\n",
    "        # Training batch progress bar\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False, unit=\"batch\")\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            all_train_preds.append(preds.cpu())\n",
    "            all_train_labels.append(labels.cpu())\n",
    "\n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{(running_corrects.double() / ((i+1) * inputs.size(0))):.4f}'\n",
    "            })\n",
    "\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc.cpu().item())\n",
    "\n",
    "        # Calculate training Precision/Recall/F1\n",
    "        train_preds = torch.cat(all_train_preds)\n",
    "        train_labels = torch.cat(all_train_labels)\n",
    "        train_precisions.append(precision_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "        train_recalls.append(recall_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "        train_f1s.append(f1_score(train_labels, train_preds, average='macro', zero_division=0))\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "\n",
    "        # Validation batch progress bar\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\", leave=False, unit=\"batch\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                all_val_preds.append(preds.cpu())\n",
    "                all_val_labels.append(labels.cpu())\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        epoch_loss = running_loss / val_size\n",
    "        epoch_acc = running_corrects.double() / val_size\n",
    "        print(f'Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        val_losses.append(epoch_loss)\n",
    "        val_accs.append(epoch_acc.cpu().item())\n",
    "\n",
    "        # Calculate validation Precision/Recall/F1\n",
    "        val_preds = torch.cat(all_val_preds)\n",
    "        val_labels = torch.cat(all_val_labels)\n",
    "        val_precisions.append(precision_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        val_recalls.append(recall_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "        val_f1s.append(f1_score(val_labels, val_preds, average='macro', zero_division=0))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            \n",
    "            os.makedirs('./best', exist_ok=True)\n",
    "            checkpoint_path = os.path.join(data_dir, 'best_model_checkpoint.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': epoch_loss,\n",
    "                'acc': epoch_acc,\n",
    "            }, checkpoint_path)\n",
    "            print(f'New best model saved, accuracy: {best_acc:.4f}')\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'best_acc': f'{best_acc:.4f}',\n",
    "            'current_acc': f'{epoch_acc:.4f}',\n",
    "            'loss': f'{epoch_loss:.4f}'\n",
    "        })\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best validation accuracy: {best_acc:.4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Save model\n",
    "    save_dir = os.path.join(data_dir, 'model')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs('./best', exist_ok=True)\n",
    "    model_path = os.path.join('./best', 'best.pth')\n",
    "    torch.save(model, model_path)\n",
    "    print(f'Final model saved to: {model_path}')\n",
    "\n",
    "    # Plot training curves\n",
    "    plot_training_curves(train_losses, val_losses, train_accs, val_accs,\n",
    "                        train_precisions, val_precisions, train_recalls, val_recalls,\n",
    "                        train_f1s, val_f1s, num_epochs, save_dir)\n",
    "\n",
    "    return model, best_acc\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, train_accs, val_accs,\n",
    "                        train_precisions, val_precisions, train_recalls, val_recalls,\n",
    "                        train_f1s, val_f1s, num_epochs, save_dir):\n",
    "    \"\"\"Plot training curves (all in English)\"\"\"\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, num_epochs+1), train_accs, 'b-', label='Train Accuracy')\n",
    "    plt.plot(range(1, num_epochs+1), val_accs, 'r-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, num_epochs+1), train_precisions, 'b--', label='Train Precision')\n",
    "    plt.plot(range(1, num_epochs+1), val_precisions, 'r--', label='Val Precision')\n",
    "    plt.plot(range(1, num_epochs+1), train_recalls, 'b:', label='Train Recall')\n",
    "    plt.plot(range(1, num_epochs+1), val_recalls, 'r:', label='Val Recall')\n",
    "    plt.title('Precision and Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, num_epochs+1), train_f1s, 'b-', label='Train F1-Score')\n",
    "    plt.plot(range(1, num_epochs+1), val_f1s, 'r-', label='Val F1-Score')\n",
    "    plt.title('F1-Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    curve_path = os.path.join(save_dir, 'training_curves_all_metrics.png')\n",
    "    plt.savefig(curve_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Training curves (including F1, Precision, Recall) saved to: {curve_path}')\n",
    "\n",
    "def plot_optuna_results(study):\n",
    "    \"\"\"Plot Optuna optimization results (all in English)\"\"\"\n",
    "    print(\"\\nPlotting Optuna optimization results...\")\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Optimization history\n",
    "    try:\n",
    "        optuna.visualization.matplotlib.plot_optimization_history(study, ax=axes[0,0])\n",
    "        axes[0,0].set_title('Optimization History')\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot plot optimization history: {e}\")\n",
    "        axes[0,0].text(0.5, 0.5, 'Optimization History\\nNot Available', ha='center', va='center')\n",
    "    \n",
    "    # 2. Parameter importance\n",
    "    try:\n",
    "        optuna.visualization.matplotlib.plot_param_importances(study, ax=axes[0,1])\n",
    "        axes[0,1].set_title('Parameter Importance')\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot plot parameter importance: {e}\")\n",
    "        axes[0,1].text(0.5, 0.5, 'Parameter Importance\\nNot Available', ha='center', va='center')\n",
    "    \n",
    "    # 3. Trial value distribution\n",
    "    values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "    if values:\n",
    "        axes[1,0].hist(values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[1,0].set_title('Trial Value Distribution')\n",
    "        axes[1,0].set_xlabel('Accuracy')\n",
    "        axes[1,0].set_ylabel('Frequency')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Best parameters table\n",
    "    axes[1,1].axis('off')\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "    \n",
    "    param_text = f\"Best Accuracy: {best_value:.4f}\\n\\nBest Parameters:\\n\"\n",
    "    for key, value in best_params.items():\n",
    "        param_text += f\"{key}: {value}\\n\"\n",
    "    \n",
    "    axes[1,1].text(0.1, 0.9, param_text, transform=axes[1,1].transAxes, \n",
    "                   fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    axes[1,1].set_title('Best Parameters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save chart\n",
    "    optuna_results_path = os.path.join(data_dir, 'optuna_optimization_results.png')\n",
    "    plt.savefig(optuna_results_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Optuna optimization results saved to: {optuna_results_path}')\n",
    "    \n",
    "    # Create detailed parameter comparison chart\n",
    "    plot_parameter_comparison(study)\n",
    "\n",
    "def plot_parameter_comparison(study):\n",
    "    \"\"\"Plot parameter comparison chart (all in English)\"\"\"\n",
    "    print(\"Plotting parameter comparison chart...\")\n",
    "    \n",
    "    df = study.trials_dataframe()\n",
    "    if len(df) == 0:\n",
    "        print(\"No trial data available for plotting\")\n",
    "        return\n",
    "    \n",
    "    # Extract numeric parameters\n",
    "    numeric_params = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith('params_') and df[col].dtype in ['float64', 'int64']:\n",
    "            numeric_params.append(col)\n",
    "    \n",
    "    if len(numeric_params) >= 2:\n",
    "        n_params = len(numeric_params)\n",
    "        n_cols = min(3, n_params)\n",
    "        n_rows = (n_params + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        elif n_cols == 1:\n",
    "            axes = [[ax] for ax in axes]\n",
    "        \n",
    "        for i, param in enumerate(numeric_params):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            ax = axes[row][col] if n_rows > 1 else axes[col]\n",
    "            \n",
    "            # Scatter plot: parameter value vs accuracy\n",
    "            x = df[param].values\n",
    "            y = df['value'].values\n",
    "            \n",
    "            # Remove NaN values\n",
    "            mask = ~(pd.isna(x) | pd.isna(y))\n",
    "            x = x[mask]\n",
    "            y = y[mask]\n",
    "            \n",
    "            if len(x) > 0:\n",
    "                ax.scatter(x, y, alpha=0.6)\n",
    "                ax.set_xlabel(param.replace('params_', ''))\n",
    "                ax.set_ylabel('Accuracy')\n",
    "                ax.set_title(f'{param.replace(\"params_\", \"\")} vs Accuracy')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide extra subplots\n",
    "        for i in range(n_params, n_rows * n_cols):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            if n_rows > 1:\n",
    "                axes[row][col].set_visible(False)\n",
    "            elif n_cols > 1:\n",
    "                axes[col].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        param_comp_path = os.path.join(data_dir, 'parameter_comparison.png')\n",
    "        plt.savefig(param_comp_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f'Parameter comparison chart saved to: {param_comp_path}')\n",
    "\n",
    "def test_samples(model, val_loader, class_names, device, num_samples=10):\n",
    "    \"\"\"Test several samples\"\"\"\n",
    "    model.eval()\n",
    "    samples_tested = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    print(f\"\\nTesting {num_samples} samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(min(len(preds), num_samples - samples_tested)):\n",
    "                pred_class = class_names[preds[i]]\n",
    "                true_class = class_names[labels[i]]\n",
    "                is_correct = preds[i] == labels[i]\n",
    "                \n",
    "                status = \"✓\" if is_correct else \"✗\"\n",
    "                print(f\"{status} Predicted: {pred_class} | Actual: {true_class}\")\n",
    "                \n",
    "                if is_correct:\n",
    "                    correct_predictions += 1\n",
    "                samples_tested += 1\n",
    "                \n",
    "                if samples_tested >= num_samples:\n",
    "                    break\n",
    "            \n",
    "            if samples_tested >= num_samples:\n",
    "                break\n",
    "    \n",
    "    accuracy = correct_predictions / samples_tested\n",
    "    print(f\"\\nTest sample accuracy: {accuracy:.4f} ({correct_predictions}/{samples_tested})\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main program\"\"\"\n",
    "    print(\"Starting handwritten character recognition training - Using Optuna optimization\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Check data path\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Data folder does not exist: {data_dir}\")\n",
    "        print(\"Please confirm if the data folder path is correct\")\n",
    "        return\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        # Load data for optimization\n",
    "        train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(data_dir, 64, subsample_rate=subsample_rate)\n",
    "        print(f\"Data loaded successfully\")\n",
    "        print(f\"   Training samples: {train_size}\")\n",
    "        print(f\"   Validation samples: {val_size}\")\n",
    "        print(f\"   Character classes: {class_num}\")\n",
    "        print(f\"   Class names: {class_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Data loading failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # === Optuna hyperparameter optimization ===\n",
    "    print(f\"\\nStarting Optuna hyperparameter optimization...\")\n",
    "    print(\"This will perform multiple trials to find the best parameter combination\")\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize', \n",
    "                               study_name='handwriting_optimization',\n",
    "                               sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    \n",
    "    # Execute optimization with simple progress tracking\n",
    "    n_trials = 5\n",
    "    print(f\"Will perform {n_trials} trials...\")\n",
    "    \n",
    "    for trial_num in range(n_trials):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔍 OPTUNA TRIAL {trial_num + 1}/{n_trials}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            study.optimize(objective, n_trials=1, timeout=None)\n",
    "            \n",
    "            if study.best_value:\n",
    "                print(f\"✅ Trial {trial_num + 1} completed!\")\n",
    "                print(f\"📊 Current Best Accuracy: {study.best_value:.4f}\")\n",
    "                print(f\"🎯 Best Parameters so far: {study.best_params}\")\n",
    "            else:\n",
    "                print(f\"❌ Trial {trial_num + 1} failed to complete\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n⏹️  Trial {trial_num + 1} interrupted by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Trial {trial_num + 1} failed with error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🏁 OPTUNA OPTIMIZATION COMPLETED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Show best results\n",
    "    print(f\"\\nOptuna optimization complete!\")\n",
    "    print(f\"Best accuracy: {study.best_value:.4f}\")\n",
    "    print(f\"Best parameters: {study.best_params}\")\n",
    "    \n",
    "    # Plot optimization results\n",
    "    plot_optuna_results(study)\n",
    "    \n",
    "    # === Use best parameters for complete training ===\n",
    "    print(f\"\\nUsing best parameters for complete training...\")\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Rebuild model and optimizer\n",
    "    train_loader, val_loader, train_size, val_size, class_num, class_names = loaddata(\n",
    "        data_dir, best_params['batch_size'])\n",
    "    \n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    num_ftrs = model._fc.in_features\n",
    "    model._fc = nn.Sequential(\n",
    "        nn.Dropout(best_params['dropout_rate']),\n",
    "        nn.Linear(num_ftrs, class_num)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set optimizer\n",
    "    if best_params['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), \n",
    "                              lr=best_params['lr'], \n",
    "                              weight_decay=best_params['weight_decay'])\n",
    "    elif best_params['optimizer'] == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), \n",
    "                               lr=best_params['lr'], \n",
    "                               weight_decay=best_params['weight_decay'])\n",
    "    else:  # SGD\n",
    "        optimizer = optim.SGD(model.parameters(), \n",
    "                             lr=best_params['lr'], \n",
    "                             momentum=0.9, \n",
    "                             weight_decay=best_params['weight_decay'])\n",
    "    \n",
    "    # Set learning rate scheduler\n",
    "    if best_params['scheduler'] == 'StepLR':\n",
    "        scheduler = get_lr_scheduler(optimizer, 'StepLR', \n",
    "                                   best_params.get('step_size', 2), \n",
    "                                   best_params['gamma'])\n",
    "    else:\n",
    "        scheduler = get_lr_scheduler(optimizer, best_params['scheduler'], \n",
    "                                   gamma=best_params['gamma'])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(f\"Using best parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Complete training\n",
    "    num_epochs_final = 5  # Reduced to 5 epochs for final training\n",
    "    try:\n",
    "        model, best_acc = train_model_full(\n",
    "            model, criterion, optimizer, scheduler,\n",
    "            train_loader, val_loader,\n",
    "            train_size, val_size,\n",
    "            num_epochs=num_epochs_final\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🎉 Complete training finished! Final accuracy: {best_acc:.4f}\")\n",
    "        \n",
    "        # Test several samples\n",
    "        print(\"\\nTesting several samples...\")\n",
    "        test_samples(model, val_loader, class_names, device)\n",
    "        \n",
    "        # Save best parameters to file\n",
    "        import json\n",
    "        params_file = os.path.join(data_dir, 'best_hyperparameters.json')\n",
    "        with open(params_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Best hyperparameters saved to: {params_file}\")\n",
    "        \n",
    "        # Create optimization summary report\n",
    "        create_optimization_summary(study, best_acc, data_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during complete training: {e}\")\n",
    "        return\n",
    "\n",
    "def create_optimization_summary(study, final_accuracy, save_dir):\n",
    "    \"\"\"Create optimization summary report (all in English)\"\"\"\n",
    "    print(\"\\nCreating optimization summary report...\")\n",
    "    \n",
    "    # Collect statistical information\n",
    "    trials_df = study.trials_dataframe()\n",
    "    completed_trials = trials_df[trials_df['state'] == 'COMPLETE']\n",
    "    \n",
    "    if len(completed_trials) == 0:\n",
    "        print(\"No completed trials to analyze\")\n",
    "        return\n",
    "    \n",
    "    summary = {\n",
    "        'optimization_summary': {\n",
    "            'total_trials': len(study.trials),\n",
    "            'completed_trials': len(completed_trials),\n",
    "            'best_trial_number': study.best_trial.number,\n",
    "            'best_validation_accuracy': study.best_value,\n",
    "            'final_training_accuracy': final_accuracy,\n",
    "            'improvement': final_accuracy - study.best_value if study.best_value else 0\n",
    "        },\n",
    "        'best_parameters': study.best_params,\n",
    "        'statistics': {\n",
    "            'mean_accuracy': float(completed_trials['value'].mean()),\n",
    "            'std_accuracy': float(completed_trials['value'].std()),\n",
    "            'min_accuracy': float(completed_trials['value'].min()),\n",
    "            'max_accuracy': float(completed_trials['value'].max())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save summary to JSON\n",
    "    import json\n",
    "    summary_file = os.path.join(save_dir, 'optimization_summary.json')\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Create text report\n",
    "    report_file = os.path.join(save_dir, 'optimization_report.txt')\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(\"Handwritten Character Recognition - Optuna Optimization Report\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Optimization Summary:\\n\")\n",
    "        f.write(f\"  Total trials: {summary['optimization_summary']['total_trials']}\\n\")\n",
    "        f.write(f\"  Completed trials: {summary['optimization_summary']['completed_trials']}\\n\")\n",
    "        f.write(f\"  Best trial number: {summary['optimization_summary']['best_trial_number']}\\n\")\n",
    "        f.write(f\"  Best validation accuracy: {summary['optimization_summary']['best_validation_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Final training accuracy: {summary['optimization_summary']['final_training_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Accuracy improvement: {summary['optimization_summary']['improvement']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Best Parameters:\\n\")\n",
    "        for key, value in summary['best_parameters'].items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"Trial Statistics:\\n\")\n",
    "        f.write(f\"  Mean accuracy: {summary['statistics']['mean_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Accuracy standard deviation: {summary['statistics']['std_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Minimum accuracy: {summary['statistics']['min_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Maximum accuracy: {summary['statistics']['max_accuracy']:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Optimization summary saved:\")\n",
    "    print(f\"  JSON format: {summary_file}\")\n",
    "    print(f\"  Text report: {report_file}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nOptimization Summary:\")\n",
    "    print(f\"   Total trials: {summary['optimization_summary']['total_trials']}\")\n",
    "    print(f\"   Best validation accuracy: {summary['optimization_summary']['best_validation_accuracy']:.4f}\")\n",
    "    print(f\"   Final training accuracy: {summary['optimization_summary']['final_training_accuracy']:.4f}\")\n",
    "    print(f\"   Accuracy improvement: {summary['optimization_summary']['improvement']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b55207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "113-2-ml-JRTFNsIF-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
